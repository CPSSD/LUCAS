{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of neural Models\n",
    "This notebook creates a comparison of how our neural models perform. We will try each model with and without word embeddings, and produce visualisations of model performance. \n",
    "\n",
    "# Feed-Forward Neural Networks\n",
    "First we will find this difference for Feed-Forward Neural Networks (FFNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Embedding, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scripts import training_helpers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from seaborn import boxplot\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cross validate our model, so lets create a function to handle this for us. It will use StratifiedKFold splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validate(get_model, X, y, cv=5, categorical=False):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "  }\n",
    "    \n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = np.array([X[x] for x in train_indices])\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = np.array([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "    \n",
    "    if categorical:\n",
    "      training_y = to_categorical(training_y)\n",
    "      test_y = to_categorical(test_y)\n",
    "    \n",
    "    model = get_model()\n",
    "    model.fit(np.array(training_X), training_y, epochs=12, batch_size=16, validation_split=0.3,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "    metrics[\"accuracies\"].append(model.evaluate(np.array(test_X), test_y)[1])\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find results for our Bag of Words (BoW) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = training_helpers.get_data_frame()\n",
    "\n",
    "predictors_raw = data_frame['review']\n",
    "num_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(predictors_raw)\n",
    "bow_predictors = tokenizer.texts_to_matrix(predictors_raw, mode='tfidf')\n",
    "labels = [x for x in data_frame['deceptive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/12\n",
      "560/560 [==============================] - 1s 3ms/step - loss: 0.8071 - acc: 0.6446 - val_loss: 0.6504 - val_acc: 0.8208\n",
      "Epoch 2/12\n",
      "560/560 [==============================] - 0s 804us/step - loss: 0.4659 - acc: 0.9321 - val_loss: 0.5257 - val_acc: 0.8625\n",
      "Epoch 3/12\n",
      "560/560 [==============================] - 0s 805us/step - loss: 0.3024 - acc: 0.9732 - val_loss: 0.4687 - val_acc: 0.8750\n",
      "Epoch 4/12\n",
      "560/560 [==============================] - 0s 856us/step - loss: 0.2227 - acc: 0.9929 - val_loss: 0.4506 - val_acc: 0.8625\n",
      "Epoch 5/12\n",
      "560/560 [==============================] - 0s 886us/step - loss: 0.2027 - acc: 0.9929 - val_loss: 0.4386 - val_acc: 0.8583\n",
      "Epoch 6/12\n",
      "560/560 [==============================] - 0s 849us/step - loss: 0.1701 - acc: 0.9982 - val_loss: 0.4258 - val_acc: 0.8708\n",
      "Epoch 7/12\n",
      "560/560 [==============================] - 0s 794us/step - loss: 0.1639 - acc: 0.9946 - val_loss: 0.4257 - val_acc: 0.8750\n",
      "Epoch 8/12\n",
      "560/560 [==============================] - 1s 896us/step - loss: 0.1509 - acc: 0.9982 - val_loss: 0.4388 - val_acc: 0.8667\n",
      "Epoch 9/12\n",
      "560/560 [==============================] - 0s 891us/step - loss: 0.1435 - acc: 1.0000 - val_loss: 0.4350 - val_acc: 0.8792\n",
      "Epoch 10/12\n",
      "560/560 [==============================] - 1s 977us/step - loss: 0.1358 - acc: 0.9946 - val_loss: 0.4369 - val_acc: 0.8708\n",
      "Epoch 11/12\n",
      "560/560 [==============================] - 0s 845us/step - loss: 0.1376 - acc: 0.9964 - val_loss: 0.4608 - val_acc: 0.8417\n",
      "800/800 [==============================] - 0s 196us/step\n",
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/12\n",
      "560/560 [==============================] - 1s 2ms/step - loss: 0.8048 - acc: 0.6268 - val_loss: 0.6342 - val_acc: 0.8333\n",
      "Epoch 2/12\n",
      "560/560 [==============================] - 0s 842us/step - loss: 0.3943 - acc: 0.9429 - val_loss: 0.5230 - val_acc: 0.8333\n",
      "Epoch 3/12\n",
      "560/560 [==============================] - 0s 809us/step - loss: 0.2257 - acc: 0.9893 - val_loss: 0.4977 - val_acc: 0.8458\n",
      "Epoch 4/12\n",
      "560/560 [==============================] - 0s 864us/step - loss: 0.1886 - acc: 0.9946 - val_loss: 0.4853 - val_acc: 0.8583\n",
      "Epoch 5/12\n",
      "560/560 [==============================] - 1s 908us/step - loss: 0.1710 - acc: 0.9911 - val_loss: 0.5166 - val_acc: 0.8083\n",
      "Epoch 6/12\n",
      "560/560 [==============================] - 0s 866us/step - loss: 0.1555 - acc: 1.0000 - val_loss: 0.4783 - val_acc: 0.8542\n",
      "Epoch 7/12\n",
      "560/560 [==============================] - 0s 863us/step - loss: 0.1413 - acc: 0.9982 - val_loss: 0.4799 - val_acc: 0.8667\n",
      "Epoch 8/12\n",
      "560/560 [==============================] - 0s 852us/step - loss: 0.1355 - acc: 0.9982 - val_loss: 0.4918 - val_acc: 0.8458\n",
      "Epoch 9/12\n",
      "560/560 [==============================] - 0s 832us/step - loss: 0.1286 - acc: 0.9982 - val_loss: 0.4684 - val_acc: 0.8667\n",
      "Epoch 10/12\n",
      "560/560 [==============================] - 0s 841us/step - loss: 0.1211 - acc: 0.9982 - val_loss: 0.4659 - val_acc: 0.8667\n",
      "Epoch 11/12\n",
      "560/560 [==============================] - 0s 855us/step - loss: 0.1186 - acc: 1.0000 - val_loss: 0.4657 - val_acc: 0.8708\n",
      "Epoch 12/12\n",
      "560/560 [==============================] - 0s 882us/step - loss: 0.1183 - acc: 0.9982 - val_loss: 0.5181 - val_acc: 0.8417\n",
      "800/800 [==============================] - 0s 211us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_bow_model():\n",
    "  model = Sequential([\n",
    "      Dense(16, activation=relu, input_shape=(num_words,), kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "ff_bow_scores = run_cross_validate(get_ff_bow_model, bow_predictors, labels, cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for our word vector method. First we must create our word vectors using a word vectorizing model generated in another experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load(\"opspam_w2v.kv\", mmap=\"r\")\n",
    "\n",
    "predictors_sequences = pad_sequences(tokenizer.texts_to_sequences(predictors_raw))\n",
    "max_sequence_length = max([len(x) for x in predictors_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = word_vectors.vector_size\n",
    "\n",
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1\n",
    "vectorizer_words = word_vectors.wv\n",
    "embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "for word, idx in corpus_words.items():\n",
    "  if word in vectorizer_words.vocab:\n",
    "    embedding_matrix[idx] = np.array(vectorizer_words[word], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8892 - acc: 0.5114 - val_loss: 0.8111 - val_acc: 0.5681\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7886 - acc: 0.5968 - val_loss: 0.7751 - val_acc: 0.6189\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7564 - acc: 0.6216 - val_loss: 0.7772 - val_acc: 0.6005\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7434 - acc: 0.6504 - val_loss: 0.7879 - val_acc: 0.6143\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6938 - acc: 0.6862 - val_loss: 0.7597 - val_acc: 0.6189\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6735 - acc: 0.6951 - val_loss: 0.7771 - val_acc: 0.6328\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6624 - acc: 0.7051 - val_loss: 0.7744 - val_acc: 0.6005\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6650 - acc: 0.6892 - val_loss: 0.7679 - val_acc: 0.6374\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6427 - acc: 0.7001 - val_loss: 0.7927 - val_acc: 0.6074\n",
      "160/160 [==============================] - 0s 501us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8793 - acc: 0.5174 - val_loss: 0.8131 - val_acc: 0.4850\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7761 - acc: 0.5084 - val_loss: 0.7622 - val_acc: 0.5358\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7892 - acc: 0.5521 - val_loss: 0.7899 - val_acc: 0.4850\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7456 - acc: 0.5462 - val_loss: 0.7498 - val_acc: 0.5196\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7224 - acc: 0.6097 - val_loss: 0.7618 - val_acc: 0.5312\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7098 - acc: 0.6137 - val_loss: 0.7470 - val_acc: 0.5935\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7114 - acc: 0.6385 - val_loss: 0.7482 - val_acc: 0.5889\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6977 - acc: 0.6415 - val_loss: 0.7432 - val_acc: 0.5982\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6865 - acc: 0.6504 - val_loss: 0.7682 - val_acc: 0.5681\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6843 - acc: 0.6395 - val_loss: 0.7978 - val_acc: 0.5612\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6784 - acc: 0.6723 - val_loss: 0.7464 - val_acc: 0.6305\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6675 - acc: 0.6614 - val_loss: 0.7450 - val_acc: 0.6351\n",
      "160/160 [==============================] - 0s 378us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.9339 - acc: 0.4965 - val_loss: 0.8415 - val_acc: 0.5266\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8042 - acc: 0.5243 - val_loss: 0.7882 - val_acc: 0.5058\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7702 - acc: 0.5948 - val_loss: 0.7674 - val_acc: 0.5450\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7445 - acc: 0.6137 - val_loss: 0.7869 - val_acc: 0.5058\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7464 - acc: 0.6068 - val_loss: 0.7683 - val_acc: 0.5797\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7240 - acc: 0.6296 - val_loss: 0.7488 - val_acc: 0.5820\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7101 - acc: 0.6643 - val_loss: 0.7541 - val_acc: 0.5727\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7180 - acc: 0.6455 - val_loss: 0.7520 - val_acc: 0.6328\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6834 - acc: 0.6961 - val_loss: 0.7522 - val_acc: 0.5958\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6734 - acc: 0.6991 - val_loss: 0.7758 - val_acc: 0.5866\n",
      "160/160 [==============================] - 0s 300us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8810 - acc: 0.5144 - val_loss: 0.8163 - val_acc: 0.5012\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8084 - acc: 0.5611 - val_loss: 0.8017 - val_acc: 0.5473\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7666 - acc: 0.5859 - val_loss: 0.7694 - val_acc: 0.5404\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7510 - acc: 0.5740 - val_loss: 0.7846 - val_acc: 0.5012\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7396 - acc: 0.5501 - val_loss: 0.7544 - val_acc: 0.5242\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7215 - acc: 0.5809 - val_loss: 0.7358 - val_acc: 0.5427\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7173 - acc: 0.5819 - val_loss: 0.7329 - val_acc: 0.6097\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7073 - acc: 0.6207 - val_loss: 0.7324 - val_acc: 0.5843\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7008 - acc: 0.6286 - val_loss: 0.7331 - val_acc: 0.6097\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7005 - acc: 0.6435 - val_loss: 0.7295 - val_acc: 0.5727\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6953 - acc: 0.6216 - val_loss: 0.7563 - val_acc: 0.5589\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6889 - acc: 0.6435 - val_loss: 0.7501 - val_acc: 0.5635\n",
      "160/160 [==============================] - 0s 323us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8440 - acc: 0.4737 - val_loss: 0.7721 - val_acc: 0.4988\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7584 - acc: 0.5333 - val_loss: 0.7625 - val_acc: 0.5035\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7466 - acc: 0.5392 - val_loss: 0.7526 - val_acc: 0.5797\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7296 - acc: 0.5730 - val_loss: 0.7233 - val_acc: 0.5566\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7142 - acc: 0.5631 - val_loss: 0.7198 - val_acc: 0.5843\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7068 - acc: 0.5899 - val_loss: 0.7166 - val_acc: 0.6120\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7006 - acc: 0.6236 - val_loss: 0.7357 - val_acc: 0.5058\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7066 - acc: 0.5938 - val_loss: 0.7147 - val_acc: 0.6028\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6984 - acc: 0.6306 - val_loss: 0.7115 - val_acc: 0.5751\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6867 - acc: 0.6187 - val_loss: 0.7070 - val_acc: 0.6490\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6780 - acc: 0.6534 - val_loss: 0.7261 - val_acc: 0.6028\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6897 - acc: 0.6514 - val_loss: 0.7084 - val_acc: 0.6051\n",
      "160/160 [==============================] - 0s 331us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.9152 - acc: 0.5005 - val_loss: 0.8182 - val_acc: 0.4919\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8013 - acc: 0.5025 - val_loss: 0.7734 - val_acc: 0.4919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7586 - acc: 0.4975 - val_loss: 0.7512 - val_acc: 0.5358\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7373 - acc: 0.4965 - val_loss: 0.7273 - val_acc: 0.5081\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7221 - acc: 0.4777 - val_loss: 0.7171 - val_acc: 0.5081\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7135 - acc: 0.4965 - val_loss: 0.7101 - val_acc: 0.5081\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7077 - acc: 0.4965 - val_loss: 0.7053 - val_acc: 0.5081\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7036 - acc: 0.4995 - val_loss: 0.7020 - val_acc: 0.4919\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7007 - acc: 0.5035 - val_loss: 0.6996 - val_acc: 0.4919\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6987 - acc: 0.5035 - val_loss: 0.6980 - val_acc: 0.4919\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6973 - acc: 0.5035 - val_loss: 0.6968 - val_acc: 0.4919\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6964 - acc: 0.5035 - val_loss: 0.6961 - val_acc: 0.4919\n",
      "160/160 [==============================] - 0s 296us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8823 - acc: 0.5074 - val_loss: 0.8056 - val_acc: 0.5681\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7783 - acc: 0.5402 - val_loss: 0.7581 - val_acc: 0.5797\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7892 - acc: 0.5144 - val_loss: 0.7879 - val_acc: 0.4988\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7573 - acc: 0.5432 - val_loss: 0.7567 - val_acc: 0.5358\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7561 - acc: 0.5829 - val_loss: 0.7791 - val_acc: 0.5658\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7518 - acc: 0.5521 - val_loss: 0.7613 - val_acc: 0.4896\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7306 - acc: 0.6048 - val_loss: 0.7492 - val_acc: 0.5774\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7213 - acc: 0.6137 - val_loss: 0.7645 - val_acc: 0.5173\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7238 - acc: 0.6058 - val_loss: 0.7305 - val_acc: 0.6282\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7110 - acc: 0.6097 - val_loss: 0.7268 - val_acc: 0.6028\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7180 - acc: 0.6008 - val_loss: 0.7249 - val_acc: 0.5289\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6984 - acc: 0.6336 - val_loss: 0.7228 - val_acc: 0.6166\n",
      "160/160 [==============================] - 0s 440us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8812 - acc: 0.4826 - val_loss: 0.8145 - val_acc: 0.4850\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8048 - acc: 0.5233 - val_loss: 0.7943 - val_acc: 0.5035\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7643 - acc: 0.5283 - val_loss: 0.7436 - val_acc: 0.5704\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7417 - acc: 0.5750 - val_loss: 0.7334 - val_acc: 0.5751\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7324 - acc: 0.5581 - val_loss: 0.7337 - val_acc: 0.5958\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7260 - acc: 0.6167 - val_loss: 0.7470 - val_acc: 0.6074\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7200 - acc: 0.6127 - val_loss: 0.7555 - val_acc: 0.5242\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7075 - acc: 0.6465 - val_loss: 0.7231 - val_acc: 0.6674\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7135 - acc: 0.6346 - val_loss: 0.7582 - val_acc: 0.5958\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6950 - acc: 0.6614 - val_loss: 0.7344 - val_acc: 0.6143\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6913 - acc: 0.6723 - val_loss: 0.7260 - val_acc: 0.6282\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6762 - acc: 0.6783 - val_loss: 0.7787 - val_acc: 0.5266\n",
      "160/160 [==============================] - 0s 423us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8875 - acc: 0.5174 - val_loss: 0.8045 - val_acc: 0.5751\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7872 - acc: 0.5482 - val_loss: 0.7768 - val_acc: 0.5727\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7536 - acc: 0.5839 - val_loss: 0.7668 - val_acc: 0.6074\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7258 - acc: 0.6415 - val_loss: 0.7642 - val_acc: 0.6005\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7147 - acc: 0.6336 - val_loss: 0.7606 - val_acc: 0.6166\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6841 - acc: 0.6852 - val_loss: 0.7601 - val_acc: 0.6236\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6749 - acc: 0.6852 - val_loss: 0.7843 - val_acc: 0.6189\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6623 - acc: 0.7080 - val_loss: 0.7861 - val_acc: 0.6028\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6589 - acc: 0.6912 - val_loss: 0.7743 - val_acc: 0.5912\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6491 - acc: 0.7090 - val_loss: 0.8071 - val_acc: 0.5935\n",
      "160/160 [==============================] - 0s 366us/step\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.9038 - acc: 0.5402 - val_loss: 0.8186 - val_acc: 0.5797\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7911 - acc: 0.5621 - val_loss: 0.7799 - val_acc: 0.5035\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7673 - acc: 0.5899 - val_loss: 0.7556 - val_acc: 0.5820\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7397 - acc: 0.5958 - val_loss: 0.7490 - val_acc: 0.5843\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7373 - acc: 0.6226 - val_loss: 0.7429 - val_acc: 0.6282\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7195 - acc: 0.6276 - val_loss: 0.7544 - val_acc: 0.5612\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7004 - acc: 0.6544 - val_loss: 0.7304 - val_acc: 0.5612\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6969 - acc: 0.6544 - val_loss: 0.7300 - val_acc: 0.6189\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6897 - acc: 0.6584 - val_loss: 0.7637 - val_acc: 0.5497\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6793 - acc: 0.6961 - val_loss: 0.7344 - val_acc: 0.6443\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6638 - acc: 0.7249 - val_loss: 0.7727 - val_acc: 0.5727\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6700 - acc: 0.7130 - val_loss: 0.7682 - val_acc: 0.5889\n",
      "160/160 [==============================] - 0s 361us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_wv_model():\n",
    "  model_ff_wv = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], trainable=False,\n",
    "                input_length=max_sequence_length),\n",
    "      Flatten(),\n",
    "      Dense(16, activation=relu, kernel_regularizer=l2(0.01)), #, input_shape=(num_words,)\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model_ff_wv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model_ff_wv\n",
    "\n",
    "ff_wv_scores = run_cross_validate(get_ff_wv_model, predictors_sequences, labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.84375, 0.89375, 0.8625, 0.90625, 0.85, 0.8375, 0.875, 0.8625, 0.875, 0.8875]\n",
      "Word vectors:  [0.64375, 0.625, 0.59375, 0.54375, 0.6, 0.5, 0.56875, 0.53125, 0.575, 0.56875]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", ff_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", ff_wv_scores['accuracies'])\n",
    "\n",
    "ff_scores_entries =[('Bag of Words', x) for x in ff_bow_scores['accuracies']] + [('Word Vectors', x) for x in ff_wv_scores['accuracies']]\n",
    "ff_scores_data_frame = DataFrame(ff_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=ff_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very strange! How could word vectors be less accurate than bag of words? This is known to occur when two conditions are met:\n",
    "    \n",
    "* The dataset is small\n",
    "* The dataset is very domain specific\n",
    "\n",
    "It is possible that these conditions actually are met here. The problem however is that running these models over our full dataset will take much longer, and will require a commited experiment to complete the investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 20000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 20000, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in bow_predictors]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_bow_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(1, 10),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(1, 20000, 1),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(pool_size=(1, 10)),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/12\n",
      "560/560 [==============================] - 13s 23ms/step - loss: 0.5304 - acc: 0.7357 - val_loss: 0.6516 - val_acc: 0.6667\n",
      "Epoch 2/12\n",
      "560/560 [==============================] - 11s 19ms/step - loss: 0.1653 - acc: 0.9589 - val_loss: 0.7696 - val_acc: 0.6292\n",
      "Epoch 3/12\n",
      "560/560 [==============================] - 10s 18ms/step - loss: 0.0820 - acc: 0.9804 - val_loss: 0.7386 - val_acc: 0.6708\n",
      "Epoch 4/12\n",
      "560/560 [==============================] - 11s 20ms/step - loss: 0.0499 - acc: 0.9946 - val_loss: 0.9655 - val_acc: 0.5708\n",
      "Epoch 5/12\n",
      "560/560 [==============================] - 13s 23ms/step - loss: 0.0282 - acc: 0.9964 - val_loss: 0.9687 - val_acc: 0.6167\n",
      "800/800 [==============================] - 7s 8ms/step\n",
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/12\n",
      "560/560 [==============================] - 13s 24ms/step - loss: 0.5012 - acc: 0.7679 - val_loss: 0.6617 - val_acc: 0.6292\n",
      "Epoch 2/12\n",
      "560/560 [==============================] - 9s 16ms/step - loss: 0.1410 - acc: 0.9696 - val_loss: 0.7467 - val_acc: 0.6292\n",
      "Epoch 3/12\n",
      "560/560 [==============================] - 10s 18ms/step - loss: 0.0662 - acc: 0.9893 - val_loss: 0.8208 - val_acc: 0.6292\n",
      "Epoch 4/12\n",
      "560/560 [==============================] - 10s 18ms/step - loss: 0.0360 - acc: 0.9982 - val_loss: 0.9100 - val_acc: 0.6333\n",
      "Epoch 5/12\n",
      "560/560 [==============================] - 12s 22ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.9885 - val_acc: 0.6208\n",
      "800/800 [==============================] - 6s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "conv_bow_scores = run_cross_validate(get_conv_bow_model, convolutional_data, labels, cv=2, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_review(review_words):\n",
    "  sentence = []\n",
    "  for word in review_words:\n",
    "    if word in word_vectors.wv:\n",
    "      sentence.append(word_vectors.wv['dog'])\n",
    "  return np.array(sentence, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vectorized_review(vectorized_review, length):\n",
    "  return np.concatenate((vectorized_review, np.zeros((length - len(vectorized_review), 100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = [vectorize_review(text_to_word_sequence(x)) for x in predictors_raw]\n",
    "pad_length = max([x.shape[0] for x in vectorized_reviews])\n",
    "vectorized_reviews = np.array([[pad_vectorized_review(x, pad_length)] for x in vectorized_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 1, 381, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 100),\n",
    "          data_format=\"channels_first\",\n",
    "          input_shape=(1, 381, 100),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(strides=(1, 1), pool_size=(2, 1), data_format=\"channels_first\"),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/12\n",
      "560/560 [==============================] - 5s 9ms/step - loss: 0.7025 - acc: 0.4911 - val_loss: 0.6932 - val_acc: 0.4917\n",
      "Epoch 2/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6923 - acc: 0.5429 - val_loss: 0.6993 - val_acc: 0.4625\n",
      "Epoch 3/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6823 - acc: 0.5571 - val_loss: 0.6747 - val_acc: 0.5792\n",
      "Epoch 4/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6699 - acc: 0.6304 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 5/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6463 - acc: 0.6589 - val_loss: 0.6832 - val_acc: 0.5667\n",
      "Epoch 6/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6338 - acc: 0.6393 - val_loss: 0.6863 - val_acc: 0.5875\n",
      "Epoch 7/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6156 - acc: 0.6821 - val_loss: 0.6981 - val_acc: 0.5667\n",
      "800/800 [==============================] - 1s 2ms/step\n",
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/12\n",
      "560/560 [==============================] - 5s 9ms/step - loss: 0.6998 - acc: 0.4714 - val_loss: 0.6930 - val_acc: 0.5208\n",
      "Epoch 2/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6941 - val_acc: 0.4875\n",
      "Epoch 3/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6897 - acc: 0.5554 - val_loss: 0.7010 - val_acc: 0.4542\n",
      "Epoch 4/12\n",
      "560/560 [==============================] - 3s 5ms/step - loss: 0.6855 - acc: 0.5643 - val_loss: 0.7088 - val_acc: 0.4542\n",
      "Epoch 5/12\n",
      "560/560 [==============================] - 4s 6ms/step - loss: 0.6791 - acc: 0.5750 - val_loss: 0.7192 - val_acc: 0.4542\n",
      "800/800 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, vectorized_reviews, labels, cv=2, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.82125, 0.7775]\n",
      "Word vectors:  [0.48, 0.51625]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", conv_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", conv_wv_scores['accuracies'])\n",
    "\n",
    "conv_scores_entries =[('Bag of Words', x) for x in conv_bow_scores['accuracies']] + [('Word Vectors', x) for x in conv_wv_scores['accuracies']]\n",
    "conv_scores_data_frame = DataFrame(conv_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=conv_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
