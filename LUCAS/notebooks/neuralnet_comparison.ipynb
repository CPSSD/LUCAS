{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of neural Models\n",
    "This notebook creates a comparison of how our neural models perform. We will try each model with and without word embeddings, and produce visualisations of model performance. \n",
    "\n",
    "# Feed-Forward Neural Networks\n",
    "First we will find this difference for Feed-Forward Neural Networks (FFNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Embedding, Flatten, LSTM, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scripts import training_helpers\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from seaborn import boxplot\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cross validate our model, so lets create a function to handle this for us. It will use StratifiedKFold splitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find results for our Bag of Words (BoW) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = training_helpers.get_data_frame()\n",
    "\n",
    "predictors_raw = data_frame['review']\n",
    "num_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(predictors_raw)\n",
    "bow_predictors = tokenizer.texts_to_matrix(predictors_raw, mode='tfidf')\n",
    "labels = [x for x in data_frame['deceptive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6841 - acc: 0.7299 - val_loss: 0.4919 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 979us/step - loss: 0.3207 - acc: 0.9533 - val_loss: 0.4375 - val_acc: 0.8845\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 954us/step - loss: 0.2247 - acc: 0.9861 - val_loss: 0.4551 - val_acc: 0.8684\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 846us/step - loss: 0.1878 - acc: 0.9921 - val_loss: 0.4220 - val_acc: 0.8799\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 996us/step - loss: 0.1701 - acc: 0.9921 - val_loss: 0.4326 - val_acc: 0.8845\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1619 - acc: 0.9960 - val_loss: 0.4388 - val_acc: 0.8707\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 942us/step - loss: 0.1410 - acc: 0.9970 - val_loss: 0.4325 - val_acc: 0.8776\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 995us/step - loss: 0.1366 - acc: 0.9950 - val_loss: 0.4526 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 289us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7056 - acc: 0.7279 - val_loss: 0.5265 - val_acc: 0.8868\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 829us/step - loss: 0.3381 - acc: 0.9434 - val_loss: 0.4575 - val_acc: 0.8799\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 990us/step - loss: 0.2363 - acc: 0.9831 - val_loss: 0.4420 - val_acc: 0.8915\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1894 - acc: 0.9960 - val_loss: 0.4378 - val_acc: 0.8915\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 924us/step - loss: 0.1702 - acc: 0.9940 - val_loss: 0.4364 - val_acc: 0.8845\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 983us/step - loss: 0.1517 - acc: 0.9980 - val_loss: 0.4367 - val_acc: 0.8915\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 807us/step - loss: 0.1410 - acc: 0.9940 - val_loss: 0.4323 - val_acc: 0.8730\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 904us/step - loss: 0.1376 - acc: 0.9960 - val_loss: 0.4444 - val_acc: 0.8776\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 816us/step - loss: 0.1390 - acc: 0.9980 - val_loss: 0.4466 - val_acc: 0.8845\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 804us/step - loss: 0.1469 - acc: 0.9921 - val_loss: 0.4863 - val_acc: 0.8661\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 843us/step - loss: 0.1403 - acc: 0.9960 - val_loss: 0.4829 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 227us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7498 - acc: 0.7071 - val_loss: 0.5638 - val_acc: 0.8799\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 815us/step - loss: 0.4170 - acc: 0.9126 - val_loss: 0.4938 - val_acc: 0.8499\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 840us/step - loss: 0.2676 - acc: 0.9722 - val_loss: 0.4810 - val_acc: 0.8637\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2140 - acc: 0.9891 - val_loss: 0.4444 - val_acc: 0.8799\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1829 - acc: 0.9950 - val_loss: 0.4518 - val_acc: 0.8822\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 975us/step - loss: 0.1756 - acc: 0.9901 - val_loss: 0.4546 - val_acc: 0.8753\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1630 - acc: 0.9960 - val_loss: 0.4691 - val_acc: 0.8845\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1602 - acc: 0.9970 - val_loss: 0.4792 - val_acc: 0.8891\n",
      "160/160 [==============================] - 0s 255us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.6894 - acc: 0.7358 - val_loss: 0.5138 - val_acc: 0.8545\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.3196 - acc: 0.9523 - val_loss: 0.4592 - val_acc: 0.8799\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2174 - acc: 0.9891 - val_loss: 0.4649 - val_acc: 0.8637\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1838 - acc: 0.9940 - val_loss: 0.4599 - val_acc: 0.8707\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.1663 - acc: 0.9960 - val_loss: 0.4437 - val_acc: 0.8799\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1553 - acc: 0.9950 - val_loss: 0.4822 - val_acc: 0.8614\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1593 - acc: 0.9950 - val_loss: 0.5086 - val_acc: 0.8637\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1509 - acc: 0.9970 - val_loss: 0.4692 - val_acc: 0.8730\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 987us/step - loss: 0.1471 - acc: 0.9940 - val_loss: 0.4755 - val_acc: 0.8753\n",
      "160/160 [==============================] - 0s 206us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7318 - acc: 0.7080 - val_loss: 0.5465 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 927us/step - loss: 0.3468 - acc: 0.9643 - val_loss: 0.4573 - val_acc: 0.8822\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 916us/step - loss: 0.2312 - acc: 0.9921 - val_loss: 0.4483 - val_acc: 0.8753\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 728us/step - loss: 0.1951 - acc: 0.9960 - val_loss: 0.4571 - val_acc: 0.8753\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 779us/step - loss: 0.1763 - acc: 0.9960 - val_loss: 0.4494 - val_acc: 0.8753\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1657 - acc: 0.9990 - val_loss: 0.4440 - val_acc: 0.8822\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 977us/step - loss: 0.1584 - acc: 0.9980 - val_loss: 0.4273 - val_acc: 0.8799\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1548 - acc: 0.9960 - val_loss: 0.4505 - val_acc: 0.8730\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1544 - acc: 0.9921 - val_loss: 0.4302 - val_acc: 0.8938\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1580 - acc: 0.9940 - val_loss: 0.4619 - val_acc: 0.8707\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1466 - acc: 0.9980 - val_loss: 0.4619 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 225us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.7268 - acc: 0.6941 - val_loss: 0.5627 - val_acc: 0.8776\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 837us/step - loss: 0.3848 - acc: 0.9325 - val_loss: 0.4620 - val_acc: 0.8822\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 972us/step - loss: 0.2405 - acc: 0.9881 - val_loss: 0.4566 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 873us/step - loss: 0.1955 - acc: 0.9970 - val_loss: 0.4575 - val_acc: 0.8845\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1771 - acc: 0.9940 - val_loss: 0.4582 - val_acc: 0.8915\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 920us/step - loss: 0.1674 - acc: 0.9940 - val_loss: 0.4646 - val_acc: 0.8776\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 863us/step - loss: 0.1607 - acc: 0.9940 - val_loss: 0.4594 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 219us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7634 - acc: 0.6634 - val_loss: 0.5894 - val_acc: 0.8822\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.4324 - acc: 0.9166 - val_loss: 0.4643 - val_acc: 0.8938\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2883 - acc: 0.9742 - val_loss: 0.4305 - val_acc: 0.8776\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2227 - acc: 0.9841 - val_loss: 0.4194 - val_acc: 0.8938\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1930 - acc: 0.9911 - val_loss: 0.4176 - val_acc: 0.8799\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1785 - acc: 0.9911 - val_loss: 0.4234 - val_acc: 0.8707\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1646 - acc: 0.9940 - val_loss: 0.4333 - val_acc: 0.8822\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1583 - acc: 0.9960 - val_loss: 0.4301 - val_acc: 0.8730\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 965us/step - loss: 0.1568 - acc: 0.9940 - val_loss: 0.4599 - val_acc: 0.8707\n",
      "160/160 [==============================] - 0s 195us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.7641 - acc: 0.6872 - val_loss: 0.5579 - val_acc: 0.8822\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.3955 - acc: 0.9355 - val_loss: 0.5036 - val_acc: 0.8522\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2586 - acc: 0.9791 - val_loss: 0.4740 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2081 - acc: 0.9921 - val_loss: 0.4843 - val_acc: 0.8845\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1824 - acc: 0.9921 - val_loss: 0.4691 - val_acc: 0.8730\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1643 - acc: 0.9970 - val_loss: 0.4535 - val_acc: 0.8707\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1504 - acc: 0.9980 - val_loss: 0.4490 - val_acc: 0.8891\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1529 - acc: 0.9950 - val_loss: 0.4477 - val_acc: 0.8753\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1434 - acc: 0.9940 - val_loss: 0.4441 - val_acc: 0.8776\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1440 - acc: 0.9950 - val_loss: 0.4579 - val_acc: 0.8707\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1499 - acc: 0.9940 - val_loss: 0.4817 - val_acc: 0.8684\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1531 - acc: 0.9940 - val_loss: 0.5193 - val_acc: 0.8661\n",
      "160/160 [==============================] - 0s 230us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.7052 - acc: 0.7130 - val_loss: 0.5127 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 736us/step - loss: 0.3357 - acc: 0.9305 - val_loss: 0.4500 - val_acc: 0.8845\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 724us/step - loss: 0.2225 - acc: 0.9871 - val_loss: 0.4319 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 718us/step - loss: 0.1869 - acc: 0.9871 - val_loss: 0.4384 - val_acc: 0.8799\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 709us/step - loss: 0.1713 - acc: 0.9911 - val_loss: 0.4498 - val_acc: 0.8661\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 718us/step - loss: 0.1615 - acc: 0.9940 - val_loss: 0.4233 - val_acc: 0.8961\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 690us/step - loss: 0.1433 - acc: 0.9970 - val_loss: 0.4405 - val_acc: 0.8799\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 704us/step - loss: 0.1467 - acc: 0.9921 - val_loss: 0.4298 - val_acc: 0.8868\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 708us/step - loss: 0.1292 - acc: 0.9980 - val_loss: 0.4451 - val_acc: 0.8868\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 715us/step - loss: 0.1257 - acc: 0.9950 - val_loss: 0.4153 - val_acc: 0.8799\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 711us/step - loss: 0.1256 - acc: 0.9970 - val_loss: 0.4726 - val_acc: 0.8776\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 1s 687us/step - loss: 0.1233 - acc: 0.9970 - val_loss: 0.4450 - val_acc: 0.8637\n",
      "160/160 [==============================] - 0s 161us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7709 - acc: 0.6614 - val_loss: 0.6258 - val_acc: 0.8476\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 798us/step - loss: 0.4380 - acc: 0.9086 - val_loss: 0.4669 - val_acc: 0.8915\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 705us/step - loss: 0.2870 - acc: 0.9682 - val_loss: 0.4391 - val_acc: 0.8891\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 713us/step - loss: 0.2232 - acc: 0.9881 - val_loss: 0.4446 - val_acc: 0.8891\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 727us/step - loss: 0.1941 - acc: 0.9970 - val_loss: 0.4226 - val_acc: 0.8868\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 736us/step - loss: 0.1772 - acc: 0.9960 - val_loss: 0.4077 - val_acc: 0.8891\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 720us/step - loss: 0.1673 - acc: 0.9950 - val_loss: 0.4217 - val_acc: 0.8961\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 693us/step - loss: 0.1556 - acc: 0.9970 - val_loss: 0.4280 - val_acc: 0.8868\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 711us/step - loss: 0.1615 - acc: 0.9950 - val_loss: 0.4308 - val_acc: 0.8799\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 697us/step - loss: 0.1561 - acc: 0.9990 - val_loss: 0.4547 - val_acc: 0.8845\n",
      "160/160 [==============================] - 0s 180us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_bow_model():\n",
    "  model = Sequential([\n",
    "      Dense(16, activation=relu, input_shape=(num_words,), kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "ff_bow_scores = run_cross_validate(get_ff_bow_model, bow_predictors, labels, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for our word vector method. First we must create our word vectors using a word vectorizing model generated in another experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load(\"opspam_w2v.kv\", mmap=\"r\")\n",
    "\n",
    "predictors_sequences = pad_sequences(tokenizer.texts_to_sequences(predictors_raw))\n",
    "max_sequence_length = max([len(x) for x in predictors_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = word_vectors.vector_size\n",
    "\n",
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1\n",
    "vectorizer_words = word_vectors.wv\n",
    "embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "for word, idx in corpus_words.items():\n",
    "  if word in vectorizer_words.vocab:\n",
    "    embedding_matrix[idx] = np.array(vectorizer_words[word], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.9209 - acc: 0.4826 - val_loss: 0.8241 - val_acc: 0.5058\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7935 - acc: 0.5283 - val_loss: 0.7679 - val_acc: 0.5820\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7564 - acc: 0.5591 - val_loss: 0.7637 - val_acc: 0.5912\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7345 - acc: 0.5809 - val_loss: 0.7491 - val_acc: 0.5843\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7040 - acc: 0.6207 - val_loss: 0.7282 - val_acc: 0.5543\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6990 - acc: 0.6246 - val_loss: 0.7280 - val_acc: 0.6143\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6904 - acc: 0.6157 - val_loss: 0.7217 - val_acc: 0.6212\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6781 - acc: 0.6375 - val_loss: 0.7192 - val_acc: 0.6467\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6694 - acc: 0.6743 - val_loss: 0.7458 - val_acc: 0.6028\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6700 - acc: 0.6713 - val_loss: 0.7373 - val_acc: 0.6189\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6504 - acc: 0.6862 - val_loss: 0.7777 - val_acc: 0.5820\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6576 - acc: 0.6783 - val_loss: 0.7460 - val_acc: 0.6005\n",
      "160/160 [==============================] - 0s 294us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 3ms/step - loss: 0.9513 - acc: 0.5084 - val_loss: 0.8599 - val_acc: 0.5081\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8459 - acc: 0.5204 - val_loss: 0.8152 - val_acc: 0.4873\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7894 - acc: 0.4965 - val_loss: 0.7690 - val_acc: 0.5219\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7602 - acc: 0.5303 - val_loss: 0.7565 - val_acc: 0.5196\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7510 - acc: 0.5611 - val_loss: 0.7703 - val_acc: 0.5681\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7588 - acc: 0.5879 - val_loss: 0.7839 - val_acc: 0.5312\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7422 - acc: 0.6097 - val_loss: 0.7811 - val_acc: 0.5035\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7296 - acc: 0.6346 - val_loss: 0.7702 - val_acc: 0.5520\n",
      "160/160 [==============================] - 0s 306us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.9359 - acc: 0.5144 - val_loss: 0.8700 - val_acc: 0.5612\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8265 - acc: 0.6157 - val_loss: 0.8473 - val_acc: 0.5704\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7612 - acc: 0.6683 - val_loss: 0.8640 - val_acc: 0.5381\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7338 - acc: 0.6733 - val_loss: 0.8086 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7032 - acc: 0.6922 - val_loss: 0.8274 - val_acc: 0.5982\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6995 - acc: 0.6842 - val_loss: 0.8140 - val_acc: 0.6097\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6837 - acc: 0.7160 - val_loss: 0.8143 - val_acc: 0.6443\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6660 - acc: 0.7398 - val_loss: 0.8751 - val_acc: 0.5704\n",
      "160/160 [==============================] - 0s 263us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8829 - acc: 0.5204 - val_loss: 0.8026 - val_acc: 0.5520\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7805 - acc: 0.5323 - val_loss: 0.7737 - val_acc: 0.5774\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7674 - acc: 0.5482 - val_loss: 0.7544 - val_acc: 0.5912\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7479 - acc: 0.5670 - val_loss: 0.7511 - val_acc: 0.5450\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7369 - acc: 0.5929 - val_loss: 0.7486 - val_acc: 0.5912\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7181 - acc: 0.6077 - val_loss: 0.7357 - val_acc: 0.5912\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6895 - acc: 0.6465 - val_loss: 0.7435 - val_acc: 0.5820\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6723 - acc: 0.6594 - val_loss: 0.7476 - val_acc: 0.6259\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6898 - acc: 0.6415 - val_loss: 0.7639 - val_acc: 0.6259\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6794 - acc: 0.6693 - val_loss: 0.7798 - val_acc: 0.5797\n",
      "160/160 [==============================] - 0s 335us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8804 - acc: 0.5114 - val_loss: 0.8206 - val_acc: 0.5173\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8086 - acc: 0.4955 - val_loss: 0.7912 - val_acc: 0.5289\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7808 - acc: 0.5134 - val_loss: 0.7772 - val_acc: 0.5751\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7533 - acc: 0.5432 - val_loss: 0.7505 - val_acc: 0.5658\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7456 - acc: 0.5482 - val_loss: 0.7338 - val_acc: 0.5958\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6944 - acc: 0.6216 - val_loss: 0.7413 - val_acc: 0.6051\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6902 - acc: 0.6495 - val_loss: 0.7583 - val_acc: 0.5958\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6705 - acc: 0.6604 - val_loss: 0.7632 - val_acc: 0.6443\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6630 - acc: 0.7021 - val_loss: 0.7724 - val_acc: 0.6628\n",
      "160/160 [==============================] - 0s 367us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8874 - acc: 0.5214 - val_loss: 0.8097 - val_acc: 0.5012\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7886 - acc: 0.5144 - val_loss: 0.7705 - val_acc: 0.5012\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7706 - acc: 0.5015 - val_loss: 0.7667 - val_acc: 0.5035\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7432 - acc: 0.5730 - val_loss: 0.7355 - val_acc: 0.5358\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7331 - acc: 0.5571 - val_loss: 0.7236 - val_acc: 0.5612\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7277 - acc: 0.5809 - val_loss: 0.7189 - val_acc: 0.5912\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7032 - acc: 0.5799 - val_loss: 0.7121 - val_acc: 0.6166\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7005 - acc: 0.5968 - val_loss: 0.7159 - val_acc: 0.5751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6993 - acc: 0.6127 - val_loss: 0.7485 - val_acc: 0.5150\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7053 - acc: 0.6117 - val_loss: 0.7538 - val_acc: 0.5866\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7019 - acc: 0.6256 - val_loss: 0.7142 - val_acc: 0.6097\n",
      "160/160 [==============================] - 0s 492us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8920 - acc: 0.5055 - val_loss: 0.8175 - val_acc: 0.4873\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7954 - acc: 0.5154 - val_loss: 0.7949 - val_acc: 0.5242\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7746 - acc: 0.5174 - val_loss: 0.7670 - val_acc: 0.5035\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7491 - acc: 0.5283 - val_loss: 0.7389 - val_acc: 0.5473\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7377 - acc: 0.5323 - val_loss: 0.7341 - val_acc: 0.5219\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7330 - acc: 0.5561 - val_loss: 0.7375 - val_acc: 0.5704\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7317 - acc: 0.5124 - val_loss: 0.7280 - val_acc: 0.5012\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7158 - acc: 0.5313 - val_loss: 0.7052 - val_acc: 0.5774\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7047 - acc: 0.5998 - val_loss: 0.7271 - val_acc: 0.5450\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7022 - acc: 0.6058 - val_loss: 0.7436 - val_acc: 0.5912\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7008 - acc: 0.6296 - val_loss: 0.7374 - val_acc: 0.5658\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6995 - acc: 0.6068 - val_loss: 0.7685 - val_acc: 0.5797\n",
      "160/160 [==============================] - 0s 350us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8891 - acc: 0.5164 - val_loss: 0.8056 - val_acc: 0.4919\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7825 - acc: 0.5045 - val_loss: 0.7710 - val_acc: 0.5196\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7575 - acc: 0.5641 - val_loss: 0.7588 - val_acc: 0.5566\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7454 - acc: 0.5710 - val_loss: 0.7511 - val_acc: 0.5704\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7276 - acc: 0.6087 - val_loss: 0.7639 - val_acc: 0.5127\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7200 - acc: 0.6197 - val_loss: 0.7439 - val_acc: 0.6212\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7128 - acc: 0.6246 - val_loss: 0.7711 - val_acc: 0.5335\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7019 - acc: 0.6385 - val_loss: 0.7634 - val_acc: 0.5473\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6985 - acc: 0.6455 - val_loss: 0.7276 - val_acc: 0.6328\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6783 - acc: 0.6683 - val_loss: 0.7609 - val_acc: 0.5797\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6801 - acc: 0.6663 - val_loss: 0.7424 - val_acc: 0.6143\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6818 - acc: 0.6763 - val_loss: 0.7433 - val_acc: 0.6282\n",
      "160/160 [==============================] - 0s 299us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.9225 - acc: 0.5074 - val_loss: 0.8355 - val_acc: 0.5058\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8099 - acc: 0.5462 - val_loss: 0.7916 - val_acc: 0.5912\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7566 - acc: 0.6197 - val_loss: 0.7677 - val_acc: 0.6074\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7362 - acc: 0.6296 - val_loss: 0.7969 - val_acc: 0.5335\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7294 - acc: 0.6326 - val_loss: 0.7467 - val_acc: 0.6467\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7109 - acc: 0.6683 - val_loss: 0.7903 - val_acc: 0.5589\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6913 - acc: 0.6753 - val_loss: 0.7964 - val_acc: 0.6236\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6834 - acc: 0.6902 - val_loss: 0.7506 - val_acc: 0.6582\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6810 - acc: 0.6931 - val_loss: 0.7542 - val_acc: 0.6536\n",
      "160/160 [==============================] - 0s 293us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8913 - acc: 0.5362 - val_loss: 0.8622 - val_acc: 0.4919\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7880 - acc: 0.5849 - val_loss: 0.7844 - val_acc: 0.5889\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7606 - acc: 0.6028 - val_loss: 0.7727 - val_acc: 0.6005\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7388 - acc: 0.6346 - val_loss: 0.7571 - val_acc: 0.5958\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7332 - acc: 0.6435 - val_loss: 0.7682 - val_acc: 0.6282\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7039 - acc: 0.6773 - val_loss: 0.7803 - val_acc: 0.6143\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7175 - acc: 0.6673 - val_loss: 0.7860 - val_acc: 0.5866\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7136 - acc: 0.6544 - val_loss: 0.7704 - val_acc: 0.6189\n",
      "160/160 [==============================] - 0s 338us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_wv_model():\n",
    "  model_ff_wv = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], trainable=False,\n",
    "                input_length=max_sequence_length),\n",
    "      Flatten(),\n",
    "      Dense(16, activation=relu, kernel_regularizer=l2(0.01)), #, input_shape=(num_words,)\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model_ff_wv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model_ff_wv\n",
    "\n",
    "ff_wv_scores = run_cross_validate(get_ff_wv_model, predictors_sequences, labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.875, 0.85625, 0.88125, 0.8875, 0.84375, 0.89375, 0.91875, 0.8375, 0.86875, 0.86875]\n",
      "Word vectors:  [0.625, 0.63125, 0.54375, 0.50625, 0.5625, 0.5875, 0.5625, 0.625, 0.54375, 0.55625]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", ff_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", ff_wv_scores['accuracies'])\n",
    "\n",
    "ff_scores_entries =[('Bag of Words', x) for x in ff_bow_scores['accuracies']] + [('Word Vectors', x) for x in ff_wv_scores['accuracies']]\n",
    "ff_scores_data_frame = DataFrame(ff_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=ff_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very strange! How could word vectors be less accurate than bag of words? This is known to occur when two conditions are met:\n",
    "    \n",
    "* The dataset is small\n",
    "* The dataset is very domain specific\n",
    "\n",
    "It is possible that these conditions actually are met here. The problem however is that running these models over our full dataset will take much longer, and will require a commited experiment to complete the investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network\n",
    "Now let's try this with a convolutional network. It has been shown that word vectors perform better for text classification than Bag of Words. If BoW is more accurate, it is a clear sign that we should investigate why. First we find the Bag of Words result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 20000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 20000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in bow_predictors]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_bow_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(1, 10),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(1, 20000, 1),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(pool_size=(1, 10)),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4525 - acc: 0.7706 - val_loss: 0.7189 - val_acc: 0.6259\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1434 - acc: 0.9503 - val_loss: 0.8025 - val_acc: 0.6259\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0719 - acc: 0.9821 - val_loss: 0.8610 - val_acc: 0.6305\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0380 - acc: 0.9930 - val_loss: 0.9733 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0228 - acc: 0.9950 - val_loss: 1.0837 - val_acc: 0.5982\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4295 - acc: 0.7954 - val_loss: 0.6546 - val_acc: 0.6582\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1265 - acc: 0.9652 - val_loss: 0.8005 - val_acc: 0.6074\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0687 - acc: 0.9841 - val_loss: 0.9633 - val_acc: 0.5958\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0367 - acc: 0.9940 - val_loss: 1.0671 - val_acc: 0.6189\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0216 - acc: 0.9990 - val_loss: 1.2579 - val_acc: 0.6028\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4153 - acc: 0.7954 - val_loss: 0.6898 - val_acc: 0.6467\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1224 - acc: 0.9623 - val_loss: 0.7831 - val_acc: 0.6605\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0698 - acc: 0.9841 - val_loss: 0.9158 - val_acc: 0.6397\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0381 - acc: 0.9901 - val_loss: 1.2487 - val_acc: 0.5935\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0204 - acc: 0.9980 - val_loss: 1.1479 - val_acc: 0.6236\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4271 - acc: 0.7984 - val_loss: 0.6573 - val_acc: 0.6420\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1186 - acc: 0.9573 - val_loss: 0.7588 - val_acc: 0.6328\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0633 - acc: 0.9871 - val_loss: 0.8871 - val_acc: 0.6166\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0381 - acc: 0.9950 - val_loss: 1.0951 - val_acc: 0.6097\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0237 - acc: 0.9960 - val_loss: 1.1573 - val_acc: 0.6374\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.6735 - val_acc: 0.6559\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.1425 - acc: 0.9484 - val_loss: 0.8690 - val_acc: 0.6351\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0864 - acc: 0.9752 - val_loss: 1.0139 - val_acc: 0.6097\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.0509 - acc: 0.9831 - val_loss: 0.9867 - val_acc: 0.6559\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0265 - acc: 0.9970 - val_loss: 1.0695 - val_acc: 0.6467\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.4488 - acc: 0.7736 - val_loss: 0.8667 - val_acc: 0.5797\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.1469 - acc: 0.9434 - val_loss: 0.7677 - val_acc: 0.6490\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0669 - acc: 0.9811 - val_loss: 0.8936 - val_acc: 0.6374\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.0368 - acc: 0.9930 - val_loss: 1.0227 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.0209 - acc: 0.9970 - val_loss: 1.1195 - val_acc: 0.6374\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0119 - acc: 1.0000 - val_loss: 1.2304 - val_acc: 0.6282\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4517 - acc: 0.7676 - val_loss: 0.6754 - val_acc: 0.6536\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1274 - acc: 0.9583 - val_loss: 0.7747 - val_acc: 0.6351\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0721 - acc: 0.9801 - val_loss: 0.9884 - val_acc: 0.5982\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0403 - acc: 0.9891 - val_loss: 1.0505 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0243 - acc: 0.9970 - val_loss: 1.2137 - val_acc: 0.6097\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4204 - acc: 0.7895 - val_loss: 0.6661 - val_acc: 0.6697\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1165 - acc: 0.9692 - val_loss: 0.8360 - val_acc: 0.6443\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0615 - acc: 0.9841 - val_loss: 0.9606 - val_acc: 0.6236\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0316 - acc: 0.9950 - val_loss: 1.1116 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 16s 15ms/step - loss: 0.0194 - acc: 0.9980 - val_loss: 1.2052 - val_acc: 0.6189\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4132 - acc: 0.8113 - val_loss: 0.6772 - val_acc: 0.6582\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1201 - acc: 0.9583 - val_loss: 0.8018 - val_acc: 0.6605\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0627 - acc: 0.9851 - val_loss: 0.9226 - val_acc: 0.6212\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 20s 20ms/step - loss: 0.0344 - acc: 0.9960 - val_loss: 1.0623 - val_acc: 0.6143\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0190 - acc: 0.9990 - val_loss: 1.1779 - val_acc: 0.6212\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4178 - acc: 0.7934 - val_loss: 0.6608 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.1207 - acc: 0.9603 - val_loss: 0.7403 - val_acc: 0.6420\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0682 - acc: 0.9891 - val_loss: 0.8421 - val_acc: 0.6513\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0426 - acc: 0.9871 - val_loss: 0.9830 - val_acc: 0.6397\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0267 - acc: 0.9960 - val_loss: 1.1546 - val_acc: 0.6328\n",
      "160/160 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "conv_bow_scores = run_cross_validate(get_conv_bow_model, convolutional_data, labels, cv=10, categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our word vector result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_review(review_words):\n",
    "  sentence = []\n",
    "  for word in review_words:\n",
    "    if word in word_vectors.wv:\n",
    "      sentence.append(word_vectors.wv[word])\n",
    "  return np.array(sentence, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vectorized_review(vectorized_review, length):\n",
    "  return np.concatenate((vectorized_review, np.zeros((length - len(vectorized_review), 100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = [vectorize_review(text_to_word_sequence(x)) for x in predictors_raw]\n",
    "pad_length = max([x.shape[0] for x in vectorized_reviews])\n",
    "vectorized_reviews = np.array([[pad_vectorized_review(x, pad_length)] for x in vectorized_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 1, 381, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 100),\n",
    "          data_format=\"channels_first\",\n",
    "          input_shape=(1, 381, 100),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(strides=(1, 1), pool_size=(2, 1), data_format=\"channels_first\"),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1332, 1, 381, 100) labels (1332, 2)\n",
      "Train on 932 samples, validate on 400 samples\n",
      "Epoch 1/12\n",
      "932/932 [==============================] - 5s 6ms/step - loss: 0.6736 - acc: 0.5687 - val_loss: 0.6186 - val_acc: 0.6875\n",
      "Epoch 2/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.5533 - acc: 0.7285 - val_loss: 0.6039 - val_acc: 0.6650\n",
      "Epoch 3/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.5469 - acc: 0.7371 - val_loss: 0.5884 - val_acc: 0.6600\n",
      "Epoch 4/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4892 - acc: 0.7607 - val_loss: 0.5220 - val_acc: 0.7400\n",
      "Epoch 5/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4605 - acc: 0.8004 - val_loss: 0.5252 - val_acc: 0.7225\n",
      "Epoch 6/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4501 - acc: 0.7876 - val_loss: 0.5201 - val_acc: 0.7225\n",
      "Epoch 7/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4068 - acc: 0.8176 - val_loss: 0.5379 - val_acc: 0.7400\n",
      "Epoch 8/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4014 - acc: 0.8262 - val_loss: 0.5814 - val_acc: 0.7125\n",
      "Epoch 9/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.3832 - acc: 0.8305 - val_loss: 0.5486 - val_acc: 0.7350\n",
      "Epoch 10/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.3719 - acc: 0.8348 - val_loss: 0.5507 - val_acc: 0.7400\n",
      "268/268 [==============================] - 0s 2ms/step\n",
      "Fitting with:  (1332, 1, 381, 100) labels (1332, 2)\n",
      "Train on 932 samples, validate on 400 samples\n",
      "Epoch 1/12\n",
      "932/932 [==============================] - 5s 6ms/step - loss: 0.6826 - acc: 0.5504 - val_loss: 0.6569 - val_acc: 0.6350\n",
      "Epoch 2/12\n",
      "932/932 [==============================] - 5s 5ms/step - loss: 0.6034 - acc: 0.6813 - val_loss: 0.5816 - val_acc: 0.6925\n",
      "Epoch 3/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.5391 - acc: 0.7146 - val_loss: 0.5647 - val_acc: 0.6975\n",
      "Epoch 4/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4797 - acc: 0.7940 - val_loss: 0.5256 - val_acc: 0.7300\n",
      "Epoch 5/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4322 - acc: 0.8058 - val_loss: 0.5264 - val_acc: 0.7375\n",
      "Epoch 6/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.4215 - acc: 0.8101 - val_loss: 0.5640 - val_acc: 0.7075\n",
      "Epoch 7/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.3933 - acc: 0.8187 - val_loss: 0.5492 - val_acc: 0.7225\n",
      "Epoch 8/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.3808 - acc: 0.8391 - val_loss: 0.5876 - val_acc: 0.7050\n",
      "268/268 [==============================] - 0s 2ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 6ms/step - loss: 0.6728 - acc: 0.5970 - val_loss: 0.6184 - val_acc: 0.6658\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.5876 - acc: 0.6860 - val_loss: 0.6067 - val_acc: 0.6459\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.5214 - acc: 0.7503 - val_loss: 0.5497 - val_acc: 0.6908\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4900 - acc: 0.7696 - val_loss: 0.5644 - val_acc: 0.7057\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4521 - acc: 0.7996 - val_loss: 0.5432 - val_acc: 0.7332\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4335 - acc: 0.8103 - val_loss: 0.5783 - val_acc: 0.6883\n",
      "Epoch 7/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4083 - acc: 0.8156 - val_loss: 0.5450 - val_acc: 0.7357\n",
      "Epoch 8/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.3916 - acc: 0.8296 - val_loss: 0.5717 - val_acc: 0.7307\n",
      "Epoch 9/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.3945 - acc: 0.8199 - val_loss: 0.5624 - val_acc: 0.7282\n",
      "266/266 [==============================] - 1s 2ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 7ms/step - loss: 0.6910 - acc: 0.5391 - val_loss: 0.6610 - val_acc: 0.6259\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6234 - acc: 0.6677 - val_loss: 0.6157 - val_acc: 0.6359\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.5613 - acc: 0.7203 - val_loss: 0.5796 - val_acc: 0.6484\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.5166 - acc: 0.7460 - val_loss: 0.5634 - val_acc: 0.6833\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4986 - acc: 0.7685 - val_loss: 0.5736 - val_acc: 0.6733\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4742 - acc: 0.7760 - val_loss: 0.5913 - val_acc: 0.6708\n",
      "Epoch 7/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4467 - acc: 0.7985 - val_loss: 0.5631 - val_acc: 0.6983\n",
      "Epoch 8/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.4415 - acc: 0.7996 - val_loss: 0.5674 - val_acc: 0.7257\n",
      "Epoch 9/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4184 - acc: 0.8114 - val_loss: 0.6409 - val_acc: 0.6633\n",
      "Epoch 10/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4153 - acc: 0.8071 - val_loss: 0.5608 - val_acc: 0.7182\n",
      "Epoch 11/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.3945 - acc: 0.8328 - val_loss: 0.5725 - val_acc: 0.7232\n",
      "Epoch 12/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.3833 - acc: 0.8285 - val_loss: 0.5850 - val_acc: 0.7082\n",
      "266/266 [==============================] - 0s 2ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 6ms/step - loss: 0.6937 - acc: 0.5348 - val_loss: 0.6664 - val_acc: 0.6085\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6317 - acc: 0.6431 - val_loss: 0.6286 - val_acc: 0.6284\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.5699 - acc: 0.6902 - val_loss: 0.6286 - val_acc: 0.6783\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.5411 - acc: 0.7288 - val_loss: 0.6082 - val_acc: 0.6559\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.5088 - acc: 0.7567 - val_loss: 0.5550 - val_acc: 0.7132\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4694 - acc: 0.7781 - val_loss: 0.5609 - val_acc: 0.7157\n",
      "Epoch 7/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4545 - acc: 0.7803 - val_loss: 0.6294 - val_acc: 0.6758\n",
      "Epoch 8/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4272 - acc: 0.7942 - val_loss: 0.5641 - val_acc: 0.7182\n",
      "Epoch 9/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.4139 - acc: 0.7996 - val_loss: 0.5681 - val_acc: 0.7182\n",
      "266/266 [==============================] - 1s 2ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 7ms/step - loss: 0.6613 - acc: 0.5852 - val_loss: 0.6191 - val_acc: 0.6359\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.5564 - acc: 0.7331 - val_loss: 0.5713 - val_acc: 0.6933\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.5411 - acc: 0.7310 - val_loss: 0.5854 - val_acc: 0.6633\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.4996 - acc: 0.7503 - val_loss: 0.5911 - val_acc: 0.6858\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.4792 - acc: 0.7696 - val_loss: 0.5582 - val_acc: 0.6933\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.4481 - acc: 0.7899 - val_loss: 0.5821 - val_acc: 0.6883\n",
      "Epoch 7/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.4252 - acc: 0.8124 - val_loss: 0.5783 - val_acc: 0.7007\n",
      "Epoch 8/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.4063 - acc: 0.8006 - val_loss: 0.5684 - val_acc: 0.6983\n",
      "Epoch 9/12\n",
      "933/933 [==============================] - 4s 4ms/step - loss: 0.3858 - acc: 0.8349 - val_loss: 0.5897 - val_acc: 0.6858\n",
      "266/266 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, vectorized_reviews, labels, cv=6, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.8375, 0.85, 0.825, 0.74375, 0.8125, 0.8125, 0.7625, 0.78125, 0.825, 0.84375]\n",
      "Word vectors:  [0.7276119394088859, 0.7388059710388752, 0.7368421057113131, 0.7706766921774786, 0.7518796987999651, 0.7593984962406015]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", conv_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", conv_wv_scores['accuracies'])\n",
    "\n",
    "conv_scores_entries =[('Bag of Words', x) for x in conv_bow_scores['accuracies']] + [('Word Vectors', x) for x in conv_wv_scores['accuracies']]\n",
    "conv_scores_data_frame = DataFrame(conv_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=conv_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before our results are alarming. We need to investigate why this can occur, it may be that we have made a mistake in generating our word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "Let's also try this same experiment for Recurrent Neural Networks. We will use LSTM since this is known to be a good option for text classification. First with Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 20000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "rnn_bow_data = np.array(np.split(bow_predictors, batches))\n",
    "rnn_bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_bow_targets = np.array([[x] for x in labels])\n",
    "rnn_bow_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.5428 - acc: 0.7637 - val_loss: 0.3637 - val_acc: 0.8799\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1343 - acc: 0.9861 - val_loss: 0.3267 - val_acc: 0.8753\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0618 - acc: 0.9990 - val_loss: 0.3146 - val_acc: 0.8891\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.3132 - val_acc: 0.8915\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0288 - acc: 1.0000 - val_loss: 0.3127 - val_acc: 0.8868\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.3150 - val_acc: 0.8868\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.3180 - val_acc: 0.8891\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.3211 - val_acc: 0.8938\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.3256 - val_acc: 0.8938\n",
      "160/160 [==============================] - 0s 237us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.5319 - acc: 0.7706 - val_loss: 0.3679 - val_acc: 0.8730\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1252 - acc: 0.9861 - val_loss: 0.3113 - val_acc: 0.8799\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0609 - acc: 0.9970 - val_loss: 0.3081 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.3008 - val_acc: 0.8891\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.0281 - acc: 1.0000 - val_loss: 0.2969 - val_acc: 0.8915\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.2971 - val_acc: 0.8891\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2988 - val_acc: 0.8961\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.3019 - val_acc: 0.8984\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 1ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.3039 - val_acc: 0.8984\n",
      "160/160 [==============================] - 0s 229us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.5196 - acc: 0.7607 - val_loss: 0.3464 - val_acc: 0.8915\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1028 - acc: 0.9791 - val_loss: 0.3072 - val_acc: 0.8938\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0453 - acc: 0.9990 - val_loss: 0.3003 - val_acc: 0.8915\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.8891\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.3056 - val_acc: 0.8868\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.3105 - val_acc: 0.8845\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.3130 - val_acc: 0.8845\n",
      "160/160 [==============================] - 0s 224us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.5540 - acc: 0.7686 - val_loss: 0.4088 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1803 - acc: 0.9772 - val_loss: 0.3416 - val_acc: 0.8776\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0933 - acc: 0.9980 - val_loss: 0.3257 - val_acc: 0.8684\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0619 - acc: 0.9990 - val_loss: 0.3187 - val_acc: 0.8637\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0456 - acc: 1.0000 - val_loss: 0.3161 - val_acc: 0.8661\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.3161 - val_acc: 0.8614\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0287 - acc: 1.0000 - val_loss: 0.3183 - val_acc: 0.8637\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 0.8637\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.3231 - val_acc: 0.8637\n",
      "160/160 [==============================] - 0s 183us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.5290 - acc: 0.7696 - val_loss: 0.3542 - val_acc: 0.8822\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1171 - acc: 0.9861 - val_loss: 0.3179 - val_acc: 0.8707\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0541 - acc: 0.9990 - val_loss: 0.3179 - val_acc: 0.8753\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0348 - acc: 1.0000 - val_loss: 0.3170 - val_acc: 0.8684\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.3195 - val_acc: 0.8684\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.3243 - val_acc: 0.8684\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.3283 - val_acc: 0.8661\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.3334 - val_acc: 0.8684\n",
      "160/160 [==============================] - 0s 214us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.5361 - acc: 0.7537 - val_loss: 0.3424 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1170 - acc: 0.9811 - val_loss: 0.3131 - val_acc: 0.8776\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0500 - acc: 0.9990 - val_loss: 0.3046 - val_acc: 0.8915\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0318 - acc: 1.0000 - val_loss: 0.3040 - val_acc: 0.8891\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.3056 - val_acc: 0.8915\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.3096 - val_acc: 0.8915\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.8891\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.3174 - val_acc: 0.8891\n",
      "160/160 [==============================] - 0s 239us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.5679 - acc: 0.7428 - val_loss: 0.4064 - val_acc: 0.8684\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.1829 - acc: 0.9762 - val_loss: 0.3367 - val_acc: 0.8799\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0931 - acc: 0.9990 - val_loss: 0.3038 - val_acc: 0.8915\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0615 - acc: 0.9990 - val_loss: 0.2955 - val_acc: 0.8915\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0450 - acc: 1.0000 - val_loss: 0.2853 - val_acc: 0.8961\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.8984\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0282 - acc: 1.0000 - val_loss: 0.2824 - val_acc: 0.8961\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.2832 - val_acc: 0.8961\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.8984\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.2855 - val_acc: 0.8938\n",
      "160/160 [==============================] - 0s 230us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 4ms/step - loss: 0.5634 - acc: 0.7458 - val_loss: 0.4024 - val_acc: 0.8684\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1787 - acc: 0.9821 - val_loss: 0.3468 - val_acc: 0.8915\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0907 - acc: 0.9980 - val_loss: 0.3273 - val_acc: 0.8845\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0602 - acc: 1.0000 - val_loss: 0.3214 - val_acc: 0.8776\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0443 - acc: 1.0000 - val_loss: 0.3196 - val_acc: 0.8776\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 0.8730\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0278 - acc: 1.0000 - val_loss: 0.3222 - val_acc: 0.8776\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.3260 - val_acc: 0.8776\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.3297 - val_acc: 0.8753\n",
      "160/160 [==============================] - 0s 198us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.5229 - acc: 0.7746 - val_loss: 0.3663 - val_acc: 0.8661\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1295 - acc: 0.9821 - val_loss: 0.3030 - val_acc: 0.9007\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0640 - acc: 0.9970 - val_loss: 0.3018 - val_acc: 0.8938\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0416 - acc: 1.0000 - val_loss: 0.2985 - val_acc: 0.8891\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0301 - acc: 1.0000 - val_loss: 0.2996 - val_acc: 0.8915\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.3038 - val_acc: 0.8868\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.3071 - val_acc: 0.8915\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.3115 - val_acc: 0.8915\n",
      "160/160 [==============================] - 0s 209us/step\n",
      "Fitting with:  (1440, 1, 20000) labels (1440, 1)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.5431 - acc: 0.7468 - val_loss: 0.3233 - val_acc: 0.8961\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1069 - acc: 0.9821 - val_loss: 0.3107 - val_acc: 0.8684\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0441 - acc: 0.9990 - val_loss: 0.2851 - val_acc: 0.8845\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0272 - acc: 1.0000 - val_loss: 0.2836 - val_acc: 0.8961\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.2866 - val_acc: 0.8868\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 0.8915\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.2920 - val_acc: 0.8915\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.2961 - val_acc: 0.8915\n",
      "160/160 [==============================] - 0s 196us/step\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_bow_model():\n",
    "  model = Sequential([\n",
    "      LSTM(8, input_shape=(1, 20000)),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "rnn_bow_scores = run_cross_validate(get_rnn_bow_model, rnn_bow_data, rnn_bow_targets, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with Word Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 62s 62ms/step - loss: 0.6888 - acc: 0.5402 - val_loss: 0.6834 - val_acc: 0.5404\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 57s 57ms/step - loss: 0.6733 - acc: 0.5909 - val_loss: 0.6546 - val_acc: 0.6236\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 56s 56ms/step - loss: 0.6554 - acc: 0.6038 - val_loss: 0.6469 - val_acc: 0.6189\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 60s 59ms/step - loss: 0.6487 - acc: 0.6187 - val_loss: 0.6443 - val_acc: 0.6259\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 58s 58ms/step - loss: 0.6480 - acc: 0.6137 - val_loss: 0.6396 - val_acc: 0.6351\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 61s 60ms/step - loss: 0.6374 - acc: 0.6266 - val_loss: 0.6375 - val_acc: 0.6397\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 62s 62ms/step - loss: 0.6345 - acc: 0.6316 - val_loss: 0.6285 - val_acc: 0.6513\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 58s 57ms/step - loss: 0.6259 - acc: 0.6455 - val_loss: 0.6333 - val_acc: 0.6420\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 64s 63ms/step - loss: 0.6282 - acc: 0.6445 - val_loss: 0.6301 - val_acc: 0.6582\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 65s 65ms/step - loss: 0.6379 - acc: 0.6256 - val_loss: 0.6290 - val_acc: 0.6513\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 58s 58ms/step - loss: 0.6169 - acc: 0.6465 - val_loss: 0.6272 - val_acc: 0.6467\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 58s 58ms/step - loss: 0.6149 - acc: 0.6534 - val_loss: 0.6222 - val_acc: 0.6605\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 67s 67ms/step - loss: 0.6884 - acc: 0.5353 - val_loss: 0.6811 - val_acc: 0.6005\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 60s 59ms/step - loss: 0.6717 - acc: 0.5968 - val_loss: 0.6534 - val_acc: 0.6189\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6515 - acc: 0.6157 - val_loss: 0.6452 - val_acc: 0.6189\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 64s 63ms/step - loss: 0.6432 - acc: 0.6226 - val_loss: 0.6378 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6430 - acc: 0.6157 - val_loss: 0.6500 - val_acc: 0.5889\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 91s 90ms/step - loss: 0.6342 - acc: 0.6326 - val_loss: 0.6359 - val_acc: 0.6097\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 65s 64ms/step - loss: 0.6276 - acc: 0.6395 - val_loss: 0.6327 - val_acc: 0.6536\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 67s 66ms/step - loss: 0.6332 - acc: 0.6346 - val_loss: 0.6309 - val_acc: 0.6420\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 71s 71ms/step - loss: 0.6214 - acc: 0.6316 - val_loss: 0.6358 - val_acc: 0.6166\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 72s 72ms/step - loss: 0.6120 - acc: 0.6524 - val_loss: 0.6467 - val_acc: 0.6420\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 70s 70ms/step - loss: 0.6185 - acc: 0.6524 - val_loss: 0.6225 - val_acc: 0.6559\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 71s 70ms/step - loss: 0.6102 - acc: 0.6475 - val_loss: 0.6225 - val_acc: 0.6467\n",
      "160/160 [==============================] - 1s 3ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 71s 70ms/step - loss: 0.6903 - acc: 0.5223 - val_loss: 0.6819 - val_acc: 0.5196\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 75s 74ms/step - loss: 0.6711 - acc: 0.5869 - val_loss: 0.6466 - val_acc: 0.6236\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 69s 69ms/step - loss: 0.6464 - acc: 0.6177 - val_loss: 0.6375 - val_acc: 0.6328\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 70s 69ms/step - loss: 0.6456 - acc: 0.6316 - val_loss: 0.6343 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 66s 66ms/step - loss: 0.6339 - acc: 0.6236 - val_loss: 0.6691 - val_acc: 0.5982\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 65s 65ms/step - loss: 0.6251 - acc: 0.6504 - val_loss: 0.6209 - val_acc: 0.6721\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 70s 69ms/step - loss: 0.6196 - acc: 0.6405 - val_loss: 0.6202 - val_acc: 0.6674\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 63s 63ms/step - loss: 0.6187 - acc: 0.6624 - val_loss: 0.6122 - val_acc: 0.6582\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 83s 82ms/step - loss: 0.6152 - acc: 0.6673 - val_loss: 0.6368 - val_acc: 0.6420\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 75s 74ms/step - loss: 0.6272 - acc: 0.6524 - val_loss: 0.6318 - val_acc: 0.6259\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 75s 74ms/step - loss: 0.6005 - acc: 0.6713 - val_loss: 0.6140 - val_acc: 0.6628\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 85s 84ms/step - loss: 0.5985 - acc: 0.6842 - val_loss: 0.6047 - val_acc: 0.6836\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 88s 87ms/step - loss: 0.6866 - acc: 0.5362 - val_loss: 0.6726 - val_acc: 0.5635\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 83s 83ms/step - loss: 0.6628 - acc: 0.5988 - val_loss: 0.6554 - val_acc: 0.6051\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 71s 70ms/step - loss: 0.6488 - acc: 0.6087 - val_loss: 0.6522 - val_acc: 0.5935\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 75s 75ms/step - loss: 0.6505 - acc: 0.6077 - val_loss: 0.6587 - val_acc: 0.5958\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 71s 70ms/step - loss: 0.6428 - acc: 0.6266 - val_loss: 0.6429 - val_acc: 0.6051\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 67s 67ms/step - loss: 0.6415 - acc: 0.6276 - val_loss: 0.6421 - val_acc: 0.6074\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 68s 68ms/step - loss: 0.6374 - acc: 0.6286 - val_loss: 0.6412 - val_acc: 0.6236\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 71s 70ms/step - loss: 0.6351 - acc: 0.6266 - val_loss: 0.6380 - val_acc: 0.6305\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 64s 63ms/step - loss: 0.6293 - acc: 0.6346 - val_loss: 0.6298 - val_acc: 0.6328\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 60s 60ms/step - loss: 0.6218 - acc: 0.6425 - val_loss: 0.6497 - val_acc: 0.6143\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 66s 66ms/step - loss: 0.6171 - acc: 0.6524 - val_loss: 0.6061 - val_acc: 0.6767\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 72s 72ms/step - loss: 0.6143 - acc: 0.6524 - val_loss: 0.6134 - val_acc: 0.6628\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 82s 81ms/step - loss: 0.6987 - acc: 0.4826 - val_loss: 0.6887 - val_acc: 0.5727\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 67s 67ms/step - loss: 0.6840 - acc: 0.5770 - val_loss: 0.6715 - val_acc: 0.6028\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 67s 66ms/step - loss: 0.6641 - acc: 0.6018 - val_loss: 0.6541 - val_acc: 0.6005\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 66s 66ms/step - loss: 0.6446 - acc: 0.6266 - val_loss: 0.6315 - val_acc: 0.6490\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 67s 67ms/step - loss: 0.6442 - acc: 0.6137 - val_loss: 0.6301 - val_acc: 0.6420\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6468 - acc: 0.6216 - val_loss: 0.6516 - val_acc: 0.6328\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6385 - acc: 0.6216 - val_loss: 0.6304 - val_acc: 0.6328\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 65s 64ms/step - loss: 0.6331 - acc: 0.6485 - val_loss: 0.6256 - val_acc: 0.6582\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 65s 64ms/step - loss: 0.6318 - acc: 0.6435 - val_loss: 0.6302 - val_acc: 0.6282\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 66s 65ms/step - loss: 0.6230 - acc: 0.6455 - val_loss: 0.6141 - val_acc: 0.6697\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6309 - acc: 0.6475 - val_loss: 0.6288 - val_acc: 0.6374\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 65s 64ms/step - loss: 0.6283 - acc: 0.6395 - val_loss: 0.6318 - val_acc: 0.6236\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 69s 68ms/step - loss: 0.6893 - acc: 0.5392 - val_loss: 0.6756 - val_acc: 0.6005\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 65s 65ms/step - loss: 0.6760 - acc: 0.5859 - val_loss: 0.6496 - val_acc: 0.6212\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6484 - acc: 0.6127 - val_loss: 0.6219 - val_acc: 0.6490\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 63s 62ms/step - loss: 0.6364 - acc: 0.6445 - val_loss: 0.6187 - val_acc: 0.6536\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 61s 61ms/step - loss: 0.6387 - acc: 0.6286 - val_loss: 0.6261 - val_acc: 0.6212\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 61s 61ms/step - loss: 0.6376 - acc: 0.6375 - val_loss: 0.6137 - val_acc: 0.6490\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 62s 61ms/step - loss: 0.6285 - acc: 0.6385 - val_loss: 0.6234 - val_acc: 0.6513\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 63s 63ms/step - loss: 0.6304 - acc: 0.6455 - val_loss: 0.6402 - val_acc: 0.6120\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 63s 63ms/step - loss: 0.6238 - acc: 0.6326 - val_loss: 0.6097 - val_acc: 0.6721\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 67s 67ms/step - loss: 0.6117 - acc: 0.6554 - val_loss: 0.6064 - val_acc: 0.6697\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 69s 68ms/step - loss: 0.6045 - acc: 0.6634 - val_loss: 0.6313 - val_acc: 0.6328\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 64s 64ms/step - loss: 0.6021 - acc: 0.6643 - val_loss: 0.5917 - val_acc: 0.6790\n",
      "160/160 [==============================] - 0s 2ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 72s 71ms/step - loss: 0.6910 - acc: 0.5194 - val_loss: 0.6821 - val_acc: 0.5635\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 68s 67ms/step - loss: 0.6755 - acc: 0.5899 - val_loss: 0.6486 - val_acc: 0.6189\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 59s 59ms/step - loss: 0.6484 - acc: 0.6356 - val_loss: 0.6686 - val_acc: 0.5982\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 61s 61ms/step - loss: 0.6367 - acc: 0.6375 - val_loss: 0.6529 - val_acc: 0.5912\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 66s 65ms/step - loss: 0.6339 - acc: 0.6524 - val_loss: 0.6297 - val_acc: 0.6513\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 63s 63ms/step - loss: 0.6282 - acc: 0.6415 - val_loss: 0.6270 - val_acc: 0.6420\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 64s 63ms/step - loss: 0.6245 - acc: 0.6465 - val_loss: 0.6343 - val_acc: 0.6490\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 59s 58ms/step - loss: 0.6247 - acc: 0.6564 - val_loss: 0.6331 - val_acc: 0.6536\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 57s 56ms/step - loss: 0.6143 - acc: 0.6673 - val_loss: 0.6305 - val_acc: 0.6605\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 72s 72ms/step - loss: 0.6135 - acc: 0.6653 - val_loss: 0.6333 - val_acc: 0.6374\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 96s 95ms/step - loss: 0.6945 - acc: 0.5134 - val_loss: 0.6855 - val_acc: 0.5820\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 80s 80ms/step - loss: 0.6778 - acc: 0.5770 - val_loss: 0.6553 - val_acc: 0.6212\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 59s 59ms/step - loss: 0.6453 - acc: 0.6147 - val_loss: 0.6244 - val_acc: 0.6536\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 65s 64ms/step - loss: 0.6669 - acc: 0.6068 - val_loss: 0.6432 - val_acc: 0.6212\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 82s 82ms/step - loss: 0.6385 - acc: 0.6276 - val_loss: 0.6249 - val_acc: 0.6443\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 83s 82ms/step - loss: 0.6359 - acc: 0.6266 - val_loss: 0.6222 - val_acc: 0.6628\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 85s 84ms/step - loss: 0.6291 - acc: 0.6346 - val_loss: 0.6182 - val_acc: 0.6744\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 88s 88ms/step - loss: 0.6293 - acc: 0.6425 - val_loss: 0.6231 - val_acc: 0.6582\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 85s 84ms/step - loss: 0.6234 - acc: 0.6266 - val_loss: 0.6308 - val_acc: 0.6236\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 81s 80ms/step - loss: 0.6269 - acc: 0.6336 - val_loss: 0.6256 - val_acc: 0.6282\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 83s 82ms/step - loss: 0.6180 - acc: 0.6435 - val_loss: 0.6045 - val_acc: 0.6767\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 78s 78ms/step - loss: 0.6172 - acc: 0.6455 - val_loss: 0.5987 - val_acc: 0.6651\n",
      "160/160 [==============================] - 0s 2ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 75s 75ms/step - loss: 0.6898 - acc: 0.5392 - val_loss: 0.6768 - val_acc: 0.6120\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 67s 66ms/step - loss: 0.6667 - acc: 0.5958 - val_loss: 0.6455 - val_acc: 0.6143\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 66s 65ms/step - loss: 0.6447 - acc: 0.6028 - val_loss: 0.6323 - val_acc: 0.6282\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 72s 71ms/step - loss: 0.6346 - acc: 0.6286 - val_loss: 0.6324 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 79s 79ms/step - loss: 0.6277 - acc: 0.6296 - val_loss: 0.6252 - val_acc: 0.6374\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 76s 76ms/step - loss: 0.6265 - acc: 0.6365 - val_loss: 0.6373 - val_acc: 0.6328\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 68s 67ms/step - loss: 0.6225 - acc: 0.6336 - val_loss: 0.6263 - val_acc: 0.6513\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 83s 83ms/step - loss: 0.6069 - acc: 0.6435 - val_loss: 0.6160 - val_acc: 0.6674\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 74s 73ms/step - loss: 0.6150 - acc: 0.6495 - val_loss: 0.6174 - val_acc: 0.6236\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 71s 71ms/step - loss: 0.6167 - acc: 0.6395 - val_loss: 0.6224 - val_acc: 0.6490\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 74s 73ms/step - loss: 0.6047 - acc: 0.6733 - val_loss: 0.6486 - val_acc: 0.6120\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 69s 68ms/step - loss: 0.6026 - acc: 0.6624 - val_loss: 0.6058 - val_acc: 0.6559\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 81s 80ms/step - loss: 0.6927 - acc: 0.5074 - val_loss: 0.6830 - val_acc: 0.5820\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 79s 78ms/step - loss: 0.6829 - acc: 0.5591 - val_loss: 0.6628 - val_acc: 0.6212\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 74s 74ms/step - loss: 0.6681 - acc: 0.5958 - val_loss: 0.6378 - val_acc: 0.6536\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 73s 72ms/step - loss: 0.6509 - acc: 0.6236 - val_loss: 0.6282 - val_acc: 0.6443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 66s 65ms/step - loss: 0.6414 - acc: 0.6207 - val_loss: 0.6306 - val_acc: 0.6374\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 77s 77ms/step - loss: 0.6386 - acc: 0.6286 - val_loss: 0.6189 - val_acc: 0.6420\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 80s 79ms/step - loss: 0.6361 - acc: 0.6395 - val_loss: 0.6182 - val_acc: 0.6513\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 81s 80ms/step - loss: 0.6230 - acc: 0.6614 - val_loss: 0.6216 - val_acc: 0.6282\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 78s 78ms/step - loss: 0.6183 - acc: 0.6475 - val_loss: 0.6271 - val_acc: 0.6536\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 78s 77ms/step - loss: 0.6245 - acc: 0.6475 - val_loss: 0.6002 - val_acc: 0.6697\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 82s 81ms/step - loss: 0.6083 - acc: 0.6683 - val_loss: 0.5864 - val_acc: 0.6813\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 78s 78ms/step - loss: 0.6335 - acc: 0.6187 - val_loss: 0.6513 - val_acc: 0.5958\n",
      "160/160 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_wv_model():\n",
    "  model = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], input_length=max_sequence_length,\n",
    "                trainable=False),\n",
    "      LSTM(8),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "rnn_wv_scores = run_cross_validate(get_rnn_wv_model, predictors_sequences, labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.89375, 0.86875, 0.88125, 0.89375, 0.86875, 0.875, 0.8875, 0.9125, 0.85625, 0.86875]\n",
      "Word vectors:  [0.7125, 0.65, 0.68125, 0.68125, 0.6375, 0.6, 0.59375, 0.74375, 0.625, 0.59375]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", rnn_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", rnn_wv_scores['accuracies'])\n",
    "\n",
    "rnn_scores_entries =[('Bag of Words', x) for x in rnn_bow_scores['accuracies']] + [('Word Vectors', x) for x in rnn_wv_scores['accuracies']]\n",
    "rnn_scores_data_frame = DataFrame(rnn_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=rnn_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bow_scores = []\n",
    "for score in ff_bow_scores['accuracies']:\n",
    "  bow_scores.append((\"Feed-Forward\", score))\n",
    "for score in conv_bow_scores['accuracies']:\n",
    "  bow_scores.append((\"Convolutional\", score))\n",
    "for score in rnn_bow_scores['accuracies']:\n",
    "  bow_scores.append((\"Recurrent\", score))\n",
    "    \n",
    "boxplot(x='model', y='accuracy', data=DataFrame(bow_scores, columns=[\"model\", \"accuracy\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wv_scores = []\n",
    "for score in ff_wv_scores['accuracies']:\n",
    "  wv_scores.append((\"Feed-Forward\", score))\n",
    "for score in conv_wv_scores['accuracies']:\n",
    "  wv_scores.append((\"Convolutional\", score))\n",
    "for score in rnn_wv_scores['accuracies']:\n",
    "  wv_scores.append((\"Recurrent\", score))\n",
    "\n",
    "boxplot(x='model', y='accuracy', data=DataFrame(wv_scores, columns=[\"model\", \"accuracy\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we can see that more work is needed to investigate why our embeddings to not perform better than bag of words. There are a number of possibilities, already suggested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
