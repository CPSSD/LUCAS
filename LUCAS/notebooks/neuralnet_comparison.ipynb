{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of neural Models\n",
    "This notebook creates a comparison of how our neural models perform. We will try each model with and without word embeddings, and produce visualisations of model performance. \n",
    "\n",
    "# Feed-Forward Neural Networks\n",
    "First we will find this difference for Feed-Forward Neural Networks (FFNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Embedding, Flatten, LSTM, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scripts import training_helpers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from seaborn import boxplot\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cross validate our model, so lets create a function to handle this for us. It will use StratifiedKFold splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validate(get_model, X, y, cv=5, categorical=False, add_target_dim=False):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "  }\n",
    "    \n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = np.array([X[x] for x in train_indices])\n",
    "    training_y = np.array([y[x] for x in train_indices])\n",
    "    test_X = np.array([X[x] for x in test_indices])\n",
    "    test_y = np.array([y[x] for x in test_indices])\n",
    "    \n",
    "    if categorical:\n",
    "      training_y = to_categorical(training_y)\n",
    "      test_y = to_categorical(test_y)\n",
    "    if add_target_dim:\n",
    "      training_y = np.array([[y] for y in training_y])\n",
    "      test_y = np.array([[y] for y in test_y])\n",
    "    \n",
    "    model = get_model()\n",
    "    print(\"Fitting with: \", np.array(training_X).shape, \"labels\", np.array(training_y).shape)\n",
    "    model.fit(np.array(training_X), training_y, epochs=12, batch_size=16, validation_split=0.3,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "    metrics[\"accuracies\"].append(model.evaluate(np.array(test_X), test_y)[1])\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find results for our Bag of Words (BoW) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = training_helpers.get_data_frame()\n",
    "\n",
    "predictors_raw = data_frame['review']\n",
    "num_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(predictors_raw)\n",
    "bow_predictors = tokenizer.texts_to_matrix(predictors_raw, mode='tfidf')\n",
    "labels = [x for x in data_frame['deceptive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6841 - acc: 0.7299 - val_loss: 0.4919 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 979us/step - loss: 0.3207 - acc: 0.9533 - val_loss: 0.4375 - val_acc: 0.8845\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 954us/step - loss: 0.2247 - acc: 0.9861 - val_loss: 0.4551 - val_acc: 0.8684\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 846us/step - loss: 0.1878 - acc: 0.9921 - val_loss: 0.4220 - val_acc: 0.8799\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 996us/step - loss: 0.1701 - acc: 0.9921 - val_loss: 0.4326 - val_acc: 0.8845\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1619 - acc: 0.9960 - val_loss: 0.4388 - val_acc: 0.8707\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 942us/step - loss: 0.1410 - acc: 0.9970 - val_loss: 0.4325 - val_acc: 0.8776\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 995us/step - loss: 0.1366 - acc: 0.9950 - val_loss: 0.4526 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 289us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7056 - acc: 0.7279 - val_loss: 0.5265 - val_acc: 0.8868\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 829us/step - loss: 0.3381 - acc: 0.9434 - val_loss: 0.4575 - val_acc: 0.8799\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 990us/step - loss: 0.2363 - acc: 0.9831 - val_loss: 0.4420 - val_acc: 0.8915\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1894 - acc: 0.9960 - val_loss: 0.4378 - val_acc: 0.8915\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 924us/step - loss: 0.1702 - acc: 0.9940 - val_loss: 0.4364 - val_acc: 0.8845\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 983us/step - loss: 0.1517 - acc: 0.9980 - val_loss: 0.4367 - val_acc: 0.8915\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 807us/step - loss: 0.1410 - acc: 0.9940 - val_loss: 0.4323 - val_acc: 0.8730\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 904us/step - loss: 0.1376 - acc: 0.9960 - val_loss: 0.4444 - val_acc: 0.8776\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 816us/step - loss: 0.1390 - acc: 0.9980 - val_loss: 0.4466 - val_acc: 0.8845\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 804us/step - loss: 0.1469 - acc: 0.9921 - val_loss: 0.4863 - val_acc: 0.8661\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 843us/step - loss: 0.1403 - acc: 0.9960 - val_loss: 0.4829 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 227us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7498 - acc: 0.7071 - val_loss: 0.5638 - val_acc: 0.8799\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 815us/step - loss: 0.4170 - acc: 0.9126 - val_loss: 0.4938 - val_acc: 0.8499\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 840us/step - loss: 0.2676 - acc: 0.9722 - val_loss: 0.4810 - val_acc: 0.8637\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2140 - acc: 0.9891 - val_loss: 0.4444 - val_acc: 0.8799\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1829 - acc: 0.9950 - val_loss: 0.4518 - val_acc: 0.8822\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 975us/step - loss: 0.1756 - acc: 0.9901 - val_loss: 0.4546 - val_acc: 0.8753\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1630 - acc: 0.9960 - val_loss: 0.4691 - val_acc: 0.8845\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1602 - acc: 0.9970 - val_loss: 0.4792 - val_acc: 0.8891\n",
      "160/160 [==============================] - 0s 255us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.6894 - acc: 0.7358 - val_loss: 0.5138 - val_acc: 0.8545\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.3196 - acc: 0.9523 - val_loss: 0.4592 - val_acc: 0.8799\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2174 - acc: 0.9891 - val_loss: 0.4649 - val_acc: 0.8637\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1838 - acc: 0.9940 - val_loss: 0.4599 - val_acc: 0.8707\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.1663 - acc: 0.9960 - val_loss: 0.4437 - val_acc: 0.8799\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1553 - acc: 0.9950 - val_loss: 0.4822 - val_acc: 0.8614\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1593 - acc: 0.9950 - val_loss: 0.5086 - val_acc: 0.8637\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1509 - acc: 0.9970 - val_loss: 0.4692 - val_acc: 0.8730\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 987us/step - loss: 0.1471 - acc: 0.9940 - val_loss: 0.4755 - val_acc: 0.8753\n",
      "160/160 [==============================] - 0s 206us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7318 - acc: 0.7080 - val_loss: 0.5465 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 927us/step - loss: 0.3468 - acc: 0.9643 - val_loss: 0.4573 - val_acc: 0.8822\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 916us/step - loss: 0.2312 - acc: 0.9921 - val_loss: 0.4483 - val_acc: 0.8753\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 728us/step - loss: 0.1951 - acc: 0.9960 - val_loss: 0.4571 - val_acc: 0.8753\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 779us/step - loss: 0.1763 - acc: 0.9960 - val_loss: 0.4494 - val_acc: 0.8753\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1657 - acc: 0.9990 - val_loss: 0.4440 - val_acc: 0.8822\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 977us/step - loss: 0.1584 - acc: 0.9980 - val_loss: 0.4273 - val_acc: 0.8799\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1548 - acc: 0.9960 - val_loss: 0.4505 - val_acc: 0.8730\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1544 - acc: 0.9921 - val_loss: 0.4302 - val_acc: 0.8938\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1580 - acc: 0.9940 - val_loss: 0.4619 - val_acc: 0.8707\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1466 - acc: 0.9980 - val_loss: 0.4619 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 225us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.7268 - acc: 0.6941 - val_loss: 0.5627 - val_acc: 0.8776\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 837us/step - loss: 0.3848 - acc: 0.9325 - val_loss: 0.4620 - val_acc: 0.8822\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 972us/step - loss: 0.2405 - acc: 0.9881 - val_loss: 0.4566 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 873us/step - loss: 0.1955 - acc: 0.9970 - val_loss: 0.4575 - val_acc: 0.8845\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1771 - acc: 0.9940 - val_loss: 0.4582 - val_acc: 0.8915\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 920us/step - loss: 0.1674 - acc: 0.9940 - val_loss: 0.4646 - val_acc: 0.8776\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 863us/step - loss: 0.1607 - acc: 0.9940 - val_loss: 0.4594 - val_acc: 0.8730\n",
      "160/160 [==============================] - 0s 219us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7634 - acc: 0.6634 - val_loss: 0.5894 - val_acc: 0.8822\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.4324 - acc: 0.9166 - val_loss: 0.4643 - val_acc: 0.8938\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2883 - acc: 0.9742 - val_loss: 0.4305 - val_acc: 0.8776\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2227 - acc: 0.9841 - val_loss: 0.4194 - val_acc: 0.8938\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1930 - acc: 0.9911 - val_loss: 0.4176 - val_acc: 0.8799\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1785 - acc: 0.9911 - val_loss: 0.4234 - val_acc: 0.8707\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1646 - acc: 0.9940 - val_loss: 0.4333 - val_acc: 0.8822\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1583 - acc: 0.9960 - val_loss: 0.4301 - val_acc: 0.8730\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 965us/step - loss: 0.1568 - acc: 0.9940 - val_loss: 0.4599 - val_acc: 0.8707\n",
      "160/160 [==============================] - 0s 195us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.7641 - acc: 0.6872 - val_loss: 0.5579 - val_acc: 0.8822\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.3955 - acc: 0.9355 - val_loss: 0.5036 - val_acc: 0.8522\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2586 - acc: 0.9791 - val_loss: 0.4740 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.2081 - acc: 0.9921 - val_loss: 0.4843 - val_acc: 0.8845\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1824 - acc: 0.9921 - val_loss: 0.4691 - val_acc: 0.8730\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1643 - acc: 0.9970 - val_loss: 0.4535 - val_acc: 0.8707\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1504 - acc: 0.9980 - val_loss: 0.4490 - val_acc: 0.8891\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1529 - acc: 0.9950 - val_loss: 0.4477 - val_acc: 0.8753\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1434 - acc: 0.9940 - val_loss: 0.4441 - val_acc: 0.8776\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1440 - acc: 0.9950 - val_loss: 0.4579 - val_acc: 0.8707\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1499 - acc: 0.9940 - val_loss: 0.4817 - val_acc: 0.8684\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 1s 1ms/step - loss: 0.1531 - acc: 0.9940 - val_loss: 0.5193 - val_acc: 0.8661\n",
      "160/160 [==============================] - 0s 230us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.7052 - acc: 0.7130 - val_loss: 0.5127 - val_acc: 0.8753\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 736us/step - loss: 0.3357 - acc: 0.9305 - val_loss: 0.4500 - val_acc: 0.8845\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 724us/step - loss: 0.2225 - acc: 0.9871 - val_loss: 0.4319 - val_acc: 0.8868\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 718us/step - loss: 0.1869 - acc: 0.9871 - val_loss: 0.4384 - val_acc: 0.8799\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 709us/step - loss: 0.1713 - acc: 0.9911 - val_loss: 0.4498 - val_acc: 0.8661\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 718us/step - loss: 0.1615 - acc: 0.9940 - val_loss: 0.4233 - val_acc: 0.8961\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 690us/step - loss: 0.1433 - acc: 0.9970 - val_loss: 0.4405 - val_acc: 0.8799\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 704us/step - loss: 0.1467 - acc: 0.9921 - val_loss: 0.4298 - val_acc: 0.8868\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 708us/step - loss: 0.1292 - acc: 0.9980 - val_loss: 0.4451 - val_acc: 0.8868\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 715us/step - loss: 0.1257 - acc: 0.9950 - val_loss: 0.4153 - val_acc: 0.8799\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 1s 711us/step - loss: 0.1256 - acc: 0.9970 - val_loss: 0.4726 - val_acc: 0.8776\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 1s 687us/step - loss: 0.1233 - acc: 0.9970 - val_loss: 0.4450 - val_acc: 0.8637\n",
      "160/160 [==============================] - 0s 161us/step\n",
      "Fitting with:  (1440, 20000) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7709 - acc: 0.6614 - val_loss: 0.6258 - val_acc: 0.8476\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 1s 798us/step - loss: 0.4380 - acc: 0.9086 - val_loss: 0.4669 - val_acc: 0.8915\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 1s 705us/step - loss: 0.2870 - acc: 0.9682 - val_loss: 0.4391 - val_acc: 0.8891\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 1s 713us/step - loss: 0.2232 - acc: 0.9881 - val_loss: 0.4446 - val_acc: 0.8891\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 1s 727us/step - loss: 0.1941 - acc: 0.9970 - val_loss: 0.4226 - val_acc: 0.8868\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 1s 736us/step - loss: 0.1772 - acc: 0.9960 - val_loss: 0.4077 - val_acc: 0.8891\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 1s 720us/step - loss: 0.1673 - acc: 0.9950 - val_loss: 0.4217 - val_acc: 0.8961\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 1s 693us/step - loss: 0.1556 - acc: 0.9970 - val_loss: 0.4280 - val_acc: 0.8868\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 1s 711us/step - loss: 0.1615 - acc: 0.9950 - val_loss: 0.4308 - val_acc: 0.8799\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 1s 697us/step - loss: 0.1561 - acc: 0.9990 - val_loss: 0.4547 - val_acc: 0.8845\n",
      "160/160 [==============================] - 0s 180us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_bow_model():\n",
    "  model = Sequential([\n",
    "      Dense(16, activation=relu, input_shape=(num_words,), kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "ff_bow_scores = run_cross_validate(get_ff_bow_model, bow_predictors, labels, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for our word vector method. First we must create our word vectors using a word vectorizing model generated in another experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load(\"opspam_w2v.kv\", mmap=\"r\")\n",
    "\n",
    "predictors_sequences = pad_sequences(tokenizer.texts_to_sequences(predictors_raw))\n",
    "max_sequence_length = max([len(x) for x in predictors_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = word_vectors.vector_size\n",
    "\n",
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1\n",
    "vectorizer_words = word_vectors.wv\n",
    "embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "for word, idx in corpus_words.items():\n",
    "  if word in vectorizer_words.vocab:\n",
    "    embedding_matrix[idx] = np.array(vectorizer_words[word], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 3s 3ms/step - loss: 0.9209 - acc: 0.4826 - val_loss: 0.8241 - val_acc: 0.5058\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7935 - acc: 0.5283 - val_loss: 0.7679 - val_acc: 0.5820\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7564 - acc: 0.5591 - val_loss: 0.7637 - val_acc: 0.5912\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7345 - acc: 0.5809 - val_loss: 0.7491 - val_acc: 0.5843\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7040 - acc: 0.6207 - val_loss: 0.7282 - val_acc: 0.5543\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6990 - acc: 0.6246 - val_loss: 0.7280 - val_acc: 0.6143\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6904 - acc: 0.6157 - val_loss: 0.7217 - val_acc: 0.6212\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6781 - acc: 0.6375 - val_loss: 0.7192 - val_acc: 0.6467\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6694 - acc: 0.6743 - val_loss: 0.7458 - val_acc: 0.6028\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6700 - acc: 0.6713 - val_loss: 0.7373 - val_acc: 0.6189\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6504 - acc: 0.6862 - val_loss: 0.7777 - val_acc: 0.5820\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6576 - acc: 0.6783 - val_loss: 0.7460 - val_acc: 0.6005\n",
      "160/160 [==============================] - 0s 294us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 3ms/step - loss: 0.9513 - acc: 0.5084 - val_loss: 0.8599 - val_acc: 0.5081\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8459 - acc: 0.5204 - val_loss: 0.8152 - val_acc: 0.4873\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7894 - acc: 0.4965 - val_loss: 0.7690 - val_acc: 0.5219\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7602 - acc: 0.5303 - val_loss: 0.7565 - val_acc: 0.5196\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7510 - acc: 0.5611 - val_loss: 0.7703 - val_acc: 0.5681\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7588 - acc: 0.5879 - val_loss: 0.7839 - val_acc: 0.5312\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7422 - acc: 0.6097 - val_loss: 0.7811 - val_acc: 0.5035\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7296 - acc: 0.6346 - val_loss: 0.7702 - val_acc: 0.5520\n",
      "160/160 [==============================] - 0s 306us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.9359 - acc: 0.5144 - val_loss: 0.8700 - val_acc: 0.5612\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8265 - acc: 0.6157 - val_loss: 0.8473 - val_acc: 0.5704\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7612 - acc: 0.6683 - val_loss: 0.8640 - val_acc: 0.5381\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7338 - acc: 0.6733 - val_loss: 0.8086 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7032 - acc: 0.6922 - val_loss: 0.8274 - val_acc: 0.5982\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6995 - acc: 0.6842 - val_loss: 0.8140 - val_acc: 0.6097\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6837 - acc: 0.7160 - val_loss: 0.8143 - val_acc: 0.6443\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6660 - acc: 0.7398 - val_loss: 0.8751 - val_acc: 0.5704\n",
      "160/160 [==============================] - 0s 263us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8829 - acc: 0.5204 - val_loss: 0.8026 - val_acc: 0.5520\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7805 - acc: 0.5323 - val_loss: 0.7737 - val_acc: 0.5774\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7674 - acc: 0.5482 - val_loss: 0.7544 - val_acc: 0.5912\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7479 - acc: 0.5670 - val_loss: 0.7511 - val_acc: 0.5450\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7369 - acc: 0.5929 - val_loss: 0.7486 - val_acc: 0.5912\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7181 - acc: 0.6077 - val_loss: 0.7357 - val_acc: 0.5912\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6895 - acc: 0.6465 - val_loss: 0.7435 - val_acc: 0.5820\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6723 - acc: 0.6594 - val_loss: 0.7476 - val_acc: 0.6259\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6898 - acc: 0.6415 - val_loss: 0.7639 - val_acc: 0.6259\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6794 - acc: 0.6693 - val_loss: 0.7798 - val_acc: 0.5797\n",
      "160/160 [==============================] - 0s 335us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 4s 4ms/step - loss: 0.8804 - acc: 0.5114 - val_loss: 0.8206 - val_acc: 0.5173\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8086 - acc: 0.4955 - val_loss: 0.7912 - val_acc: 0.5289\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7808 - acc: 0.5134 - val_loss: 0.7772 - val_acc: 0.5751\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7533 - acc: 0.5432 - val_loss: 0.7505 - val_acc: 0.5658\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7456 - acc: 0.5482 - val_loss: 0.7338 - val_acc: 0.5958\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6944 - acc: 0.6216 - val_loss: 0.7413 - val_acc: 0.6051\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6902 - acc: 0.6495 - val_loss: 0.7583 - val_acc: 0.5958\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6705 - acc: 0.6604 - val_loss: 0.7632 - val_acc: 0.6443\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6630 - acc: 0.7021 - val_loss: 0.7724 - val_acc: 0.6628\n",
      "160/160 [==============================] - 0s 367us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8874 - acc: 0.5214 - val_loss: 0.8097 - val_acc: 0.5012\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7886 - acc: 0.5144 - val_loss: 0.7705 - val_acc: 0.5012\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7706 - acc: 0.5015 - val_loss: 0.7667 - val_acc: 0.5035\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7432 - acc: 0.5730 - val_loss: 0.7355 - val_acc: 0.5358\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7331 - acc: 0.5571 - val_loss: 0.7236 - val_acc: 0.5612\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7277 - acc: 0.5809 - val_loss: 0.7189 - val_acc: 0.5912\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7032 - acc: 0.5799 - val_loss: 0.7121 - val_acc: 0.6166\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7005 - acc: 0.5968 - val_loss: 0.7159 - val_acc: 0.5751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6993 - acc: 0.6127 - val_loss: 0.7485 - val_acc: 0.5150\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7053 - acc: 0.6117 - val_loss: 0.7538 - val_acc: 0.5866\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7019 - acc: 0.6256 - val_loss: 0.7142 - val_acc: 0.6097\n",
      "160/160 [==============================] - 0s 492us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8920 - acc: 0.5055 - val_loss: 0.8175 - val_acc: 0.4873\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7954 - acc: 0.5154 - val_loss: 0.7949 - val_acc: 0.5242\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7746 - acc: 0.5174 - val_loss: 0.7670 - val_acc: 0.5035\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7491 - acc: 0.5283 - val_loss: 0.7389 - val_acc: 0.5473\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7377 - acc: 0.5323 - val_loss: 0.7341 - val_acc: 0.5219\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7330 - acc: 0.5561 - val_loss: 0.7375 - val_acc: 0.5704\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7317 - acc: 0.5124 - val_loss: 0.7280 - val_acc: 0.5012\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7158 - acc: 0.5313 - val_loss: 0.7052 - val_acc: 0.5774\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7047 - acc: 0.5998 - val_loss: 0.7271 - val_acc: 0.5450\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7022 - acc: 0.6058 - val_loss: 0.7436 - val_acc: 0.5912\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7008 - acc: 0.6296 - val_loss: 0.7374 - val_acc: 0.5658\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6995 - acc: 0.6068 - val_loss: 0.7685 - val_acc: 0.5797\n",
      "160/160 [==============================] - 0s 350us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8891 - acc: 0.5164 - val_loss: 0.8056 - val_acc: 0.4919\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7825 - acc: 0.5045 - val_loss: 0.7710 - val_acc: 0.5196\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7575 - acc: 0.5641 - val_loss: 0.7588 - val_acc: 0.5566\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7454 - acc: 0.5710 - val_loss: 0.7511 - val_acc: 0.5704\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7276 - acc: 0.6087 - val_loss: 0.7639 - val_acc: 0.5127\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7200 - acc: 0.6197 - val_loss: 0.7439 - val_acc: 0.6212\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7128 - acc: 0.6246 - val_loss: 0.7711 - val_acc: 0.5335\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7019 - acc: 0.6385 - val_loss: 0.7634 - val_acc: 0.5473\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6985 - acc: 0.6455 - val_loss: 0.7276 - val_acc: 0.6328\n",
      "Epoch 10/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6783 - acc: 0.6683 - val_loss: 0.7609 - val_acc: 0.5797\n",
      "Epoch 11/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6801 - acc: 0.6663 - val_loss: 0.7424 - val_acc: 0.6143\n",
      "Epoch 12/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6818 - acc: 0.6763 - val_loss: 0.7433 - val_acc: 0.6282\n",
      "160/160 [==============================] - 0s 299us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.9225 - acc: 0.5074 - val_loss: 0.8355 - val_acc: 0.5058\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.8099 - acc: 0.5462 - val_loss: 0.7916 - val_acc: 0.5912\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7566 - acc: 0.6197 - val_loss: 0.7677 - val_acc: 0.6074\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7362 - acc: 0.6296 - val_loss: 0.7969 - val_acc: 0.5335\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7294 - acc: 0.6326 - val_loss: 0.7467 - val_acc: 0.6467\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7109 - acc: 0.6683 - val_loss: 0.7903 - val_acc: 0.5589\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6913 - acc: 0.6753 - val_loss: 0.7964 - val_acc: 0.6236\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6834 - acc: 0.6902 - val_loss: 0.7506 - val_acc: 0.6582\n",
      "Epoch 9/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.6810 - acc: 0.6931 - val_loss: 0.7542 - val_acc: 0.6536\n",
      "160/160 [==============================] - 0s 293us/step\n",
      "Fitting with:  (1440, 784) labels (1440,)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 5s 5ms/step - loss: 0.8913 - acc: 0.5362 - val_loss: 0.8622 - val_acc: 0.4919\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7880 - acc: 0.5849 - val_loss: 0.7844 - val_acc: 0.5889\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7606 - acc: 0.6028 - val_loss: 0.7727 - val_acc: 0.6005\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7388 - acc: 0.6346 - val_loss: 0.7571 - val_acc: 0.5958\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7332 - acc: 0.6435 - val_loss: 0.7682 - val_acc: 0.6282\n",
      "Epoch 6/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7039 - acc: 0.6773 - val_loss: 0.7803 - val_acc: 0.6143\n",
      "Epoch 7/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7175 - acc: 0.6673 - val_loss: 0.7860 - val_acc: 0.5866\n",
      "Epoch 8/12\n",
      "1007/1007 [==============================] - 2s 2ms/step - loss: 0.7136 - acc: 0.6544 - val_loss: 0.7704 - val_acc: 0.6189\n",
      "160/160 [==============================] - 0s 338us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_wv_model():\n",
    "  model_ff_wv = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], trainable=False,\n",
    "                input_length=max_sequence_length),\n",
    "      Flatten(),\n",
    "      Dense(16, activation=relu, kernel_regularizer=l2(0.01)), #, input_shape=(num_words,)\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model_ff_wv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model_ff_wv\n",
    "\n",
    "ff_wv_scores = run_cross_validate(get_ff_wv_model, predictors_sequences, labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.875, 0.85625, 0.88125, 0.8875, 0.84375, 0.89375, 0.91875, 0.8375, 0.86875, 0.86875]\n",
      "Word vectors:  [0.625, 0.63125, 0.54375, 0.50625, 0.5625, 0.5875, 0.5625, 0.625, 0.54375, 0.55625]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", ff_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", ff_wv_scores['accuracies'])\n",
    "\n",
    "ff_scores_entries =[('Bag of Words', x) for x in ff_bow_scores['accuracies']] + [('Word Vectors', x) for x in ff_wv_scores['accuracies']]\n",
    "ff_scores_data_frame = DataFrame(ff_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=ff_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very strange! How could word vectors be less accurate than bag of words? This is known to occur when two conditions are met:\n",
    "    \n",
    "* The dataset is small\n",
    "* The dataset is very domain specific\n",
    "\n",
    "It is possible that these conditions actually are met here. The problem however is that running these models over our full dataset will take much longer, and will require a commited experiment to complete the investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network\n",
    "Now let's try this with a convolutional network. It has been shown that word vectors perform better for text classification than Bag of Words. If BoW is more accurate, it is a clear sign that we should investigate why. First we find th Bag of Words result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 20000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 20000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in bow_predictors]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_bow_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(1, 10),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(1, 20000, 1),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(pool_size=(1, 10)),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4104 - acc: 0.8133 - val_loss: 0.6877 - val_acc: 0.6420\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.1270 - acc: 0.9563 - val_loss: 0.7839 - val_acc: 0.6305\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0611 - acc: 0.9891 - val_loss: 0.9292 - val_acc: 0.6120\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0412 - acc: 0.9911 - val_loss: 1.1034 - val_acc: 0.5889\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0321 - acc: 0.9930 - val_loss: 1.1769 - val_acc: 0.5912\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 15ms/step - loss: 0.4176 - acc: 0.8064 - val_loss: 0.7610 - val_acc: 0.6189\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1339 - acc: 0.9494 - val_loss: 0.8414 - val_acc: 0.6328\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0693 - acc: 0.9861 - val_loss: 0.9440 - val_acc: 0.6212\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0391 - acc: 0.9940 - val_loss: 1.1303 - val_acc: 0.6028\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0212 - acc: 0.9970 - val_loss: 1.2155 - val_acc: 0.6143\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 15ms/step - loss: 0.4728 - acc: 0.7716 - val_loss: 0.6186 - val_acc: 0.6674\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1369 - acc: 0.9503 - val_loss: 0.6578 - val_acc: 0.6697\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0724 - acc: 0.9821 - val_loss: 0.9531 - val_acc: 0.6351\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0431 - acc: 0.9940 - val_loss: 0.8991 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0247 - acc: 0.9970 - val_loss: 1.0405 - val_acc: 0.6212\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.3978 - acc: 0.8113 - val_loss: 0.6811 - val_acc: 0.6328\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.1221 - acc: 0.9623 - val_loss: 0.7995 - val_acc: 0.6305\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0695 - acc: 0.9801 - val_loss: 0.9517 - val_acc: 0.6051\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0386 - acc: 0.9930 - val_loss: 1.0806 - val_acc: 0.6212\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0240 - acc: 0.9950 - val_loss: 1.1951 - val_acc: 0.6120\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.4336 - acc: 0.7786 - val_loss: 0.6395 - val_acc: 0.6697\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 21s 21ms/step - loss: 0.1405 - acc: 0.9434 - val_loss: 0.7227 - val_acc: 0.6582\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 21s 21ms/step - loss: 0.0678 - acc: 0.9871 - val_loss: 0.8461 - val_acc: 0.6536\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0361 - acc: 0.9950 - val_loss: 1.0363 - val_acc: 0.6236\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0234 - acc: 0.9960 - val_loss: 1.1762 - val_acc: 0.6143\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.4447 - acc: 0.7805 - val_loss: 0.6711 - val_acc: 0.6282\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.1323 - acc: 0.9573 - val_loss: 0.7492 - val_acc: 0.6351\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.0769 - acc: 0.9782 - val_loss: 0.9064 - val_acc: 0.6189\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0408 - acc: 0.9930 - val_loss: 1.0671 - val_acc: 0.6028\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0253 - acc: 1.0000 - val_loss: 1.1682 - val_acc: 0.6120\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4321 - acc: 0.7776 - val_loss: 0.6643 - val_acc: 0.6467\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.1234 - acc: 0.9623 - val_loss: 0.8116 - val_acc: 0.6282\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0605 - acc: 0.9881 - val_loss: 0.9559 - val_acc: 0.6097\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0384 - acc: 0.9930 - val_loss: 1.0549 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0208 - acc: 0.9990 - val_loss: 1.1966 - val_acc: 0.6074\n",
      "160/160 [==============================] - 1s 8ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.4149 - acc: 0.8054 - val_loss: 0.7225 - val_acc: 0.6490\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 18s 17ms/step - loss: 0.1227 - acc: 0.9682 - val_loss: 0.7774 - val_acc: 0.6490\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 20s 20ms/step - loss: 0.0639 - acc: 0.9841 - val_loss: 0.9796 - val_acc: 0.6166\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0328 - acc: 0.9950 - val_loss: 1.0511 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0191 - acc: 0.9980 - val_loss: 1.1928 - val_acc: 0.6236\n",
      "160/160 [==============================] - 1s 7ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.4449 - acc: 0.7954 - val_loss: 0.6790 - val_acc: 0.6397\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.1311 - acc: 0.9543 - val_loss: 0.7680 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0736 - acc: 0.9811 - val_loss: 0.9357 - val_acc: 0.6467\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0440 - acc: 0.9901 - val_loss: 1.1653 - val_acc: 0.6005\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0265 - acc: 0.9960 - val_loss: 1.1660 - val_acc: 0.6259\n",
      "160/160 [==============================] - 2s 11ms/step\n",
      "Fitting with:  (1440, 1, 20000, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 21s 21ms/step - loss: 0.4229 - acc: 0.8024 - val_loss: 0.6728 - val_acc: 0.6605\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 20s 20ms/step - loss: 0.1182 - acc: 0.9583 - val_loss: 0.7547 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.0661 - acc: 0.9791 - val_loss: 0.9196 - val_acc: 0.6490\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0412 - acc: 0.9901 - val_loss: 1.1109 - val_acc: 0.6120\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0232 - acc: 0.9970 - val_loss: 1.1591 - val_acc: 0.6305\n",
      "160/160 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "conv_bow_scores = run_cross_validate(get_conv_bow_model, convolutional_data, labels, cv=10, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.84375, 0.86875, 0.7625, 0.825, 0.86875, 0.85, 0.8, 0.8375, 0.8375, 0.78125]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_bow_scores['accuracies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our word vector result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_review(review_words):\n",
    "  sentence = []\n",
    "  for word in review_words:\n",
    "    if word in word_vectors.wv:\n",
    "      sentence.append(word_vectors.wv['dog'])\n",
    "  return np.array(sentence, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vectorized_review(vectorized_review, length):\n",
    "  return np.concatenate((vectorized_review, np.zeros((length - len(vectorized_review), 100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = [vectorize_review(text_to_word_sequence(x)) for x in predictors_raw]\n",
    "pad_length = max([x.shape[0] for x in vectorized_reviews])\n",
    "vectorized_reviews = np.array([[pad_vectorized_review(x, pad_length)] for x in vectorized_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 1, 381, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 100),\n",
    "          data_format=\"channels_first\",\n",
    "          input_shape=(1, 381, 100),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(strides=(1, 1), pool_size=(2, 1), data_format=\"channels_first\"),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1332, 1, 381, 100) labels (1332, 2)\n",
      "Train on 932 samples, validate on 400 samples\n",
      "Epoch 1/12\n",
      "932/932 [==============================] - 6s 7ms/step - loss: 0.6975 - acc: 0.4764 - val_loss: 0.6934 - val_acc: 0.4475\n",
      "Epoch 2/12\n",
      "932/932 [==============================] - 5s 5ms/step - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6942 - val_acc: 0.5025\n",
      "Epoch 3/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.6849 - acc: 0.5590 - val_loss: 0.6958 - val_acc: 0.4925\n",
      "Epoch 4/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.6753 - acc: 0.5869 - val_loss: 0.7030 - val_acc: 0.5225\n",
      "Epoch 5/12\n",
      "932/932 [==============================] - 4s 5ms/step - loss: 0.6682 - acc: 0.5912 - val_loss: 0.7110 - val_acc: 0.5000\n",
      "268/268 [==============================] - 1s 2ms/step\n",
      "Fitting with:  (1332, 1, 381, 100) labels (1332, 2)\n",
      "Train on 932 samples, validate on 400 samples\n",
      "Epoch 1/12\n",
      "932/932 [==============================] - 6s 7ms/step - loss: 0.6989 - acc: 0.4775 - val_loss: 0.6931 - val_acc: 0.4975\n",
      "Epoch 2/12\n",
      "932/932 [==============================] - 5s 5ms/step - loss: 0.6932 - acc: 0.4710 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 3/12\n",
      "932/932 [==============================] - 5s 5ms/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 4/12\n",
      "932/932 [==============================] - 5s 5ms/step - loss: 0.6934 - acc: 0.4775 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 5/12\n",
      "932/932 [==============================] - 5s 5ms/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "268/268 [==============================] - 1s 2ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 7ms/step - loss: 0.6957 - acc: 0.4759 - val_loss: 0.6931 - val_acc: 0.4938\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 5s 6ms/step - loss: 0.6919 - acc: 0.5252 - val_loss: 0.6923 - val_acc: 0.5037\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6842 - acc: 0.5659 - val_loss: 0.6956 - val_acc: 0.5012\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 5s 6ms/step - loss: 0.6716 - acc: 0.5884 - val_loss: 0.6993 - val_acc: 0.5112\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6600 - acc: 0.5949 - val_loss: 0.7082 - val_acc: 0.5187\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6565 - acc: 0.5949 - val_loss: 0.7069 - val_acc: 0.5187\n",
      "266/266 [==============================] - 1s 3ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 6ms/step - loss: 0.6965 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.5137\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6920 - acc: 0.5359 - val_loss: 0.6932 - val_acc: 0.5212\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6865 - acc: 0.5627 - val_loss: 0.6960 - val_acc: 0.5237\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6786 - acc: 0.5756 - val_loss: 0.6971 - val_acc: 0.5287\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6761 - acc: 0.5713 - val_loss: 0.7037 - val_acc: 0.5087\n",
      "266/266 [==============================] - 1s 2ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 6s 7ms/step - loss: 0.6979 - acc: 0.5091 - val_loss: 0.6924 - val_acc: 0.5112\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6928 - acc: 0.5177 - val_loss: 0.6922 - val_acc: 0.5287\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6823 - acc: 0.5852 - val_loss: 0.6925 - val_acc: 0.5287\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6680 - acc: 0.5927 - val_loss: 0.6970 - val_acc: 0.5237\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6621 - acc: 0.5916 - val_loss: 0.6982 - val_acc: 0.5387\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6552 - acc: 0.5981 - val_loss: 0.7037 - val_acc: 0.5411\n",
      "266/266 [==============================] - 1s 3ms/step\n",
      "Fitting with:  (1334, 1, 381, 100) labels (1334, 2)\n",
      "Train on 933 samples, validate on 401 samples\n",
      "Epoch 1/12\n",
      "933/933 [==============================] - 7s 7ms/step - loss: 0.6992 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.4888\n",
      "Epoch 2/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6934 - acc: 0.4770 - val_loss: 0.6933 - val_acc: 0.4888\n",
      "Epoch 3/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6930 - acc: 0.5091 - val_loss: 0.6934 - val_acc: 0.4988\n",
      "Epoch 4/12\n",
      "933/933 [==============================] - 4s 5ms/step - loss: 0.6915 - acc: 0.5456 - val_loss: 0.6944 - val_acc: 0.4788\n",
      "Epoch 5/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6869 - acc: 0.5552 - val_loss: 0.6962 - val_acc: 0.4913\n",
      "Epoch 6/12\n",
      "933/933 [==============================] - 5s 5ms/step - loss: 0.6740 - acc: 0.5809 - val_loss: 0.7028 - val_acc: 0.5212\n",
      "266/266 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, vectorized_reviews, labels, cv=6, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.84375, 0.86875, 0.7625, 0.825, 0.86875, 0.85, 0.8, 0.8375, 0.8375, 0.78125]\n",
      "Word vectors:  [0.4738805965701146, 0.5, 0.488721804063123, 0.5300751881940025, 0.4812030079669522, 0.496240601727837]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", conv_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", conv_wv_scores['accuracies'])\n",
    "\n",
    "conv_scores_entries =[('Bag of Words', x) for x in conv_bow_scores['accuracies']] + [('Word Vectors', x) for x in conv_wv_scores['accuracies']]\n",
    "conv_scores_data_frame = DataFrame(conv_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=conv_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before our results are alarming. We need the reasons why this can occur, it may be that we have made a mistake in generating our word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "Let's also try this same experiment for Recurrent Neural Networks. We will use LSTM since this is known to be a good option for text classification. First with Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 1600\n",
    "rnn_bow_data = np.array(np.split(bow_predictors, batches))\n",
    "rnn_bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_bow_targets = np.array([[x] for x in labels])\n",
    "rnn_bow_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_bow_model():\n",
    "  model = Sequential([\n",
    "      LSTM(8, input_shape=(1, 20000)),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "rnn_bow_scores = run_cross_validate(get_rnn_bow_model, rnn_bow_data, rnn_bow_targets, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with Word Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_wv_model():\n",
    "  model = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], input_length=max_sequence_length,\n",
    "                trainable=False),\n",
    "      LSTM(8),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "rnn_wv_scores = run_cross_validate(get_rnn_wv_model, predictors_sequences, labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Bag of words: \", rnn_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", rnn_wv_scores['accuracies'])\n",
    "\n",
    "rnn_scores_entries =[('Bag of Words', x) for x in rnn_bow_scores['accuracies']] + [('Word Vectors', x) for x in rnn_wv_scores['accuracies']]\n",
    "rnn_scores_data_frame = DataFrame(rnn_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(x='input type', y='accuracy', data=rnn_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_scores = []\n",
    "for score in ff_bow_scores['accuracies']:\n",
    "  bow_scores.append(\"Feed-Forward\", score)\n",
    "for score in conv_bow_scores['accuracies']:\n",
    "  bow_scores.append(\"Convolutional\", score)\n",
    "for score in rnn_bow_scores['accuracies']:\n",
    "  bow_scores.append(\"Recurrent\", score)\n",
    "    \n",
    "boxplot(x='input type', y='accuracy', data=conv_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_scores = []\n",
    "for score in ff_wv_scores['accuracies']:\n",
    "  wv_scores.append(\"Feed-Forward\", score)\n",
    "for score in conv_wv_scores['accuracies']:\n",
    "  wv_scores.append(\"Convolutional\", score)\n",
    "for score in rnn_wv_scores['accuracies']:\n",
    "  bow_scores.append(\"Recurrent\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
