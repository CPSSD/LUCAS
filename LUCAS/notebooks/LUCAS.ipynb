{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Approach - The Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step towards developing a better solution to the 'Opinion spam problem', and also to generate a benchmark for ourselves, we used a traditional statistical modelling approach to create our first model. In this case we used Naive Bayes from the scikit learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a common method for producing a baseline that can be compared to other, experimental methods. It is based on Bayes theroem with a 'naive' twist.\n",
    "\n",
    "It is Bayes Theorem with an assumption: “The conditions are independent.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayes Theorem, we say 'Probability of A given B'. The condition is B:\n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)} $$\n",
    "\n",
    "Now, when we have multiple conditions Bayes Theorem looks like this:\n",
    "\n",
    "$$ P(A \\mid x_1,...x_n) = \\frac{P(x_1,...x_n \\mid A) \\, P(A)}{P(x_1,...x_n)} $$\n",
    "\n",
    "Where x<sub>1</sub>,…x<sub>n</sub> denotes the intersection of all events x<sub>1</sub>,…x<sub>n</sub> together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the class y with the maximum probability. We can cycle through all possible combinations of all x<sub>i</sub> events, but this has a complexity of m<sup>n</sup>. (m = possible values for a feature, n = number of features) ([more here](https://en.wikipedia.org/wiki/Bayesian_network#Example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using this exponential algorithm, we can change the complexity to linear time by making an assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes assumes features are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By assuming our incoming features are independent, we can compute our formula by simply multiplying all of their feature probabilities together. We no longer need to worry about combinations, so our algorithm goes from exponential to linear.\n",
    "\n",
    "$$ P(A \\mid x_1,...x_n) \\propto P(A) \\prod_{i=1}^n P(x_i \\mid A) $$\n",
    "\n",
    "Note also that our denominator P(x<sub>1</sub>,...,x<sub>n</sub>) is not needed since it is constant. We have excluded the denominator in our above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption made by Naive Bayes is very unlikely to be a correct assumption, however it often performs well regardless. This means that it has become a common benchmark technique for data science experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
