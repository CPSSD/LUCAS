{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Approach - The Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step towards developing a better solution to the 'Opinion spam problem', and also to generate a benchmark for ourselves, we used a traditional statistical modelling approach to create our first model. In this case we used Naive Bayes from the scikit learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a common method for producing a baseline that can be compared to other, experimental methods. It is based on Bayes theroem with a 'naive' twist.\n",
    "\n",
    "It is Bayes Theorem with an assumption: “The conditions are independent.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayes Theorem, we say 'Probability of A given B'. The condition is B:\n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)} $$\n",
    "\n",
    "Now, when we have multiple conditions Bayes Theorem looks like this:\n",
    "\n",
    "$$ P(A \\mid x_1,...x_n) = \\frac{P(x_1,...x_n \\mid A) \\, P(A)}{P(x_1,...x_n)} $$\n",
    "\n",
    "Where x<sub>1</sub>,…x<sub>n</sub> denotes the joint probability of features x<sub>1</sub>,…x<sub>n</sub> together.\n",
    "\n",
    "The joint probability can be expressed like this:\n",
    "\n",
    "$$ P(x_1,x_2,x_3,..,x_n) = P(x_1 \\mid x_2,x_3,..,x_n)P(x_2 \\mid x_3,..,x_n)...P(x_n) $$\n",
    "\n",
    "Now for each term we resolve, we need to reuse this expression. This exponential algorithm is highly computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using this exponential algorithm, we can change the complexity to linear time by making an assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes assumes features are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By assuming our incoming features are independent, we can compute our formula by simply multiplying all of their feature probabilities together. We no longer need to worry about combinations, so our algorithm goes from exponential to linear.\n",
    "\n",
    "$$ P(A \\mid x_1,...x_n) \\propto P(A) \\prod_{i=1}^n P(x_i \\mid A) $$\n",
    "\n",
    "Note also that our denominator P(x<sub>1</sub>,...,x<sub>n</sub>) is not needed since it is constant. We have excluded the denominator in our above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption made by Naive Bayes is very unlikely to be a correct assumption, however it often performs well regardless. This means that it has become a common benchmark technique for data science experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
