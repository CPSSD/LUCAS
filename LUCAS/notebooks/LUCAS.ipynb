{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Approach - The Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step towards developing a better solution to the 'Opinion spam problem', and also to generate a benchmark for ourselves, we used a traditional statistical modelling approach to create our first model. In this case we used Naive Bayes from the scikit learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a common method for producing a baseline that can be compared to other, experimental methods. It is based on Bayes theroem with a 'naive' twist.\n",
    "\n",
    "It is Bayes Theorem with an assumption: “The conditions are independent.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayes Theorem, we say 'Probability of A given B'. The condition is B:\n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)} $$\n",
    "\n",
    "Now, when we have multiple conditions Bayes Theorem looks like this:\n",
    "\n",
    "$$ P(A \\mid x_1,...x_n) = \\frac{P(x_1,...x_n \\mid A) \\, P(A)}{P(x_1,...x_n)} $$\n",
    "\n",
    "Where x<sub>1</sub>,…x<sub>n</sub> denotes the joint probability of features x<sub>1</sub>,…x<sub>n</sub> together.\n",
    "\n",
    "The joint probability can be expressed like this:\n",
    "\n",
    "$$ P(x_1,x_2,x_3,..,x_n) = P(x_1 \\mid x_2,x_3,..,x_n)P(x_2 \\mid x_3,..,x_n)...P(x_n) $$\n",
    "\n",
    "Now for each term we resolve, we need to reuse this expression. This exponential algorithm is highly computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using this exponential algorithm, we can change the complexity to linear time by making an assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes assumes features are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By assuming our incoming features are independent, we can compute our formula by simply multiplying all of their feature probabilities together. We no longer need to worry about combinations, so our algorithm goes from exponential to linear.\n",
    "\n",
    "\n",
    "$$ P(A \\mid x_1,...x_n) \\propto P(A) \\prod_{i=1}^n P(x_i \\mid A) $$\n",
    "\n",
    "Note also that our denominator P(x<sub>1</sub>,...,x<sub>n</sub>) is not needed since it is constant. We have excluded the denominator in our above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption made by Naive Bayes is very unlikely to be a correct assumption, however it often performs well regardless. This means that it has become a common benchmark technique for data science experiments.\n",
    "\n",
    "Assuming that the words in every document are conditionally independent (according to the naive assumption), two different models can be used to compute the class-conditional probabilities: The Multi-variate Bernoulli model and the Multinomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multivariate Bernoulli model is based on binary data: Every token in the feature vector of a document is associated with the value 1 or 0. The feature vector has m dimensions where m is the number of words in the whole vocabulary. (1 means the word occurred in the document, 0 means it did not.) \n",
    "\n",
    "The Bernoulli trials can be written as\n",
    "\n",
    "$$ P(x \\mid {\\omega}_j) = \\prod_{i=1}^m P(x_i \\mid {\\omega}_j)^{b} \\cdot (1-P(x_i \\mid {\\omega}_j))^{(1-b)} , \\ \\ \\ (b {\\in} 0,1). $$\n",
    "\n",
    "Let $ \\hat P (x_i \\mid {\\omega}_j) $ be the maximum-likelihood estimate that a particular word (or token) $ x_i $ occurs in class $ {\\omega}_j $.\n",
    "\n",
    "$$ \\hat P (x_i \\mid {\\omega}_j) = \\frac {(df_{x_i ,y} + 1)} {(df_y+2)}, \\ \\ (i = (1, ... m)) $$\n",
    "where\n",
    "\n",
    "- $ df_{xi,y} $ is the number of documents in the training dataset that contain the feature $ x_i $ and belong to class $ {\\omega}_j $.\n",
    "\n",
    "- $ df_y $ is the number of documents in the training dataset that belong to class $ {\\omega}_j $.\n",
    "\n",
    "- $ +1 $ and $ +2 $ are the parameters of Laplace smoothing. (So if a class never seen before is tested, the class probabilities are not set to 0)\n",
    "\n",
    "Although this model works, it is better to take term frequency into account rather than assigning them binary values. We then find ourselves at.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Classification\n",
    "\n",
    "Multinomial Naive Bayes models the distribution of words in a document as a multinomial. A document is treated as a sequence of words and it is assumed that each word position is generated independently of every other. For classification, we assume that there are a fixed number of classes, $ c {\\in} \\{1, 2, . . . , m\\} $, each with\n",
    "a fixed set of multinomial parameters. The parameter\n",
    "vector for a class $c$ is $\\vec{c} = \\{{\\theta} _{c1}, {\\theta} _{c2}, . . . , {\\theta} _{cn}\\}$, where\n",
    "$n$ is the size of the vocabulary, \n",
    "\n",
    "$ \\ \\sum_i{\\theta} _{ci} = 1 \\ $, and $ \\ {\\theta} _{ci} \\ $ is the probability that word $ i $ occurs in that class. The\n",
    "likelihood of a document is a product of the parameters\n",
    "of the words that appear in the document,\n",
    "\n",
    "(1) $$ p(d|\\vec{\\theta} c) = \\frac {(\\sum_i fi)!} {\\prod_i fi!} \\prod_i({\\theta} _{ci})^{fi} $$\n",
    "\n",
    "We then apply a prior distribution over the set of classes, we arrive at the minimum error classification rule (Duda & Hart, 1973) which selects the class with the largest posterior probability,\n",
    "\n",
    "(2) $$ l(d) = argmax_c[log p(\\vec{\\theta} _c) +\\sum_if_ilog({\\theta} _{ci})] $$ \n",
    "\n",
    "(3) $$ = argmax_c[b_c + \\sum_if_iw_{ci}] $$\n",
    "\n",
    "where $b_c$ is the threshold term and $w_{ci}$ is the class $c$\n",
    "weight for word $i$. These values are natural parameters\n",
    "for the decision boundary. This is especially easy\n",
    "to see for binary classification, where the boundary is\n",
    "defined by setting the differences between the positive\n",
    "and negative class parameters equal to zero,\n",
    "\n",
    "(4) $$ (b^+ - b^-) +\\sum_i fi (w_{+i} - w_{-i}) = 0. $$\n",
    "\n",
    "<sup>(The form of this equation is identical to the decision boundary learned by the (linear) Support Vector Machine, logistic regression, linear least squares and the perceptron. Naive Bayes’ relatively poor performance results from how it chooses the $b_c$ and $w_{ci}$.)<sup>\n",
    "\n",
    "To estimate the parameters from the training data, we select a Dirichlet Prior and take the expectation of the parameter with respect to the posterior. \n",
    "\n",
    "This gives us a simple form for the estimate of the multinomial parameter, which involves the number of times word $i$ appears in the documents in class $c$ ($N_{ci}$), divided by the total number of word occurrences in class $c$ ($N_c$). For word $i$, a prior adds in ${\\alpha} _i$ imagined occurrences so that the estimate is a smoothed version of the maximum likelihood estimate,\n",
    "\n",
    "(5) $$ \\hat{\\theta} _{ci} = \\frac {N_{ci} + {\\alpha} _i} {N_c + {\\alpha} } $$\n",
    "\n",
    "where ${\\alpha} $ denotes the sum of the ${\\alpha} _i$. While ${\\alpha} _i$ can be set differently for each word, we follow common practice (Laplace smoothing) by setting ${\\alpha} _i = 1$ for all words. Substituting the true parameters in 2. with our estimates, we get the MNB classifier,\n",
    "\n",
    "(6) $$ l_{MNB}(d) = argmax_c[log \\ \\hat p({\\theta} _c)+\\sum_i f_i log \\frac {N_{ci} + {\\alpha} _i} {N_c + {\\alpha} }] \\ \\ , $$ \n",
    "\n",
    "where $ \\hat p({\\theta} _c)$ is the class prior estimate.\n",
    "\n",
    "The weights for the decision boundary defined by the MNB classifier are the log parameter estimates,\n",
    "(7) $$ \\hat w_{ci} = log \\hat {\\theta} _{ci}. $$\n",
    "\n",
    "## Complement Naive Bayes\n",
    "\n",
    "It has been shown (see fig. 1) that skewed training data (that is, more training samples in one class than another) will cause the classifier to have bias towards that class, and prefer it. To deal with the problem, we introduce the 'Complement' class of Naive Bayes, CNB. Instead of the regular MNB, which uses training data from a single class, $c$, to estimate training weights, CNB estimates using data from every class except $c$.\n",
    "\n",
    "![Fig. 1](\"img/cnb_fig1.png\")\n",
    "\n",
    "\n",
    "CNB’s estimate is \n",
    "(8) $$ \\hat {\\theta} _{\\overline{c}i} = \\frac {N_{\\overline{c}i} + {\\alpha} _i} {N_{\\overline{c}} + {\\alpha} } $$\n",
    "\n",
    "\n",
    "where $N_{\\overline {c}i}$ is the number of times word $i$ occurred in documents in classes other than $c$ and $N\\overline{c}$ is the total number of word occurrences in classes other than $c$, and ${\\alpha} _i$ and ${\\alpha} $ are smoothing parameters. As before, the weight estimate is $\\hat w_{\\overline{c}i} = log \\hat {\\theta} _{\\overline{c}i}$ and the classification rule is\n",
    "\n",
    "(9) $$ l_{CNB}(d) = argmax_c[log \\ \\ p(\\vec {\\theta} _c) -\\sum _i f_i log \\frac {N_{\\overline{c}i} + {\\alpha} _i} {N_{\\overline{c}} + {\\alpha} } ]. $$\n",
    "\n",
    "The negative sign represents the fact that we want to assign to class c documents that poorly match the complement parameter estimates.\n",
    "\n",
    "CNB is related to the one-versus-all-but-one technique that is frequently used in multi-label classification, where each example may have more than one label. Berger (1999) and Zhang and Oles (2001) have found that one-vs-all-but-one MNB works better than regular MNB. The combined classification rule is basically the above equation (8) except it has the complement weights subtracted from the normal weights. However, CNB performs better than OVA MNB and regular MNB because it eliminates the biased regular MNB weights.\n",
    "\n",
    "## Weight Magnitude Errors\n",
    "\n",
    "Because of the independence assumption, Naive Bayes can be biased to give more weight to those classes that most violate the indepence assumption. A good example of this is:\n",
    "\n",
    "Consider the problem of distinguishing between documents that discuss Boston and ones that discuss San Francisco. Let’s assume that “Boston” appears in Boston documents about as often as “San Francisco” appears in San Francisco documents (as one might expect). \n",
    "\n",
    "Let’s also assume that it’s rare to see the words “San” and “Francisco” apart. \n",
    "Then, each time a test document has an occurrence of “San Francisco,” Multinomial Naive Bayes will double count — it will add in the weight for “San” and the weight for “Francisco.”\n",
    "\n",
    "Since “San Francisco” and “Boston” occur equally in their respective classes, a single occurrence of “San Francisco” will contribute twice the weight as an occurrence of “Boston.” Hence, the summed contributions of the classification weights may be larger for one class than another — this will cause MNB to prefer one class incorrectly. \n",
    "\n",
    "For example, if a document has five occurrences of “Boston” and three of “San Francisco,” MNB will label the document as “San Francisco” rather than “Boston.”\n",
    "\n",
    "We correct for the fact that some classes have greater dependencies by normalizing the weight vectors. Instead of assigning $ \\hat w_{ci} = log \\hat {\\theta} _{ci} $, we assign\n",
    "\n",
    "(10) $$ \\hat w_{ci} = \\frac {log \\hat {\\theta} _{ci}} {\\sum_k \\mid log \\hat {\\theta} _{ck}} . $$\n",
    "\n",
    "We call this, combined with CNB, Weight-normalized Complement Naive Bayes (WCNB). Experiments indicate that WCNB is effective. \n",
    "\n",
    "## Transforming Term Frequency\n",
    "\n",
    "It was found that term distributions had heavier tails than predicted by the multinomial model, instead appearing like a power-law distribution. Using a simple transform, we can make these power-law-like term distributions look more multinomial.\n",
    "\n",
    "(11) $$ f'_i = log(d + f_i). $$\n",
    "\n",
    "One such transform, \n",
    "\n",
    "(12) $$ f'_i = log(1 + f_i), $$ \n",
    "\n",
    "has the advantages of being an identity transform for zero and one counts, while pushing down larger counts as we would like.\n",
    "\n",
    "<sup>(Although setting d = 1 does not match the data as well as an optimized d, it does produce a distribution that is much closer to the empirical distribution than the best fit multinomial.)<sup>\n",
    "    \n",
    "## Transforming by Document Frequency\n",
    "\n",
    "Another useful transform discounts terms that occur in many documents. Common words are unlikely to be related to the class of a document, but random variations can create apparent fictitious correlations.\n",
    "This adds noise to the parameter estimates and hence the classification weights. Since common words appear often, they can hold sway over a classification decision even if their weight differences between classes is small. For this reason, it is advantageous to downweight these words.\n",
    "\n",
    "'Inverse document frequency' (Jones, 1972) is one way to do this. Given by:\n",
    "\n",
    "$$ f'_i = f_ilog \\frac {\\sum_j1} {\\sum_j{\\delta} _{ij}} $$\n",
    "\n",
    "where ${\\delta}_{ij}$ is 1 if word i occurs in document j, 0 otherwise, and the sum is over all document indices (Salton & Buckley, 1988). Rare words are given increased term frequencies; common words are given less weight.\n",
    "\n",
    "## Transforming Based on Length\n",
    "\n",
    "Documents have strong word inter-dependencies. After a word first appears in a document, it is more likely to appear again. Since MNB assumes occurrence independence, long documents can negatively effect parameter estimates.\n",
    "We normalize word counts to avoid this problem. We again use a common transform that is not seen with Naive Bayes. We discount the influence of long documents by transforming the term frequencies according to\n",
    "\n",
    "$$ f'_i = \\frac {f_i} {\\sqrt{\\sum_k (fk)^2}} $$\n",
    "\n",
    "\n",
    "yielding a length 1 term frequency vector for each document. The transform keeps any single document\n",
    "from dominating the parameter estimates.\n",
    "\n",
    "## Conclusion and derivation of steps to arrive at TWCNB\n",
    "\n",
    "Let \n",
    "- $ \\vec{d} = (\\vec{d}_1, . . . , \\vec{d}_n) $ be a set of documents; $d_{ij}$ is the count of word $i$ in document $j$.\n",
    "- Let $\\vec{y} = (y_1, . . . , y_n)$ be the labels.\n",
    "\n",
    "TWCNB(~d, ~y):\n",
    "\n",
    "- $ d_{ij} = log(d_{ij} + 1) $ (TF transform)\n",
    "\n",
    "\n",
    "- $ d_{ij} = d_{ij} log \\frac {\\sum_k1} {\\sum_k{\\delta} _{ik}} $ (IDF transform)\n",
    "\n",
    "\n",
    "- $ d'_{ij} = \\frac {d_{ij}} {\\sqrt{\\sum_k (d_{kj})^2}} $(Length norm.)\n",
    "\n",
    "\n",
    "-  $ \\hat{\\theta} _{ci} = \\frac {\\sum_{j:y_j \\neq c} d_{ij} + \\alpha_i} {\\sum_{j:y_j \\neq c} \\sum_k d_{kj} + \\alpha} $ (Complement)\n",
    "\n",
    "\n",
    "- $ w_{ci} = log \\hat {\\theta} _{ci} $\n",
    "   \n",
    "   \n",
    "- $ w_{ci} = \\frac {w_{ci}} {\\sum_i w_{ci}} $ (weight normalization)\n",
    "   \n",
    "\n",
    "- Let t = (t1, . . . , tn) be a test document; let $t_i$ be the count of word $i$.\n",
    "\n",
    "\n",
    "- Label the document according to\n",
    "$$ l(t) = arg min_c[\\sum_i t_i w_{ci}] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
