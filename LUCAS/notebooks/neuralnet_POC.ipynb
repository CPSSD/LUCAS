{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof-of-concept: Classification of review deception via a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to move on to a deep-learning approach to fake review opinion spam detection, shown in numerous papers and studies to be an effective method of classification. \n",
    "In this notebook, in a similar fashion to the other notebooks in this directory, we will use the small dataset with minimal features and set up a very basic neural network using Tensorflow and Keras.\n",
    "Let's start by importing the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start by getting our data. We use the get_data_frame function from the training helpers file, and split it into our X and Y (data and labels). \n",
    "\n",
    "It's been shown that a 70/30 train/test split is generally quite good, with a 70/30 train/validation split on the training data.\n",
    "\n",
    "Training data is used to adjust weights, validation is used to make sure that these weight changes result in an increase in accuracy. Test data is used to run the final set of weights over data it's never seen before for final validation and to produce an accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import training_helpers as th\n",
    "df = th.get_data_frame()\n",
    "\n",
    "X = df['review']\n",
    "y = df['deceptive']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're dealing with a tonne of features, so let's use the keras Tokenizer, which lets us choose a number of the most commonly occuring words in our vocab. We'll set this number to 2000, to get rid of the least important ones, and then fit it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 2000\n",
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that takes a list of reviews and converts it to a list of tfidf vectors based on the above dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return tokenizer.texts_to_matrix(data, mode='tfidf')\n",
    "\n",
    "train_data = tokenize(X_train)\n",
    "test_data = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at these tfidf vectors and the shape of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        1.9429853 1.6775115 ... 0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now that our data is prepared, lets create our neural network.\n",
    "\n",
    "Let's create a basic network with one hidden layer of 4 nodes, fully connected (Dense). \n",
    "\n",
    "The first layer needs to specify the size of the input in 'input_shape', which is the number of features in an input. Its output size will be the first parameter (4). In this case, we use a very small number of nodes as it's a very small dataset, so 4 is optimal to prevent overfitting.\n",
    "\n",
    "The following layers can infer the size.\n",
    "\n",
    "We use the Adam optimizer, which is a replacement optimizer for stochastic gradient descent, and the binary_crossentropy loss function, which is the binary log loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4)                 8004      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 8,029\n",
      "Trainable params: 8,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    " \n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train and validate the model. We fit it on the training data, using 70% for training (weight adjustment) and 30% for validation. Validation is important as it's data that hasn't been seen in the training, so we get a feel for how well the model generalizes and make sure overfitting does not occur. In general, how good a model is can be judged based on how low it's validation loss is.\n",
    "\n",
    "Batch size is how many chunks we want to split out data up into. We cant pass the entire dataset in one epoch, so we split it up into chunks of size n, where n is the batch size. The number of iterations it takes to complete one epoch is the training size / batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 784 samples, validate on 336 samples\n",
      "Epoch 1/25\n",
      " - 3s - loss: 0.6572 - acc: 0.6059 - val_loss: 0.5594 - val_acc: 0.7827\n",
      "Epoch 2/25\n",
      " - 1s - loss: 0.3476 - acc: 0.9031 - val_loss: 0.3623 - val_acc: 0.8661\n",
      "Epoch 3/25\n",
      " - 1s - loss: 0.1447 - acc: 0.9668 - val_loss: 0.3416 - val_acc: 0.8631\n",
      "Epoch 4/25\n",
      " - 1s - loss: 0.0679 - acc: 0.9885 - val_loss: 0.3209 - val_acc: 0.8631\n",
      "Epoch 5/25\n",
      " - 1s - loss: 0.0352 - acc: 0.9974 - val_loss: 0.3241 - val_acc: 0.8631\n",
      "Epoch 6/25\n",
      " - 1s - loss: 0.0202 - acc: 0.9987 - val_loss: 0.3367 - val_acc: 0.8661\n",
      "Epoch 7/25\n",
      " - 1s - loss: 0.0125 - acc: 0.9987 - val_loss: 0.3500 - val_acc: 0.8690\n",
      "Epoch 8/25\n",
      " - 1s - loss: 0.0085 - acc: 0.9987 - val_loss: 0.3629 - val_acc: 0.8720\n",
      "Epoch 9/25\n",
      " - 1s - loss: 0.0062 - acc: 0.9987 - val_loss: 0.3745 - val_acc: 0.8720\n",
      "Epoch 10/25\n",
      " - 1s - loss: 0.0049 - acc: 0.9987 - val_loss: 0.3875 - val_acc: 0.8750\n",
      "Epoch 11/25\n",
      " - 1s - loss: 0.0039 - acc: 0.9987 - val_loss: 0.3967 - val_acc: 0.8661\n",
      "Epoch 12/25\n",
      " - 1s - loss: 0.0033 - acc: 0.9987 - val_loss: 0.4086 - val_acc: 0.8810\n",
      "Epoch 13/25\n",
      " - 1s - loss: 0.0028 - acc: 0.9987 - val_loss: 0.4171 - val_acc: 0.8750\n",
      "Epoch 14/25\n",
      " - 1s - loss: 0.0024 - acc: 0.9987 - val_loss: 0.4264 - val_acc: 0.8720\n",
      "Epoch 15/25\n",
      " - 1s - loss: 0.0021 - acc: 0.9987 - val_loss: 0.4385 - val_acc: 0.8780\n",
      "Epoch 16/25\n",
      " - 1s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4453 - val_acc: 0.8750\n",
      "Epoch 17/25\n",
      " - 1s - loss: 8.7913e-04 - acc: 1.0000 - val_loss: 0.4544 - val_acc: 0.8720\n",
      "Epoch 18/25\n",
      " - 1s - loss: 7.1072e-04 - acc: 1.0000 - val_loss: 0.4622 - val_acc: 0.8661\n",
      "Epoch 19/25\n",
      " - 1s - loss: 5.9477e-04 - acc: 1.0000 - val_loss: 0.4709 - val_acc: 0.8720\n",
      "Epoch 20/25\n",
      " - 1s - loss: 4.9869e-04 - acc: 1.0000 - val_loss: 0.4786 - val_acc: 0.8720\n",
      "Epoch 21/25\n",
      " - 1s - loss: 4.2153e-04 - acc: 1.0000 - val_loss: 0.4866 - val_acc: 0.8690\n",
      "Epoch 22/25\n",
      " - 1s - loss: 3.6028e-04 - acc: 1.0000 - val_loss: 0.4940 - val_acc: 0.8720\n",
      "Epoch 23/25\n",
      " - 1s - loss: 3.0919e-04 - acc: 1.0000 - val_loss: 0.5017 - val_acc: 0.8631\n",
      "Epoch 24/25\n",
      " - 1s - loss: 2.6626e-04 - acc: 1.0000 - val_loss: 0.5091 - val_acc: 0.8690\n",
      "Epoch 25/25\n",
      " - 1s - loss: 2.3031e-04 - acc: 1.0000 - val_loss: 0.5162 - val_acc: 0.8690\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                      y_train,\n",
    "                      epochs=25,\n",
    "                      batch_size=4,\n",
    "                      validation_split=0.3,\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the validation loss consistently decreases as the epochs go on, and validation accuracy consistently increases. If the validation loss is decreasing up until the end, then increase the number of epochs or reduce the batch size.\n",
    "\n",
    "From the val_acc seen at the end of every epoch, we can predict the value of accuracy on the test data. Lets evaluate the model and give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 0s 120us/step\n",
      "['loss', 'acc'] [0.692434874176979, 0.8354166666666667]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, y_test)\n",
    "print(model.metrics_names, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, the accuracy is about as predicted! And we have a pretty low loss too. This indicates a good model that will generalize well.\n",
    "Let's plot the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a3a648588>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a40b13ac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, acc, 'ro', label='Training accuracy')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving\n",
    "\n",
    "Let's try try add regularization and a dropout layer. Let's also use 5-fold cross validation to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.7431 - acc: 0.5759 - val_loss: 0.6649 - val_acc: 0.7526\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 659us/step - loss: 0.5683 - acc: 0.7489 - val_loss: 0.5403 - val_acc: 0.8411\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 717us/step - loss: 0.4116 - acc: 0.8493 - val_loss: 0.4553 - val_acc: 0.8568\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 738us/step - loss: 0.3435 - acc: 0.8884 - val_loss: 0.4271 - val_acc: 0.8672\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 757us/step - loss: 0.2919 - acc: 0.9330 - val_loss: 0.4144 - val_acc: 0.8932\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 765us/step - loss: 0.2440 - acc: 0.9632 - val_loss: 0.4220 - val_acc: 0.8724\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 765us/step - loss: 0.2192 - acc: 0.9621 - val_loss: 0.4315 - val_acc: 0.8568\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 800us/step - loss: 0.2070 - acc: 0.9721 - val_loss: 0.4178 - val_acc: 0.8802\n",
      "acc: 88.44%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 4ms/step - loss: 0.7421 - acc: 0.5379 - val_loss: 0.7084 - val_acc: 0.6250\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 819us/step - loss: 0.6537 - acc: 0.7333 - val_loss: 0.5951 - val_acc: 0.7760\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 916us/step - loss: 0.4803 - acc: 0.8471 - val_loss: 0.4826 - val_acc: 0.8620\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 976us/step - loss: 0.3742 - acc: 0.9040 - val_loss: 0.4396 - val_acc: 0.8594\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.2992 - acc: 0.9420 - val_loss: 0.4320 - val_acc: 0.8594\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 852us/step - loss: 0.2615 - acc: 0.9565 - val_loss: 0.4335 - val_acc: 0.8646\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 796us/step - loss: 0.2488 - acc: 0.9598 - val_loss: 0.4549 - val_acc: 0.8490\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 828us/step - loss: 0.2153 - acc: 0.9654 - val_loss: 0.4590 - val_acc: 0.8594\n",
      "acc: 84.69%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 4ms/step - loss: 0.7860 - acc: 0.5435 - val_loss: 0.6823 - val_acc: 0.7682\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 818us/step - loss: 0.6058 - acc: 0.7612 - val_loss: 0.5195 - val_acc: 0.8438\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 861us/step - loss: 0.4566 - acc: 0.8549 - val_loss: 0.4704 - val_acc: 0.8594\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 980us/step - loss: 0.3661 - acc: 0.9007 - val_loss: 0.4176 - val_acc: 0.8672\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 829us/step - loss: 0.3269 - acc: 0.9174 - val_loss: 0.3979 - val_acc: 0.8672\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 906us/step - loss: 0.2923 - acc: 0.9297 - val_loss: 0.3953 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 863us/step - loss: 0.2546 - acc: 0.9542 - val_loss: 0.3724 - val_acc: 0.8776\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 929us/step - loss: 0.2414 - acc: 0.9621 - val_loss: 0.3627 - val_acc: 0.8724\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 837us/step - loss: 0.2346 - acc: 0.9632 - val_loss: 0.3668 - val_acc: 0.8828\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.2205 - acc: 0.9643 - val_loss: 0.3734 - val_acc: 0.8828\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 1s 963us/step - loss: 0.2032 - acc: 0.9766 - val_loss: 0.3789 - val_acc: 0.8776\n",
      "acc: 87.19%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 5ms/step - loss: 0.7561 - acc: 0.5636 - val_loss: 0.6688 - val_acc: 0.7630\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 963us/step - loss: 0.6060 - acc: 0.7746 - val_loss: 0.5490 - val_acc: 0.8255\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 913us/step - loss: 0.5010 - acc: 0.8125 - val_loss: 0.4898 - val_acc: 0.8490\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 934us/step - loss: 0.4031 - acc: 0.8783 - val_loss: 0.4607 - val_acc: 0.8516\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 910us/step - loss: 0.3582 - acc: 0.8973 - val_loss: 0.4684 - val_acc: 0.8490\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 904us/step - loss: 0.2990 - acc: 0.9263 - val_loss: 0.4445 - val_acc: 0.8516\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 967us/step - loss: 0.2853 - acc: 0.9397 - val_loss: 0.4470 - val_acc: 0.8672\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 964us/step - loss: 0.2604 - acc: 0.9498 - val_loss: 0.4488 - val_acc: 0.8568\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.2568 - acc: 0.9520 - val_loss: 0.4601 - val_acc: 0.8464\n",
      "acc: 88.12%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 5ms/step - loss: 0.7246 - acc: 0.6127 - val_loss: 0.5998 - val_acc: 0.7812\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 862us/step - loss: 0.4748 - acc: 0.8147 - val_loss: 0.4971 - val_acc: 0.8307\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 937us/step - loss: 0.3190 - acc: 0.9040 - val_loss: 0.4230 - val_acc: 0.8594\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 923us/step - loss: 0.2568 - acc: 0.9308 - val_loss: 0.4227 - val_acc: 0.8568\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 929us/step - loss: 0.2382 - acc: 0.9420 - val_loss: 0.4325 - val_acc: 0.8594\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 986us/step - loss: 0.2187 - acc: 0.9397 - val_loss: 0.4845 - val_acc: 0.8464\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 927us/step - loss: 0.1955 - acc: 0.9542 - val_loss: 0.4598 - val_acc: 0.8750\n",
      "acc: 84.38%\n",
      "86.56% (+/- 1.71%)\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "cvscores = []\n",
    "\n",
    "X = df['review']\n",
    "y = np.array(df['deceptive']).ravel()\n",
    "\n",
    "X = tokenize(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "             \n",
    "for train, test in kfold.split(X_tokens, y):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    " \n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X[train], y[train], epochs=50, batch_size=8, validation_split=0.3, verbose=1, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86% is very good, and a definite improvement. The early stopping also helps hugely.\n",
    "Now let's see what a better word representation does for us. Let's try use word embeddings as our festure set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9839\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vectors = KeyedVectors.load('../wordvec/opspam_w2v.kv')\n",
    "label_encoder = LabelEncoder()\n",
    "tokenizer = text.Tokenizer(num_words=5000)\n",
    "\n",
    "X = df['review']\n",
    "y = np.array(df['deceptive']).ravel()\n",
    "\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "X_sequences = np.array(tokenizer.texts_to_sequences(X))\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938408374834841\n",
      "(9839, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in vectors.wv.vocab:\n",
    "        vector = vectors.wv[word]\n",
    "    else:\n",
    "        vector = None\n",
    "    if vector is not None: \n",
    "        embedding_matrix[i] = np.array(vector, dtype=np.float32)\n",
    "        \n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements/vocab_size)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 4ms/step - loss: 0.7468 - acc: 0.5022 - val_loss: 0.7250 - val_acc: 0.5781\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 826us/step - loss: 0.7196 - acc: 0.5458 - val_loss: 0.7076 - val_acc: 0.6276\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7068 - acc: 0.5971 - val_loss: 0.7025 - val_acc: 0.5703\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7008 - acc: 0.6094 - val_loss: 0.6947 - val_acc: 0.6432\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6949 - acc: 0.6150 - val_loss: 0.6904 - val_acc: 0.6354\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6942 - acc: 0.5815 - val_loss: 0.6881 - val_acc: 0.5781\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 989us/step - loss: 0.6915 - acc: 0.5960 - val_loss: 0.6853 - val_acc: 0.6667\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6868 - acc: 0.6429 - val_loss: 0.6871 - val_acc: 0.5938\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6839 - acc: 0.6540 - val_loss: 0.6808 - val_acc: 0.6536\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 1s 957us/step - loss: 0.6826 - acc: 0.6417 - val_loss: 0.6798 - val_acc: 0.6615\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6784 - acc: 0.6708 - val_loss: 0.6794 - val_acc: 0.6276\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6787 - acc: 0.6384 - val_loss: 0.6791 - val_acc: 0.6250\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6745 - acc: 0.6585 - val_loss: 0.6729 - val_acc: 0.6745\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6737 - acc: 0.6417 - val_loss: 0.6722 - val_acc: 0.5964\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6718 - acc: 0.6540 - val_loss: 0.6694 - val_acc: 0.6615\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6691 - acc: 0.6685 - val_loss: 0.6690 - val_acc: 0.6615\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6672 - acc: 0.6362 - val_loss: 0.6751 - val_acc: 0.6068\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6630 - acc: 0.6529 - val_loss: 0.6643 - val_acc: 0.6484\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6640 - acc: 0.6864 - val_loss: 0.6643 - val_acc: 0.6693\n",
      "Epoch 20/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6613 - acc: 0.6741 - val_loss: 0.6614 - val_acc: 0.6615\n",
      "Epoch 21/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6606 - acc: 0.6763 - val_loss: 0.6598 - val_acc: 0.6510\n",
      "Epoch 22/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6589 - acc: 0.6496 - val_loss: 0.6595 - val_acc: 0.6536\n",
      "Epoch 23/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6607 - acc: 0.6417 - val_loss: 0.6586 - val_acc: 0.6745\n",
      "Epoch 24/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6578 - acc: 0.6786 - val_loss: 0.6580 - val_acc: 0.6667\n",
      "Epoch 25/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6528 - acc: 0.6741 - val_loss: 0.6751 - val_acc: 0.5807\n",
      "Epoch 26/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6538 - acc: 0.6730 - val_loss: 0.6560 - val_acc: 0.6667\n",
      "Epoch 27/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6504 - acc: 0.6853 - val_loss: 0.6561 - val_acc: 0.6719\n",
      "Epoch 28/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6527 - acc: 0.6808 - val_loss: 0.6538 - val_acc: 0.6667\n",
      "Epoch 29/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6506 - acc: 0.6752 - val_loss: 0.6527 - val_acc: 0.6693\n",
      "Epoch 30/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6506 - acc: 0.6652 - val_loss: 0.6519 - val_acc: 0.6693\n",
      "Epoch 31/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6525 - acc: 0.6484 - val_loss: 0.6501 - val_acc: 0.6641\n",
      "Epoch 32/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6479 - acc: 0.6730 - val_loss: 0.6573 - val_acc: 0.6641\n",
      "Epoch 33/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6475 - acc: 0.6786 - val_loss: 0.6514 - val_acc: 0.6693\n",
      "Epoch 34/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6466 - acc: 0.6752 - val_loss: 0.6619 - val_acc: 0.6328\n",
      "acc: 70.62%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 5ms/step - loss: 0.7517 - acc: 0.5257 - val_loss: 0.7293 - val_acc: 0.5885\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7248 - acc: 0.5480 - val_loss: 0.7135 - val_acc: 0.6016\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7116 - acc: 0.5748 - val_loss: 0.7044 - val_acc: 0.5599\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7038 - acc: 0.5982 - val_loss: 0.7060 - val_acc: 0.5625\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6985 - acc: 0.6094 - val_loss: 0.6956 - val_acc: 0.6146\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6954 - acc: 0.6083 - val_loss: 0.6889 - val_acc: 0.6328\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6874 - acc: 0.6183 - val_loss: 0.6876 - val_acc: 0.6146\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6810 - acc: 0.6775 - val_loss: 0.6734 - val_acc: 0.6589\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6781 - acc: 0.6462 - val_loss: 0.6695 - val_acc: 0.6745\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6737 - acc: 0.6562 - val_loss: 0.6665 - val_acc: 0.6745\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6709 - acc: 0.6685 - val_loss: 0.6639 - val_acc: 0.6745\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6656 - acc: 0.6574 - val_loss: 0.6616 - val_acc: 0.6719\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6629 - acc: 0.6730 - val_loss: 0.6584 - val_acc: 0.6641\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6607 - acc: 0.6585 - val_loss: 0.6543 - val_acc: 0.6875\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6585 - acc: 0.6663 - val_loss: 0.6570 - val_acc: 0.6641\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6548 - acc: 0.6830 - val_loss: 0.6552 - val_acc: 0.6641\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6518 - acc: 0.6819 - val_loss: 0.6646 - val_acc: 0.6328\n",
      "acc: 61.25%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 4ms/step - loss: 0.7429 - acc: 0.5033 - val_loss: 0.7211 - val_acc: 0.5495\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7150 - acc: 0.5078 - val_loss: 0.7059 - val_acc: 0.5938\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7050 - acc: 0.5268 - val_loss: 0.6999 - val_acc: 0.6198\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7012 - acc: 0.5413 - val_loss: 0.6958 - val_acc: 0.6276\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6976 - acc: 0.5480 - val_loss: 0.6930 - val_acc: 0.6380\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6953 - acc: 0.5848 - val_loss: 0.6918 - val_acc: 0.6328\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5949 - val_loss: 0.6901 - val_acc: 0.6224\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6912 - acc: 0.5949 - val_loss: 0.6866 - val_acc: 0.6641\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6896 - acc: 0.6328 - val_loss: 0.6864 - val_acc: 0.6536\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6896 - acc: 0.5759 - val_loss: 0.6832 - val_acc: 0.6927\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6871 - acc: 0.5926 - val_loss: 0.6814 - val_acc: 0.5651\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 1s 987us/step - loss: 0.6867 - acc: 0.6116 - val_loss: 0.6828 - val_acc: 0.6120\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - ETA: 0s - loss: 0.6836 - acc: 0.611 - 1s 922us/step - loss: 0.6842 - acc: 0.6094 - val_loss: 0.6811 - val_acc: 0.6198\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6825 - acc: 0.6250 - val_loss: 0.6762 - val_acc: 0.6953\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6812 - acc: 0.6161 - val_loss: 0.6759 - val_acc: 0.6719\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6806 - acc: 0.6127 - val_loss: 0.6725 - val_acc: 0.6250\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6791 - acc: 0.6462 - val_loss: 0.6705 - val_acc: 0.6615\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6781 - acc: 0.6362 - val_loss: 0.6738 - val_acc: 0.6302\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6754 - acc: 0.6328 - val_loss: 0.6679 - val_acc: 0.6979\n",
      "Epoch 20/50\n",
      "896/896 [==============================] - 1s 960us/step - loss: 0.6743 - acc: 0.6384 - val_loss: 0.6779 - val_acc: 0.5755\n",
      "Epoch 21/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6736 - acc: 0.6295 - val_loss: 0.6654 - val_acc: 0.6693\n",
      "Epoch 22/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6703 - acc: 0.6663 - val_loss: 0.6623 - val_acc: 0.7031\n",
      "Epoch 23/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6695 - acc: 0.6730 - val_loss: 0.6599 - val_acc: 0.7005\n",
      "Epoch 24/50\n",
      "896/896 [==============================] - 1s 960us/step - loss: 0.6643 - acc: 0.6529 - val_loss: 0.6852 - val_acc: 0.5417\n",
      "Epoch 25/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6652 - acc: 0.6585 - val_loss: 0.6563 - val_acc: 0.7057\n",
      "Epoch 26/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6646 - acc: 0.6551 - val_loss: 0.6540 - val_acc: 0.7005\n",
      "Epoch 27/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6652 - acc: 0.6730 - val_loss: 0.6548 - val_acc: 0.6719\n",
      "Epoch 28/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6595 - acc: 0.6786 - val_loss: 0.6505 - val_acc: 0.6693\n",
      "Epoch 29/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6615 - acc: 0.6473 - val_loss: 0.6535 - val_acc: 0.6589\n",
      "Epoch 30/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6575 - acc: 0.6663 - val_loss: 0.6471 - val_acc: 0.7109\n",
      "Epoch 31/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6569 - acc: 0.6730 - val_loss: 0.6460 - val_acc: 0.6745\n",
      "Epoch 32/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6541 - acc: 0.6730 - val_loss: 0.6456 - val_acc: 0.6797\n",
      "Epoch 33/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6550 - acc: 0.6931 - val_loss: 0.6423 - val_acc: 0.6901\n",
      "Epoch 34/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6518 - acc: 0.6819 - val_loss: 0.6450 - val_acc: 0.6745\n",
      "Epoch 35/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6519 - acc: 0.6908 - val_loss: 0.6403 - val_acc: 0.6979\n",
      "Epoch 36/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6485 - acc: 0.6987 - val_loss: 0.6408 - val_acc: 0.6823\n",
      "Epoch 37/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6496 - acc: 0.6819 - val_loss: 0.6556 - val_acc: 0.6328\n",
      "Epoch 38/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6481 - acc: 0.6897 - val_loss: 0.6458 - val_acc: 0.6693\n",
      "acc: 60.00%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 5ms/step - loss: 0.7544 - acc: 0.4978 - val_loss: 0.7338 - val_acc: 0.5260\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7254 - acc: 0.5179 - val_loss: 0.7180 - val_acc: 0.5104\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7106 - acc: 0.5458 - val_loss: 0.7127 - val_acc: 0.4714\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7030 - acc: 0.5547 - val_loss: 0.7069 - val_acc: 0.4922\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6943 - acc: 0.6183 - val_loss: 0.6945 - val_acc: 0.5312\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6919 - acc: 0.5904 - val_loss: 0.6909 - val_acc: 0.5547\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6861 - acc: 0.6272 - val_loss: 0.6971 - val_acc: 0.5495\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6833 - acc: 0.6172 - val_loss: 0.6975 - val_acc: 0.5365\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6803 - acc: 0.6596 - val_loss: 0.6832 - val_acc: 0.5964\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6784 - acc: 0.6696 - val_loss: 0.6823 - val_acc: 0.6224\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6739 - acc: 0.6596 - val_loss: 0.6844 - val_acc: 0.6042\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6724 - acc: 0.6685 - val_loss: 0.6782 - val_acc: 0.6328\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6683 - acc: 0.6853 - val_loss: 0.6765 - val_acc: 0.6354\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6653 - acc: 0.6585 - val_loss: 0.6780 - val_acc: 0.6172\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6650 - acc: 0.6708 - val_loss: 0.6736 - val_acc: 0.6354\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6631 - acc: 0.6763 - val_loss: 0.6720 - val_acc: 0.6328\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6593 - acc: 0.6652 - val_loss: 0.6755 - val_acc: 0.6094\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6603 - acc: 0.6719 - val_loss: 0.6813 - val_acc: 0.6094\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6573 - acc: 0.6763 - val_loss: 0.6779 - val_acc: 0.6042\n",
      "acc: 67.19%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 4s 5ms/step - loss: 0.7739 - acc: 0.4676 - val_loss: 0.7547 - val_acc: 0.4479\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7449 - acc: 0.5123 - val_loss: 0.7358 - val_acc: 0.5026\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7307 - acc: 0.5056 - val_loss: 0.7301 - val_acc: 0.5052\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7180 - acc: 0.5658 - val_loss: 0.7137 - val_acc: 0.5599\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7084 - acc: 0.6127 - val_loss: 0.7055 - val_acc: 0.6224\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.7020 - acc: 0.6507 - val_loss: 0.7010 - val_acc: 0.5729\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6971 - acc: 0.6038 - val_loss: 0.6946 - val_acc: 0.6016\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6960 - acc: 0.5926 - val_loss: 0.6892 - val_acc: 0.6562\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6860 - acc: 0.6652 - val_loss: 0.6847 - val_acc: 0.6536\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6822 - acc: 0.6529 - val_loss: 0.6859 - val_acc: 0.5807\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6806 - acc: 0.6417 - val_loss: 0.6933 - val_acc: 0.5521\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6768 - acc: 0.6484 - val_loss: 0.6747 - val_acc: 0.6667\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6711 - acc: 0.6842 - val_loss: 0.6717 - val_acc: 0.6667\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6713 - acc: 0.6339 - val_loss: 0.6687 - val_acc: 0.6641\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6717 - acc: 0.6462 - val_loss: 0.6675 - val_acc: 0.6536\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6648 - acc: 0.6808 - val_loss: 0.6634 - val_acc: 0.6615\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 1s 952us/step - loss: 0.6639 - acc: 0.6652 - val_loss: 0.6658 - val_acc: 0.6536\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6597 - acc: 0.6518 - val_loss: 0.6610 - val_acc: 0.6589\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6601 - acc: 0.6596 - val_loss: 0.6630 - val_acc: 0.6484\n",
      "Epoch 20/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6535 - acc: 0.6842 - val_loss: 0.6614 - val_acc: 0.6484\n",
      "Epoch 21/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6561 - acc: 0.6719 - val_loss: 0.6543 - val_acc: 0.6719\n",
      "Epoch 22/50\n",
      "896/896 [==============================] - 1s 941us/step - loss: 0.6536 - acc: 0.6585 - val_loss: 0.6534 - val_acc: 0.6693\n",
      "Epoch 23/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6516 - acc: 0.6540 - val_loss: 0.6507 - val_acc: 0.6797\n",
      "Epoch 24/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6480 - acc: 0.6763 - val_loss: 0.6555 - val_acc: 0.6510\n",
      "Epoch 25/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6447 - acc: 0.6931 - val_loss: 0.6529 - val_acc: 0.6510\n",
      "Epoch 26/50\n",
      "896/896 [==============================] - 1s 968us/step - loss: 0.6446 - acc: 0.6875 - val_loss: 0.6470 - val_acc: 0.6823\n",
      "Epoch 27/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6444 - acc: 0.6875 - val_loss: 0.6591 - val_acc: 0.6536\n",
      "Epoch 28/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6396 - acc: 0.6953 - val_loss: 0.6460 - val_acc: 0.6693\n",
      "Epoch 29/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6393 - acc: 0.6830 - val_loss: 0.6506 - val_acc: 0.6589\n",
      "Epoch 30/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6376 - acc: 0.6953 - val_loss: 0.6444 - val_acc: 0.6719\n",
      "Epoch 31/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6357 - acc: 0.6908 - val_loss: 0.6447 - val_acc: 0.6719\n",
      "Epoch 32/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6333 - acc: 0.6920 - val_loss: 0.6646 - val_acc: 0.5990\n",
      "Epoch 33/50\n",
      "896/896 [==============================] - 1s 1ms/step - loss: 0.6411 - acc: 0.6629 - val_loss: 0.6482 - val_acc: 0.6302\n",
      "acc: 65.62%\n",
      "75.75% (+/- 11.22%)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 512\n",
    "X = pad_sequences(X_sequences, padding='post', maxlen=maxlen)\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        keras.layers.GlobalMaxPool1D(),\n",
    "        keras.layers.Dense(4, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=50, batch_size=8, validation_split=0.3, verbose=1, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niallwalsh/miniconda3/envs/lucas/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 0.7894 - acc: 0.4989 - val_loss: 0.7420 - val_acc: 0.5859\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.7196 - acc: 0.6440 - val_loss: 0.7081 - val_acc: 0.7135\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.6768 - acc: 0.7478 - val_loss: 0.6747 - val_acc: 0.7812\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.6321 - acc: 0.7801 - val_loss: 0.6366 - val_acc: 0.7344\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.5664 - acc: 0.8025 - val_loss: 0.5918 - val_acc: 0.7839\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.5041 - acc: 0.8382 - val_loss: 0.5582 - val_acc: 0.7969\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.4515 - acc: 0.8583 - val_loss: 0.5407 - val_acc: 0.8047\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.4073 - acc: 0.8828 - val_loss: 0.5315 - val_acc: 0.7865\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3697 - acc: 0.9007 - val_loss: 0.5248 - val_acc: 0.7943\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3374 - acc: 0.9219 - val_loss: 0.5170 - val_acc: 0.7969\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3103 - acc: 0.9297 - val_loss: 0.5253 - val_acc: 0.7865\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.2873 - acc: 0.9375 - val_loss: 0.5082 - val_acc: 0.8099\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.2635 - acc: 0.9542 - val_loss: 0.5027 - val_acc: 0.8099\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2441 - acc: 0.9676 - val_loss: 0.5136 - val_acc: 0.8047\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2271 - acc: 0.9721 - val_loss: 0.5024 - val_acc: 0.8099\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.2099 - acc: 0.9777 - val_loss: 0.5037 - val_acc: 0.7917\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.1968 - acc: 0.9833 - val_loss: 0.4993 - val_acc: 0.8047\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.1842 - acc: 0.9900 - val_loss: 0.4996 - val_acc: 0.7969\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1730 - acc: 0.9888 - val_loss: 0.4987 - val_acc: 0.8021\n",
      "Epoch 20/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1613 - acc: 0.9933 - val_loss: 0.5045 - val_acc: 0.8151\n",
      "Epoch 21/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1521 - acc: 0.9955 - val_loss: 0.5027 - val_acc: 0.8125\n",
      "Epoch 22/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1434 - acc: 0.9978 - val_loss: 0.4952 - val_acc: 0.7969\n",
      "Epoch 23/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1352 - acc: 0.9989 - val_loss: 0.4932 - val_acc: 0.8021\n",
      "Epoch 24/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1289 - acc: 0.9989 - val_loss: 0.4927 - val_acc: 0.7943\n",
      "Epoch 25/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1217 - acc: 0.9989 - val_loss: 0.4899 - val_acc: 0.7969\n",
      "Epoch 26/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1153 - acc: 1.0000 - val_loss: 0.4892 - val_acc: 0.8047\n",
      "Epoch 27/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1100 - acc: 1.0000 - val_loss: 0.4857 - val_acc: 0.8021\n",
      "Epoch 28/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1048 - acc: 1.0000 - val_loss: 0.4845 - val_acc: 0.7995\n",
      "Epoch 29/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0997 - acc: 1.0000 - val_loss: 0.4876 - val_acc: 0.8151\n",
      "Epoch 30/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0953 - acc: 1.0000 - val_loss: 0.4790 - val_acc: 0.8073\n",
      "Epoch 31/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0911 - acc: 1.0000 - val_loss: 0.4798 - val_acc: 0.8177\n",
      "Epoch 32/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0874 - acc: 1.0000 - val_loss: 0.4785 - val_acc: 0.8203\n",
      "Epoch 33/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0843 - acc: 1.0000 - val_loss: 0.4759 - val_acc: 0.7995\n",
      "Epoch 34/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0806 - acc: 1.0000 - val_loss: 0.4735 - val_acc: 0.8177\n",
      "Epoch 35/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0768 - acc: 1.0000 - val_loss: 0.4718 - val_acc: 0.8047\n",
      "Epoch 36/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0739 - acc: 1.0000 - val_loss: 0.4699 - val_acc: 0.8177\n",
      "Epoch 37/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0713 - acc: 1.0000 - val_loss: 0.4802 - val_acc: 0.8073\n",
      "Epoch 38/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0680 - acc: 1.0000 - val_loss: 0.4680 - val_acc: 0.8203\n",
      "Epoch 39/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0654 - acc: 1.0000 - val_loss: 0.4671 - val_acc: 0.8177\n",
      "Epoch 40/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.0631 - acc: 1.0000 - val_loss: 0.4672 - val_acc: 0.8203\n",
      "Epoch 41/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.0604 - acc: 1.0000 - val_loss: 0.4664 - val_acc: 0.8203\n",
      "Epoch 42/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.0579 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 0.8125\n",
      "Epoch 43/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.0561 - acc: 1.0000 - val_loss: 0.4602 - val_acc: 0.8229\n",
      "Epoch 44/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0538 - acc: 1.0000 - val_loss: 0.4590 - val_acc: 0.8229\n",
      "Epoch 45/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.0525 - acc: 1.0000 - val_loss: 0.4594 - val_acc: 0.8255\n",
      "Epoch 46/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0500 - acc: 1.0000 - val_loss: 0.4569 - val_acc: 0.8229\n",
      "Epoch 47/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0485 - acc: 1.0000 - val_loss: 0.4596 - val_acc: 0.8125\n",
      "Epoch 48/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.0467 - acc: 1.0000 - val_loss: 0.4544 - val_acc: 0.8281\n",
      "Epoch 49/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0454 - acc: 1.0000 - val_loss: 0.4560 - val_acc: 0.8255\n",
      "Epoch 50/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.4587 - val_acc: 0.8151\n",
      "acc: 84.06%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 14s 16ms/step - loss: 0.7497 - acc: 0.5614 - val_loss: 0.7165 - val_acc: 0.6510\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.6976 - acc: 0.6775 - val_loss: 0.6995 - val_acc: 0.5182\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.6684 - acc: 0.6797 - val_loss: 0.6642 - val_acc: 0.7526\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.6324 - acc: 0.7634 - val_loss: 0.6372 - val_acc: 0.7240\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.5836 - acc: 0.8259 - val_loss: 0.6012 - val_acc: 0.7682\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.5343 - acc: 0.8415 - val_loss: 0.5797 - val_acc: 0.7370\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.4830 - acc: 0.8638 - val_loss: 0.5526 - val_acc: 0.7630\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.4378 - acc: 0.8817 - val_loss: 0.5291 - val_acc: 0.7734\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.4003 - acc: 0.8906 - val_loss: 0.5180 - val_acc: 0.7812\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3636 - acc: 0.9141 - val_loss: 0.5192 - val_acc: 0.7682\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3342 - acc: 0.9275 - val_loss: 0.5078 - val_acc: 0.7734\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.3088 - acc: 0.9386 - val_loss: 0.5006 - val_acc: 0.7943\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 15s 17ms/step - loss: 0.2867 - acc: 0.9509 - val_loss: 0.5050 - val_acc: 0.7786\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 14s 15ms/step - loss: 0.2654 - acc: 0.9598 - val_loss: 0.4984 - val_acc: 0.7969\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.2466 - acc: 0.9688 - val_loss: 0.5008 - val_acc: 0.7839\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.2297 - acc: 0.9766 - val_loss: 0.4991 - val_acc: 0.8099\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2136 - acc: 0.9821 - val_loss: 0.5011 - val_acc: 0.7812\n",
      "acc: 83.44%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 15s 17ms/step - loss: 0.7420 - acc: 0.5547 - val_loss: 0.7155 - val_acc: 0.6771\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.7005 - acc: 0.6842 - val_loss: 0.6838 - val_acc: 0.6562\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 8s 9ms/step - loss: 0.6652 - acc: 0.7333 - val_loss: 0.6481 - val_acc: 0.7292\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.6131 - acc: 0.7857 - val_loss: 0.5999 - val_acc: 0.7630\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.5613 - acc: 0.7991 - val_loss: 0.5597 - val_acc: 0.7786\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.5038 - acc: 0.8281 - val_loss: 0.5284 - val_acc: 0.7943\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.4584 - acc: 0.8382 - val_loss: 0.5074 - val_acc: 0.7943\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.4200 - acc: 0.8605 - val_loss: 0.4935 - val_acc: 0.8047\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.3857 - acc: 0.8728 - val_loss: 0.4753 - val_acc: 0.8125\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.3559 - acc: 0.8984 - val_loss: 0.4704 - val_acc: 0.8047\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.3290 - acc: 0.9118 - val_loss: 0.4670 - val_acc: 0.7969\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.3048 - acc: 0.9230 - val_loss: 0.4530 - val_acc: 0.8333\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.2835 - acc: 0.9408 - val_loss: 0.4536 - val_acc: 0.8229\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 8s 9ms/step - loss: 0.2648 - acc: 0.9509 - val_loss: 0.4566 - val_acc: 0.8021\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2459 - acc: 0.9621 - val_loss: 0.4442 - val_acc: 0.8307\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2309 - acc: 0.9688 - val_loss: 0.4402 - val_acc: 0.8229\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.2162 - acc: 0.9710 - val_loss: 0.4370 - val_acc: 0.8307\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 8s 9ms/step - loss: 0.2028 - acc: 0.9777 - val_loss: 0.4351 - val_acc: 0.8307\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.1907 - acc: 0.9833 - val_loss: 0.4412 - val_acc: 0.8125\n",
      "Epoch 20/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1784 - acc: 0.9866 - val_loss: 0.4303 - val_acc: 0.8333\n",
      "Epoch 21/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.1684 - acc: 0.9888 - val_loss: 0.4292 - val_acc: 0.8333\n",
      "Epoch 22/50\n",
      "896/896 [==============================] - 9s 11ms/step - loss: 0.1579 - acc: 0.9922 - val_loss: 0.4457 - val_acc: 0.8125\n",
      "Epoch 23/50\n",
      "896/896 [==============================] - 8s 9ms/step - loss: 0.1496 - acc: 0.9944 - val_loss: 0.4620 - val_acc: 0.8125\n",
      "Epoch 24/50\n",
      "896/896 [==============================] - 8s 9ms/step - loss: 0.1426 - acc: 0.9944 - val_loss: 0.4347 - val_acc: 0.8151\n",
      "acc: 80.62%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 0.7558 - acc: 0.5335 - val_loss: 0.7217 - val_acc: 0.5651\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.7009 - acc: 0.6551 - val_loss: 0.6916 - val_acc: 0.6823\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.6599 - acc: 0.7232 - val_loss: 0.6684 - val_acc: 0.6562\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.6147 - acc: 0.7656 - val_loss: 0.6280 - val_acc: 0.7240\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.5609 - acc: 0.7779 - val_loss: 0.6056 - val_acc: 0.7109\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.5132 - acc: 0.8192 - val_loss: 0.5754 - val_acc: 0.7474\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.4708 - acc: 0.8348 - val_loss: 0.5556 - val_acc: 0.7812\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.4327 - acc: 0.8616 - val_loss: 0.5396 - val_acc: 0.7917\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3987 - acc: 0.8683 - val_loss: 0.5221 - val_acc: 0.7812\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3660 - acc: 0.8895 - val_loss: 0.5193 - val_acc: 0.7917\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3383 - acc: 0.9018 - val_loss: 0.5006 - val_acc: 0.8125\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.3144 - acc: 0.9275 - val_loss: 0.4977 - val_acc: 0.8021\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2863 - acc: 0.9330 - val_loss: 0.4854 - val_acc: 0.8099\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.2655 - acc: 0.9464 - val_loss: 0.4821 - val_acc: 0.8073\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2471 - acc: 0.9554 - val_loss: 0.4775 - val_acc: 0.8177\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.2290 - acc: 0.9676 - val_loss: 0.4720 - val_acc: 0.8125\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.2124 - acc: 0.9732 - val_loss: 0.4703 - val_acc: 0.8177\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1977 - acc: 0.9810 - val_loss: 0.4682 - val_acc: 0.8229\n",
      "Epoch 19/50\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.1854 - acc: 0.9821 - val_loss: 0.4638 - val_acc: 0.8099\n",
      "Epoch 20/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.1732 - acc: 0.9866 - val_loss: 0.4630 - val_acc: 0.8099\n",
      "Epoch 21/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.1638 - acc: 0.9900 - val_loss: 0.4629 - val_acc: 0.8177\n",
      "Epoch 22/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1529 - acc: 0.9922 - val_loss: 0.4617 - val_acc: 0.8177\n",
      "Epoch 23/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1436 - acc: 0.9955 - val_loss: 0.4633 - val_acc: 0.8177\n",
      "Epoch 24/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.1374 - acc: 0.9967 - val_loss: 0.4598 - val_acc: 0.8073\n",
      "Epoch 25/50\n",
      "896/896 [==============================] - 10s 12ms/step - loss: 0.1296 - acc: 0.9967 - val_loss: 0.4654 - val_acc: 0.8203\n",
      "Epoch 26/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1229 - acc: 0.9978 - val_loss: 0.4587 - val_acc: 0.8203\n",
      "Epoch 27/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.1164 - acc: 0.9989 - val_loss: 0.4584 - val_acc: 0.8203\n",
      "Epoch 28/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1112 - acc: 0.9989 - val_loss: 0.4572 - val_acc: 0.8125\n",
      "Epoch 29/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.1055 - acc: 0.9978 - val_loss: 0.4609 - val_acc: 0.8229\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/896 [==============================] - 8s 9ms/step - loss: 0.1003 - acc: 0.9989 - val_loss: 0.4550 - val_acc: 0.8151\n",
      "Epoch 31/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0957 - acc: 0.9989 - val_loss: 0.4546 - val_acc: 0.8203\n",
      "Epoch 32/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0920 - acc: 0.9989 - val_loss: 0.4565 - val_acc: 0.8177\n",
      "Epoch 33/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0880 - acc: 0.9989 - val_loss: 0.4546 - val_acc: 0.8177\n",
      "Epoch 34/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0845 - acc: 0.9989 - val_loss: 0.4507 - val_acc: 0.8255\n",
      "Epoch 35/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0805 - acc: 1.0000 - val_loss: 0.4498 - val_acc: 0.8177\n",
      "Epoch 36/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0777 - acc: 0.9989 - val_loss: 0.4522 - val_acc: 0.8203\n",
      "Epoch 37/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0748 - acc: 1.0000 - val_loss: 0.4480 - val_acc: 0.8307\n",
      "Epoch 38/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0714 - acc: 1.0000 - val_loss: 0.4483 - val_acc: 0.8229\n",
      "Epoch 39/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0690 - acc: 1.0000 - val_loss: 0.4446 - val_acc: 0.8281\n",
      "Epoch 40/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0663 - acc: 1.0000 - val_loss: 0.4406 - val_acc: 0.8255\n",
      "Epoch 41/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0641 - acc: 1.0000 - val_loss: 0.4409 - val_acc: 0.8229\n",
      "Epoch 42/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0612 - acc: 1.0000 - val_loss: 0.4462 - val_acc: 0.8281\n",
      "Epoch 43/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0594 - acc: 1.0000 - val_loss: 0.4391 - val_acc: 0.8255\n",
      "Epoch 44/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0569 - acc: 1.0000 - val_loss: 0.4386 - val_acc: 0.8281\n",
      "Epoch 45/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0548 - acc: 1.0000 - val_loss: 0.4382 - val_acc: 0.8255\n",
      "Epoch 46/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0531 - acc: 1.0000 - val_loss: 0.4363 - val_acc: 0.8307\n",
      "Epoch 47/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.0508 - acc: 1.0000 - val_loss: 0.4353 - val_acc: 0.8333\n",
      "Epoch 48/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.4397 - val_acc: 0.8255\n",
      "Epoch 49/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0472 - acc: 1.0000 - val_loss: 0.4360 - val_acc: 0.8333\n",
      "Epoch 50/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.0459 - acc: 1.0000 - val_loss: 0.4367 - val_acc: 0.8307\n",
      "acc: 83.12%\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/50\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 0.7591 - acc: 0.4944 - val_loss: 0.7402 - val_acc: 0.5052\n",
      "Epoch 2/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.7226 - acc: 0.5469 - val_loss: 0.7125 - val_acc: 0.5964\n",
      "Epoch 3/50\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 0.6923 - acc: 0.6228 - val_loss: 0.6925 - val_acc: 0.5260\n",
      "Epoch 4/50\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 0.6645 - acc: 0.6752 - val_loss: 0.6730 - val_acc: 0.5599\n",
      "Epoch 5/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.6359 - acc: 0.7054 - val_loss: 0.6495 - val_acc: 0.7135\n",
      "Epoch 6/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.6064 - acc: 0.7578 - val_loss: 0.6320 - val_acc: 0.7109\n",
      "Epoch 7/50\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.5772 - acc: 0.7801 - val_loss: 0.6170 - val_acc: 0.7240\n",
      "Epoch 8/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.5474 - acc: 0.8103 - val_loss: 0.6057 - val_acc: 0.7344\n",
      "Epoch 9/50\n",
      "896/896 [==============================] - 9s 10ms/step - loss: 0.5149 - acc: 0.8460 - val_loss: 0.5989 - val_acc: 0.7214\n",
      "Epoch 10/50\n",
      "896/896 [==============================] - 11s 12ms/step - loss: 0.4838 - acc: 0.8705 - val_loss: 0.5914 - val_acc: 0.7422\n",
      "Epoch 11/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.4552 - acc: 0.8895 - val_loss: 0.5824 - val_acc: 0.7578\n",
      "Epoch 12/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.4261 - acc: 0.9152 - val_loss: 0.5752 - val_acc: 0.7630\n",
      "Epoch 13/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3970 - acc: 0.9330 - val_loss: 0.5631 - val_acc: 0.7760\n",
      "Epoch 14/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3733 - acc: 0.9464 - val_loss: 0.5532 - val_acc: 0.7786\n",
      "Epoch 15/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3505 - acc: 0.9520 - val_loss: 0.5525 - val_acc: 0.7786\n",
      "Epoch 16/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3282 - acc: 0.9699 - val_loss: 0.5821 - val_acc: 0.7396\n",
      "Epoch 17/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.3090 - acc: 0.9710 - val_loss: 0.5535 - val_acc: 0.7708\n",
      "Epoch 18/50\n",
      "896/896 [==============================] - 10s 11ms/step - loss: 0.2901 - acc: 0.9754 - val_loss: 0.5532 - val_acc: 0.7734\n",
      "acc: 85.62%\n",
      "78.29% (+/- 9.89%)\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True),\n",
    "        keras.layers.GlobalMaxPool1D(),\n",
    "        keras.layers.Dense(4, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=50, batch_size=8, validation_split=0.3, verbose=1, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we allow the model to train our embedding layer, we get high accuracies, low loss, and the model does not converge as fast. This leads me to believe that word embeddings are definitley a good feature set, and produce a more generalizing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
