{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Neural Models 2\n",
    "\n",
    "## Feedforward Neural Network\n",
    "\n",
    "Following an experiment to compare neural models, we discovered odd results showing that bag of words could outperform embeddings. This experiment attempts to tweak the embeddings to show the expected results under the assumption that the problem is not the amount of data. If the problem is the amount of data we will investigate this in another experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will show Bag of Words results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import training_helpers\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Embedding, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from pandas import DataFrame\n",
    "from seaborn import boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "read_existing_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = training_helpers.get_data_frame()\n",
    "raw_features = data_frame['review']\n",
    "labels = [x for x in data_frame['deceptive']]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_features)\n",
    "bow_features = tokenizer.texts_to_matrix(raw_features, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 144us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 155us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 139us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 172us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 172us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 190us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 199us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 219us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 129us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 196us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_bow_model():\n",
    "  model = Sequential([\n",
    "      Dense(16, activation=relu, input_shape=(corpus_vocab_size,), kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "ff_bow_scores = run_cross_validate(get_ff_bow_model, bow_features, labels, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will get results for embeddings. This time I will use Word2Vec. Although this was not trained directly on words from our dataset, the Word2Vec has a higher dimensionality (making it harder to run on our machines) and so may show better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = None\n",
    "embedding_length = 0\n",
    "if read_existing_embeddings:\n",
    "  embedding_length=300\n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  with open('data.json', 'r') as infile:\n",
    "      data = json.load(infile)\n",
    "      for i in range(len(data)):\n",
    "          embedding_matrix[i] = np.array(data[i], dtype=np.float32)\n",
    "else:\n",
    "  word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin\",\n",
    "                                                                 binary=True)\n",
    "  embedding_length = word_vectors.vector_size\n",
    "    \n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  for word, index in corpus_words.items():\n",
    "    if word in word_vectors.vocab:\n",
    "      embedding_matrix[index] = np.array(word_vectors[word], dtype=np.float32)\n",
    "\n",
    "  with open('data.json', 'w') as outfile:\n",
    "      json.dump(embedding_matrix.tolist(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ff_wv_model():\n",
    "  model_ff_wv = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], trainable=False,\n",
    "                input_length=corpus_vocab_size),\n",
    "      Flatten(),\n",
    "      Dense(16, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model_ff_wv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model_ff_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1280, 9839) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.0891 - acc: 0.5882 - val_loss: 1.2110 - val_acc: 0.6198\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 38s 43ms/step - loss: 1.2013 - acc: 0.7132 - val_loss: 1.3584 - val_acc: 0.6745\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 38s 42ms/step - loss: 1.2186 - acc: 0.7333 - val_loss: 1.4261 - val_acc: 0.7005\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 38s 42ms/step - loss: 1.2512 - acc: 0.7054 - val_loss: 1.3712 - val_acc: 0.6484\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 38s 43ms/step - loss: 1.2558 - acc: 0.7533 - val_loss: 1.4386 - val_acc: 0.6719\n",
      "320/320 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1280, 9839) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 0.9364 - acc: 0.5569 - val_loss: 1.0129 - val_acc: 0.6198\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 40s 44ms/step - loss: 1.1214 - acc: 0.6853 - val_loss: 1.2568 - val_acc: 0.6693\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2156 - acc: 0.7355 - val_loss: 1.3580 - val_acc: 0.6823\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2638 - acc: 0.7533 - val_loss: 1.5648 - val_acc: 0.6693\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2139 - acc: 0.7879 - val_loss: 1.4685 - val_acc: 0.6354\n",
      "320/320 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1280, 9839) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 47s 52ms/step - loss: 1.1827 - acc: 0.6306 - val_loss: 1.3397 - val_acc: 0.6823\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.1347 - acc: 0.7779 - val_loss: 1.4744 - val_acc: 0.7135\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 43s 47ms/step - loss: 1.2657 - acc: 0.7746 - val_loss: 1.3647 - val_acc: 0.6979\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2354 - acc: 0.8013 - val_loss: 1.5523 - val_acc: 0.6901\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2216 - acc: 0.8114 - val_loss: 1.4533 - val_acc: 0.7109\n",
      "320/320 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1280, 9839) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 51s 57ms/step - loss: 0.9980 - acc: 0.5904 - val_loss: 1.1417 - val_acc: 0.6328\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2157 - acc: 0.6775 - val_loss: 1.3167 - val_acc: 0.6797\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2370 - acc: 0.7243 - val_loss: 1.2874 - val_acc: 0.6875\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2164 - acc: 0.7143 - val_loss: 1.3321 - val_acc: 0.6797\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.2309 - acc: 0.7355 - val_loss: 1.3525 - val_acc: 0.6901\n",
      "320/320 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1280, 9839) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 64s 71ms/step - loss: 0.9597 - acc: 0.5413 - val_loss: 1.0838 - val_acc: 0.5859\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.0716 - acc: 0.6696 - val_loss: 1.1920 - val_acc: 0.6693\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.0781 - acc: 0.7277 - val_loss: 1.0938 - val_acc: 0.6823\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 44s 49ms/step - loss: 1.0370 - acc: 0.7277 - val_loss: 1.2011 - val_acc: 0.6536\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 1.0845 - acc: 0.7779 - val_loss: 1.2248 - val_acc: 0.6979\n",
      "320/320 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "ff_wv_scores = run_cross_validate(get_ff_wv_model, bow_features, labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.875, 0.8625, 0.88125, 0.88125, 0.86875, 0.91875, 0.875, 0.8625, 0.88125, 0.8875]\n",
      "Word vectors:  [0.628125, 0.709375, 0.665625, 0.703125, 0.659375]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", ff_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", ff_wv_scores['accuracies'])\n",
    "\n",
    "ff_scores_entries =[('Bag of Words', x) for x in ff_bow_scores['accuracies']] + [('Word Vectors', x) for x in ff_wv_scores['accuracies']]\n",
    "ff_scores_data_frame = DataFrame(ff_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=ff_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like word vectors are doing better now! Although they should be more accurate than Bag of Words, unless this is an exceptional case. The next step here is to investigate how BoW and word vectors perform on more data, since a small amount of data is a case known to cause results like this. It is also very unlikely Bag of Words will perform as well on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks were let down the most by word embeddings. Research has shown that word embeddings perform better than Bag of Words (Convolutional Neural Networks for Sentence Classification, Yoon Kim 2014). We will our convolutional network on Word2Vec embeddings to see if we obtain an improved accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 9839, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in bow_features]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.4324 - acc: 0.7885 - val_loss: 0.6778 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.1362 - acc: 0.9543 - val_loss: 0.7423 - val_acc: 0.6536\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 10s 10ms/step - loss: 0.0747 - acc: 0.9791 - val_loss: 1.0627 - val_acc: 0.6028\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 10s 10ms/step - loss: 0.0461 - acc: 0.9891 - val_loss: 1.0094 - val_acc: 0.6189\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.0248 - acc: 0.9930 - val_loss: 1.3448 - val_acc: 0.6051\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.4264 - acc: 0.7994 - val_loss: 0.6557 - val_acc: 0.6443\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.1387 - acc: 0.9553 - val_loss: 0.7479 - val_acc: 0.6305\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.0733 - acc: 0.9742 - val_loss: 0.9434 - val_acc: 0.6259\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.0415 - acc: 0.9940 - val_loss: 1.0827 - val_acc: 0.6166\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.0261 - acc: 0.9940 - val_loss: 1.1967 - val_acc: 0.6282\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.4465 - acc: 0.7786 - val_loss: 0.6975 - val_acc: 0.6236\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.1425 - acc: 0.9444 - val_loss: 0.8509 - val_acc: 0.6189\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 9s 8ms/step - loss: 0.0708 - acc: 0.9851 - val_loss: 0.9089 - val_acc: 0.6143\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.0432 - acc: 0.9911 - val_loss: 1.0334 - val_acc: 0.6305\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.0236 - acc: 0.9970 - val_loss: 1.1864 - val_acc: 0.6143\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.4243 - acc: 0.7885 - val_loss: 0.6911 - val_acc: 0.6374\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.1245 - acc: 0.9643 - val_loss: 0.7451 - val_acc: 0.6674\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 12s 12ms/step - loss: 0.0765 - acc: 0.9752 - val_loss: 0.9545 - val_acc: 0.6374\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.0450 - acc: 0.9911 - val_loss: 1.0052 - val_acc: 0.6189\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.0233 - acc: 0.9970 - val_loss: 1.2420 - val_acc: 0.6028\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.4022 - acc: 0.8123 - val_loss: 0.7691 - val_acc: 0.6143\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.1173 - acc: 0.9652 - val_loss: 0.8098 - val_acc: 0.6397\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 10s 10ms/step - loss: 0.0619 - acc: 0.9871 - val_loss: 0.8985 - val_acc: 0.6467\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.0361 - acc: 0.9950 - val_loss: 1.0816 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 9s 9ms/step - loss: 0.0239 - acc: 0.9970 - val_loss: 1.3569 - val_acc: 0.5751\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 10s 10ms/step - loss: 0.4439 - acc: 0.7746 - val_loss: 0.7618 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 11s 11ms/step - loss: 0.1520 - acc: 0.9513 - val_loss: 0.8029 - val_acc: 0.6443\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 10s 10ms/step - loss: 0.0742 - acc: 0.9801 - val_loss: 0.9421 - val_acc: 0.6328\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 8s 7ms/step - loss: 0.0462 - acc: 0.9881 - val_loss: 1.0902 - val_acc: 0.6005\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0291 - acc: 0.9970 - val_loss: 1.2390 - val_acc: 0.6005\n",
      "160/160 [==============================] - 1s 3ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 8ms/step - loss: 0.4206 - acc: 0.7885 - val_loss: 0.7202 - val_acc: 0.6420\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.1246 - acc: 0.9553 - val_loss: 0.8187 - val_acc: 0.6189\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0619 - acc: 0.9841 - val_loss: 0.9767 - val_acc: 0.6051\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 8s 8ms/step - loss: 0.0334 - acc: 0.9960 - val_loss: 1.1138 - val_acc: 0.5982\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 11s 11ms/step - loss: 0.0195 - acc: 0.9990 - val_loss: 1.1899 - val_acc: 0.6212\n",
      "160/160 [==============================] - 1s 3ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 8ms/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.6634 - val_acc: 0.6420\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.1395 - acc: 0.9553 - val_loss: 0.7723 - val_acc: 0.6397\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0818 - acc: 0.9772 - val_loss: 0.9780 - val_acc: 0.6166\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0547 - acc: 0.9851 - val_loss: 0.9796 - val_acc: 0.6351\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0315 - acc: 0.9960 - val_loss: 1.1264 - val_acc: 0.6236\n",
      "160/160 [==============================] - 1s 3ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 8ms/step - loss: 0.4251 - acc: 0.7954 - val_loss: 0.6879 - val_acc: 0.6282\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 12s 11ms/step - loss: 0.1296 - acc: 0.9583 - val_loss: 0.7713 - val_acc: 0.6328\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0653 - acc: 0.9881 - val_loss: 0.8976 - val_acc: 0.6236\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0381 - acc: 0.9891 - val_loss: 0.9888 - val_acc: 0.6212\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0245 - acc: 0.9960 - val_loss: 1.1244 - val_acc: 0.6374\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 9s 8ms/step - loss: 0.4590 - acc: 0.7676 - val_loss: 0.6749 - val_acc: 0.6397\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.1401 - acc: 0.9513 - val_loss: 0.7494 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 7s 7ms/step - loss: 0.0717 - acc: 0.9811 - val_loss: 0.9037 - val_acc: 0.6120\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 10s 10ms/step - loss: 0.0403 - acc: 0.9930 - val_loss: 1.0770 - val_acc: 0.6097\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 10s 9ms/step - loss: 0.0257 - acc: 0.9970 - val_loss: 1.2016 - val_acc: 0.5935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_conv_bow_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(1, 10),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(1, corpus_vocab_size, 1),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(pool_size=(1, 10)),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "conv_bow_scores = run_cross_validate(get_conv_bow_model, convolutional_data, labels, cv=10, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_words = [text_to_word_sequence(x) for x in raw_features]\n",
    "max_review_len = max([len(x) for x in reviews_words])\n",
    "\n",
    "vectorized_reviews = np.zeros((len(reviews_words), max_review_len, 300, 1))\n",
    "for i, review in enumerate(reviews_words):\n",
    "    for j, word in enumerate(review):\n",
    "        vectorized_reviews[i][j] = \n",
    "        np.append(review_vectors, embedding_matrix[corpus_words[word]])\n",
    "    np.append(vectorized_reviews, np.array(review_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1332,) labels (1332, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_13_input to have 4 dimensions, but got array with shape (1332, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2579a718c94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mconv_wv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_conv_wv_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Amazoff/LUCAS/scripts/cross_validate.py\u001b[0m in \u001b[0;36mrun_cross_validate\u001b[0;34m(get_model, X, y, cv, categorical, add_target_dim, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m     model.fit(np.array(training_X), training_y, epochs=12, batch_size=16,\n\u001b[1;32m     30\u001b[0m               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m               callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    321\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_13_input to have 4 dimensions, but got array with shape (1332, 1)"
     ]
    }
   ],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 300),\n",
    "          data_format=\"channels_first\",\n",
    "          input_shape=(1, 381, 300),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(strides=(1, 1), pool_size=(2, 1), data_format=\"channels_first\"),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, vectorized_reviews, labels, cv=6, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
