{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Better Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "This notebook has been heavily unit tested, and as a result a lot of code has been removed from the notebook itself. I have demonstrated as much as possible through importing units and running them on example values.\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpNYC\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "num_each_class = 8141\n",
    "reviews = shuffle(review_set.reviews)\n",
    "\n",
    "i = 0\n",
    "fake_reviews = []\n",
    "for x in reviews:\n",
    "  if i == num_each_class:\n",
    "    break\n",
    "  if x.label:\n",
    "    fake_reviews.append(x)\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "genuine_reviews = []\n",
    "for x in reviews:\n",
    "  if i == num_each_class:\n",
    "    break\n",
    "  if x.label == False:\n",
    "    fake_reviews.append(x)\n",
    "    i+=1\n",
    "    \n",
    "all_reviews = [(x, find_words(x.review_content)) for x in shuffle(fake_reviews + genuine_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16282\n"
     ]
    }
   ],
   "source": [
    "print(len(all_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are:\n",
    "* Length of the review\n",
    "* Average word length\n",
    "* Number of sentences\n",
    "* Average sentence length\n",
    "* Percentage of numerals\n",
    "* Percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 5.5, 2, 26.0, 0.25, 0.25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import structural_features\n",
    "\n",
    "review = review_pb2.Review()\n",
    "review.review_content = \"1 very horrible restaurant. Eat 10 Starbucks instead.\"\n",
    "structural_features((review, [\"1\", \"very\", \"horrible\", \"restaurant\", \"Eat\", \"10\", \"Starbucks\", \"instead\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_structural = [structural_features(x) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, '0.3333333333333333'),\n",
       " (11, '0.3333333333333333'),\n",
       " (35, '0.3333333333333333')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from exp2_feature_extraction import pos_features\n",
    "\n",
    "sample_pos_features = pos_features([\"Dog\", \"and\", \"beautiful\"], nltk)\n",
    "[(i, str(sample_pos_features[i])) for i in range(0, len(sample_pos_features)) if sample_pos_features[i] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 5 is JJ, which is for adjective. This represents \"beautiful\" in the input. 11 is NNP, which is proper noun, which represents \"Dog\". 35 is CC which is coordinating conjuction, which is \"and\". These are given as percentages, where each here is one third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pos = [pos_features(x[1], nltk) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features. These will be:\n",
    "* Percentage of words that have positive sentiment\n",
    "* Percentage of words that have negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "none: 0.0\n",
      "good: 0.4404\n",
      "bad: -0.5423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "print(\"Scores:\")\n",
    "print(\"none:\", sentiment_analyzer.polarity_scores(\"none\")[\"compound\"])\n",
    "print(\"good:\", sentiment_analyzer.polarity_scores(\"good\")[\"compound\"])\n",
    "print(\"bad:\", sentiment_analyzer.polarity_scores(\"bad\")[\"compound\"])\n",
    "\n",
    "from exp2_feature_extraction import sentiment_features\n",
    "sentiment_features([\"good\", \"good\", \"none\", \"bad\"], sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sentiment = [sentiment_features(x[1], sentiment_analyzer) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the topic model features from LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some data cleaning, which will improve our results when generating LDA topics. This cleaning is stemming and lemmatization, removing words with three or less characters, and removing stopwords.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming will reduce a word to a 'stem' form, which can be used to normalise words that mean the same thing. For example 'cleanly' and 'cleanest' would be stemmed to 'clean'. Lemmatization uses a vocabulary and morphological analysis to more intelligently normalise words, for example 'car' and 'automobile' could go to 'vehicle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gonna', 'stay', 'coffe', 'littl', 'store', 'happili', 'surpris']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import preprocess_words\n",
    "preprocess_words([\"I\", \"was\", \"gonna\", \"just\", \"stay\", \"in\", \"for\", \"coffee\", \"but\", \"this\", \"little\", \"store\", \"happily\", \"surprised\", \"me\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def get_topic_features_maker(reviews, num_topics=100, bigrams=False):\n",
    "  preprocessed_words = [preprocess_words(x[1], bigrams=bigrams) for x in reviews]\n",
    "  \n",
    "  dictionary = gensim.corpora.Dictionary(preprocessed_words)\n",
    "  dictionary.filter_extremes(no_below=15, no_above=0.33, keep_n=100000)\n",
    "  bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_words]\n",
    "  lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=2)\n",
    "    \n",
    "  def make_topic_features(review_words):\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess_words(review_words, bigrams=bigrams)))\n",
    "    return topic_features(topics, num_topics)\n",
    "\n",
    "  def get_terms(topic_id):\n",
    "    return [dictionary.id2token[x[0]] + \" \" + str(x[1]) for x in lda_model.get_topic_terms(topic_id, 5)]\n",
    "  return (make_topic_features, get_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "topic_features_maker, get_topic_terms = get_topic_features_maker(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_terms(term_frequencies, term_getter):\n",
    "  for topic_id in range(0, len(term_frequencies)):\n",
    "    if term_frequencies[topic_id] > 0:\n",
    "      print(\"----- Topic\", topic_id)\n",
    "      print(\"\\n\".join(term_getter(topic_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Topic 19\n",
      "pizza 0.21524006\n",
      "crust 0.04352088\n",
      "chees 0.024899844\n",
      "sauc 0.019861035\n",
      "top 0.018330967\n",
      "----- Topic 88\n",
      "pasta 0.12399301\n",
      "vegetarian 0.032519918\n",
      "dish 0.029265996\n",
      "ravioli 0.021183636\n",
      "delici 0.020061402\n",
      "----- Topic 97\n",
      "ramen 0.21542433\n",
      "broth 0.07984852\n",
      "noodl 0.04758751\n",
      "ippudo 0.047039736\n",
      "wait 0.041843496\n"
     ]
    }
   ],
   "source": [
    "from exp2_feature_extraction import topic_features\n",
    "\n",
    "words = [\"Pizza\", \"Pasta\", \"Ramen\", \"Noodles\"]\n",
    "tf = topic_features_maker(words)\n",
    "print_topic_terms(tf, get_topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unigram_topic = [topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_topic_features_maker, get_topic_terms = get_topic_features_maker(all_reviews, bigrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Topic 12\n",
      "fri chicken 0.18048993\n",
      "bloodi mari 0.10303593\n",
      "comfort food 0.060871754\n",
      "time love 0.056455065\n",
      "pomm frite 0.04096748\n",
      "----- Topic 74\n",
      "place order 0.13034548\n",
      "dessert good 0.06687016\n",
      "bone marrow 0.062271353\n",
      "recommend friend 0.05877991\n",
      "lobster roll 0.050766602\n"
     ]
    }
   ],
   "source": [
    "words = [\"Fried\", \"Chicken\", \"Pizza\", \"Slice\"]\n",
    "tf2 = bigram_topic_features_maker(words)\n",
    "print_topic_terms(tf2, get_topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bigram_topic = [bigram_topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes:\n",
    "* The maximum number of reviews in a day\n",
    "* Average review length\n",
    "* Standard deviation of reviewer's ratings\n",
    "* Percentage of positive review ratings\n",
    "* Percentage of negative review ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6.0, 1.4142135623730951, 0.5, 0.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviewer_features\n",
    "\n",
    "review1 = review_pb2.Review()\n",
    "review1.review_content=\"1234\"\n",
    "review1.date=\"2018-11-12\"\n",
    "review1.rating=2\n",
    "\n",
    "review2 = review_pb2.Review()\n",
    "review2.review_content=\"12345678\"\n",
    "review2.date=\"2018-11-12\"\n",
    "review2.rating=4\n",
    "\n",
    "user_id = 1234\n",
    "reviewer_map = {\n",
    "    user_id: [review1, review2]\n",
    "}\n",
    "\n",
    "reviewer_features(user_id, reviewer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, our first feature is the number of reviews on this day (2), the average review length (6.0), the standard devation of our ratings (1.41...), percentage of positive review ratings (0.5) and percentage of negative review ratings (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "reviews_reviewer_map = reviews_by_reviewer([x[0] for x in all_reviews])\n",
    "features_reviewer = [reviewer_features(x[0].user_id, reviews_reviewer_map) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "predictor_features = hstack([coo_matrix(features_structural), coo_matrix(features_sentiment),\n",
    "                             coo_matrix(features_pos), coo_matrix(features_unigram_topic),\n",
    "                             coo_matrix(features_bigram_topic), coo_matrix(features_reviewer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x[0].label for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.01517105, 0.01427197, 0.01396346, 0.01411963, 0.01473594]),\n",
       " 'score_time': array([0.00193429, 0.00168014, 0.00157595, 0.00183821, 0.00153089]),\n",
       " 'test_score': array([0.58870473, 0.60380835, 0.59090909, 0.60534398, 0.61179361])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=5, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not seem to be able to replicate the results from the paper. Adding more topics for bigrams/unigrams seems to have virtually no difference. Using bigrams alone for lda topics seems to improve by 0.01. By joining unigrams and bigrams to be modelled by the same LDA we get about - 0.005.\n",
    "\n",
    "Based on the paper, the possible differences are:\n",
    "\n",
    "* Structural features - all very clear, difficult to get wrong\n",
    "* POS Percentages - perhaps another POS tagger could improve results. We are using one from nltk, but there are alternatives\n",
    "* Semantic features - perhaps another Semantic tagger could improve results, but it's hard to imagine it being that much better at tagging\n",
    "* Unigram / Bigram features - It's not very obvious exactly what they're doing. Maybe a better LDA topic modeller could be used? The paper claims that high accuracy can be obtained through just unigrams and bigrams, so this could be tried too.\n",
    "* Reviewer features - all very clear, difficult to get this wrong.\n",
    "\n",
    "Let's try with just Unigrams/Bigrams to see how accurate we are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_accuracy(features):\n",
    "  predictor_features = hstack([coo_matrix(x) for x in features])\n",
    "  naive_bayes = MultinomialNB()\n",
    "  fold = 5\n",
    "  results = cross_validate(naive_bayes, predictor_features, targets, cv=fold, return_train_score=False)\n",
    "  return sum([x for x in results['test_score']])/fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams only: 0.6099995399719156\n"
     ]
    }
   ],
   "source": [
    "print(\"ngrams only:\", avg_accuracy([features_unigram_topic, features_bigram_topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of interest, let's try training with only our other features to see how predictive they are alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags only:   0.5674998454003978\n",
      "Structural only: 0.5959359158254186\n",
      "Sentiment only:  0.5315682206566186\n",
      "Reviewer only:   0.6227137358352828\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags only:  \", avg_accuracy([features_pos]))\n",
    "print(\"Structural only:\", avg_accuracy([features_structural]))\n",
    "print(\"Sentiment only: \", avg_accuracy([features_sentiment]))\n",
    "print(\"Reviewer only:  \", avg_accuracy([features_reviewer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though our best predicting features are the reviewer features and ngrams. Let's try combining them. To avoid swamping the reviewer features, we will reduce our topics to a small number, 10 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_topic_features_predictive_maker, get_topic_terms = get_topic_features_maker(all_reviews, num_topics=10)\n",
    "features_unigram_topic_predictive = [unigram_topic_features_predictive_maker(x[1]) for x in all_reviews]\n",
    "\n",
    "bigram_topic_features_predictive_maker, get_topic_terms = get_topic_features_maker(all_reviews, num_topics=10,\n",
    "                                                                                   bigrams=True)\n",
    "features_bigram_topic_predictive = [bigram_topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249246609743847"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_accuracy([features_reviewer, features_unigram_topic_predictive, features_bigram_topic_predictive])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it seems to be better to use only unigram topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams without bigrams: 0.6280563964265621\n",
      "bigrams without unigrams: 0.6251705874634051\n"
     ]
    }
   ],
   "source": [
    "print(\"unigrams without bigrams:\", avg_accuracy([features_reviewer, features_unigram_topic]))\n",
    "print(\"bigrams without unigrams:\", avg_accuracy([features_reviewer, features_bigram_topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a smaller number of topics (like we have in the predictive feature sets) actually decreases the accuracy for unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams without bigrams: 0.6274427491278319\n",
      "bigrams without unigrams: 0.6252319748176102\n"
     ]
    }
   ],
   "source": [
    "print(\"unigrams without bigrams:\", avg_accuracy([features_reviewer, features_unigram_topic_predictive]))\n",
    "print(\"bigrams without unigrams:\", avg_accuracy([features_reviewer, features_bigram_topic_predictive]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try forgetting about LDA topic models and try using bag of words to generate our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x[0].review_content for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "unigram_count_vect = CountVectorizer()\n",
    "features_ngram_bow = unigram_count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67018818165227"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [features_reviewer for i in range(0, 4)]\n",
    "features.append(features_ngram_bow)\n",
    "avg_accuracy(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that bag of words is unexpectedly accurate. Oddly TFIDF is less accurate than Bag of words.\n",
    "\n",
    "Bag of words is not supposed to be good for classifying reviews. Is it the case that those 'non-interesting' words like 'the', 'a', 'is', etc are in some way predictive that someone is faking their opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64721913023018"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "features = [features_sentiment for i in range(0, 9)]\n",
    "features_ngram_tfidf = tfidf_transformer.fit_transform(features_ngram_bow)\n",
    "features.append(features_ngram_tfidf)\n",
    "avg_accuracy(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also trying this with HashingVectorizer, again we get a lower accuracy than bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/feature_extraction/hashing.py:102: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/feature_extraction/hashing.py:102: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6536679698281909"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hashing_vectorizer = HashingVectorizer(n_features=9950, non_negative=True)\n",
    "\n",
    "features_ngram_hash = hashing_vectorizer.fit_transform(corpus)\n",
    "avg_accuracy([features_pos, features_pos, features_ngram_hash])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this experiment it appears that we should not attempt to reproduce the results of the mentioned paper. It should also be very easy to use more data than the paper used. The data source used by the paper has more genuine and fake reviews available, so we can train with more data.\n",
    "\n",
    "We can see that reviewer centric data is helpful in classifying fake or genuine. There is space here for adding new features, and this could be a good way to improve accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
