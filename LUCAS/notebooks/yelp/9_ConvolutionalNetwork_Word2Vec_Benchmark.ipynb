{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network Benchmark\n",
    "\n",
    "To obtain a benchmark for a basic convolutional network, we will create a simple network that informs us what to expect when using these networks. This will not contain any novel specializations, it is done to find a baseline which we can improve upon.\n",
    "\n",
    "The architecture used here is inspired by the research in the form of the paper ['A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification'](https://arxiv.org/pdf/1510.03820.pdf), and it not complex.\n",
    "\n",
    "While previous experiments have been done to find applicability of convolutional networks, this is the first to run it over a large set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from exp8_feature_extraction import get_balanced_dataset\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_contents = [x.review_content for x in all_reviews]\n",
    "labels = [1 if x.label else 0 for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_reviews = []\n",
    "short_labels = []\n",
    "max_review_chars = 1000\n",
    "for i, review in enumerate(reviews_contents):\n",
    "    if len(review.split()) > max_review_chars:\n",
    "        continue\n",
    "    short_reviews.append(review)\n",
    "    short_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_words = 150\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(short_reviews)\n",
    "\n",
    "short_sequences = [x for x in tokenizer.texts_to_sequences(short_reviews) if len(x) <= max_review_words]\n",
    "word_sequences = np.array(pad_sequences(short_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_labels_2 = []\n",
    "for i, sequence in enumerate(tokenizer.texts_to_sequences(short_reviews)):\n",
    "    if len(sequence) <= max_review_words:\n",
    "        short_labels_2.append(short_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(word_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_existing_embeddings = False\n",
    "embeddings_file_name = \"conv-embeddings.json\"\n",
    "\n",
    "embedding_matrix = None\n",
    "embedding_length = 0\n",
    "if read_existing_embeddings:\n",
    "  embedding_length = 300\n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  with open(embeddings_file_name, 'r') as infile:\n",
    "      data = json.load(infile)\n",
    "      for i in range(len(data)):\n",
    "          embedding_matrix[i] = np.array(data[i], dtype=np.float32)\n",
    "else:\n",
    "  word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"../../data/GoogleNews-vectors-negative300.bin\",\n",
    "                                                                 binary=True)\n",
    "  embedding_length = word_vectors.vector_size\n",
    "    \n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  for word, index in corpus_words.items():\n",
    "    if word in word_vectors.vocab:\n",
    "      embedding_matrix[index] = np.array(word_vectors[word], dtype=np.float32)\n",
    "\n",
    "  with open(embeddings_file_name, 'w') as outfile:\n",
    "      json.dump(embedding_matrix.tolist(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hello_wv = embedding_matrix[tokenizer.texts_to_sequences([\"hello\"])[0][0]]\n",
    "assert hello_wv[0] > -0.055 and hello_wv[0] < -0.054\n",
    "assert hello_wv[1] > 0.017 and hello_wv[1] < 0.018\n",
    "assert hello_wv[2] > -0.006 and hello_wv[2] < -0.005"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
