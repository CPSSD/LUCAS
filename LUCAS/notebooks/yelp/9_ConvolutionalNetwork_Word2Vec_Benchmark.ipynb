{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network Benchmark\n",
    "\n",
    "To obtain a benchmark for a basic convolutional network, we will create a simple network that informs us what to expect when using these networks. This will not contain any novel specializations, it is done to find a baseline which we can improve upon.\n",
    "\n",
    "The architecture used here is inspired by the research in the form of the paper ['A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification'](https://arxiv.org/pdf/1510.03820.pdf), and it not complex.\n",
    "\n",
    "While previous experiments have been done to find applicability of convolutional networks, this is the first to run it over a large set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from exp8_feature_extraction import get_balanced_dataset\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_contents = [x.review_content for x in all_reviews]\n",
    "labels = [1 if x.label else 0 for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_reviews = []\n",
    "short_labels = []\n",
    "max_review_chars = 1000\n",
    "for i, review in enumerate(reviews_contents):\n",
    "    if len(review.split()) > max_review_chars:\n",
    "        continue\n",
    "    short_reviews.append(review)\n",
    "    short_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_words = 150\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(short_reviews)\n",
    "\n",
    "short_sequences = [x for x in tokenizer.texts_to_sequences(short_reviews) if len(x) <= max_review_words]\n",
    "word_sequences = np.array(pad_sequences(short_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_labels_2 = []\n",
    "for i, sequence in enumerate(tokenizer.texts_to_sequences(short_reviews)):\n",
    "    if len(sequence) <= max_review_words:\n",
    "        short_labels_2.append(short_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(word_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_existing_embeddings = False\n",
    "embeddings_file_name = \"conv-embeddings.json\"\n",
    "\n",
    "embedding_matrix = None\n",
    "embedding_length = 0\n",
    "if read_existing_embeddings:\n",
    "  embedding_length = 300\n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  with open(embeddings_file_name, 'r') as infile:\n",
    "      data = json.load(infile)\n",
    "      for i in range(len(data)):\n",
    "          embedding_matrix[i] = np.array(data[i], dtype=np.float32)\n",
    "else:\n",
    "  word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"../../data/GoogleNews-vectors-negative300.bin\",\n",
    "                                                                 binary=True)\n",
    "  embedding_length = word_vectors.vector_size\n",
    "    \n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  for word, index in corpus_words.items():\n",
    "    if word in word_vectors.vocab:\n",
    "      embedding_matrix[index] = np.array(word_vectors[word], dtype=np.float32)\n",
    "\n",
    "  with open(embeddings_file_name, 'w') as outfile:\n",
    "      json.dump(embedding_matrix.tolist(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hello_wv = embedding_matrix[tokenizer.texts_to_sequences([\"hello\"])[0][0]]\n",
    "assert hello_wv[0] > -0.055 and hello_wv[0] < -0.054\n",
    "assert hello_wv[1] > 0.017 and hello_wv[1] < 0.018\n",
    "assert hello_wv[2] > -0.006 and hello_wv[2] < -0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vectorized_reviews(word_sequences, max_review_len, embedding_matrix):\n",
    "    vectorized_reviews = np.zeros((len(word_sequences), max_review_len, embedding_matrix.shape[1], 1))\n",
    "    for i, word_sequence in enumerate(word_sequences):\n",
    "        for j, word in enumerate(word_sequence):\n",
    "            for k, val in enumerate(embedding_matrix[word]):\n",
    "                vectorized_reviews[i][j][k][0] = val\n",
    "    return vectorized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_matrix = np.array([[1, 2, 3], [3, 2, 1]])\n",
    "\n",
    "actual_vectorized_reviews = to_vectorized_reviews([[0, 1]], 2, test_embedding_matrix)\n",
    "assert np.array_equal(actual_vectorized_reviews, np.array([[[[1], [2], [3]], [[3], [2], [1]]]]))\n",
    "\n",
    "actual_vectorized_reviews = to_vectorized_reviews([[1, 0]], 2, test_embedding_matrix)\n",
    "assert np.array_equal(actual_vectorized_reviews, np.array([[[[3], [2], [1]], [[1], [2], [3]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = to_vectorized_reviews(word_sequences, len(word_sequences[0]), embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will separate some data, holding it until after I have finished tweaking the model. This is because tweaking the model towards the data I'm testing against is likely to cause a misrepresentation of how well the model performs. Using unseen data is better representative of future unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_vectors = vectorized_reviews[:-10000]\n",
    "training_labels = short_labels_2[:-10000]\n",
    "held_vectors = vectorized_reviews[-10000:]\n",
    "held_labels = short_labels_2[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"vectorized_reviews_abc\", vectorized_reviews)\n",
    "#with open(\"vectorized_reviews_abc\", 'w') as outfile:\n",
    "#    np.save(outfile, vectorized_reviews)\n",
    "\n",
    "import pickle\n",
    "with open(\"vectorized_reviews.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(vectorized_reviews, outfile, protocol=4)\n",
    "with open(\"vectorized_reviews_targets.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(short_labels_2, outfile, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vectorized_reviews.pkl\", 'rb') as infile:\n",
    "  loaded_vectorized_reviews = pickle.load(infile)\n",
    "with open(\"vectorized_reviews_targets.pkl\", 'rb') as infile:\n",
    "  loaded_short_labels_2 = pickle.load(infile)"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
