{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network Benchmark\n",
    "\n",
    "To obtain a benchmark for a basic convolutional network, we will create a simple network that informs us what to expect when using these networks. This will not contain any novel specializations, it is done to find a baseline which we can improve upon.\n",
    "\n",
    "The architecture used here is inspired by the research in the form of the paper ['A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification'](https://arxiv.org/pdf/1510.03820.pdf), and is not complex.\n",
    "\n",
    "While previous experiments have been done to find applicability of convolutional networks, this is the first to run it over a large set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from exp8_feature_extraction import get_balanced_dataset\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Concatenate, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "from seaborn import boxplot\n",
    "from pandas import DataFrame\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROSS_FOLD_NUM = 0 # Change this to whichever fold you wish to run\n",
    "max_review_words = 218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_record(record):\n",
    "    example = tf.parse_single_example(record, {\n",
    "        'feature': tf.FixedLenFeature([300*max_review_words], tf.float32),\n",
    "        'reviewer': tf.FixedLenFeature([5], tf.float32),\n",
    "        'label': tf.FixedLenFeature([2], tf.int64),\n",
    "    })\n",
    "    example['feature'] = tf.reshape(example['feature'], [max_review_words, 300, 1])\n",
    "    return (example['feature'], example['reviewer']), example['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['TEST-225-' + str(i) + '.tfrecord' for i in range(2)]\n",
    "train_filenames = [x for i, x in enumerate(filenames) if i != CROSS_FOLD_NUM]\n",
    "\n",
    "epochs = 12\n",
    "steps = 293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_filenames).map(_decode_record).repeat(epochs).batch(46)\n",
    "validation_dataset = tf.data.TFRecordDataset(filenames[CROSS_FOLD_NUM]).map(_decode_record).repeat(epochs).batch(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_wv_model():\n",
    "  i1 = Input(shape=(max_review_words, 300, 1))\n",
    "  i2 = Input(shape=(5,))\n",
    "\n",
    "  l1 = Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 300),\n",
    "          data_format=\"channels_last\",\n",
    "          activation=relu)(i1)\n",
    "  l2 = tf.keras.layers.GlobalMaxPooling2D(data_format=\"channels_last\")(l1)\n",
    "  l3 = Dropout(0.5)(l2)\n",
    "  l4 = Flatten()(l3)\n",
    "  l5 = Concatenate(axis=1)([l4, i2])\n",
    "  l6 = Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(l5)\n",
    "  l8 = Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(l6)\n",
    "  l9 = Dense(2, activation='softmax')(l8)\n",
    "  model = Model(inputs=[i1, i2], outputs=l9)\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_model = get_conv_wv_model()\n",
    "history = conv_model.fit(train_dataset, steps_per_epoch=9*steps, epochs=epochs, validation_data=validation_dataset, validation_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_wv_scores = {\n",
    "    \"loss\": [0.5714481531963413, 0.5705330361685248, 0.5726435979885452, 0.5717445115587411,\n",
    "                0.5776157323207465, 0.5763675441310674, 0.5784062919152881, 0.5681992454943804,\n",
    "                0.5710791625057065, 0.5722369383829853],\n",
    "    \"accuracies\": [0.7098976151935069, 0.711233122967209, 0.7064104499263568, 0.7056685010727762,\n",
    "               0.705742698073794, 0.7009200229986536, 0.706039476109853, 0.7114557111222589,\n",
    "               0.7096008357334462, 0.709452444782843],\n",
    "    \"f1\": [0.7098975576231504, 0.7112330631591354, 0.7064103907285697, 0.7056684424852755,\n",
    "              0.7057426380622915, 0.7009199638008664, 0.7060394177257812, 0.7114556513141853,\n",
    "              0.7096007765356591, 0.7094523859919135]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Loss:\")\n",
    "val_loss = history.history['val_loss']\n",
    "print(val_loss)\n",
    "print(\"Validation Accuracy:\")\n",
    "val_accuracy = history.history['val_acc']\n",
    "print(val_accuracy)\n",
    "print(\"Validation F1:\")\n",
    "val_f1 = history.history['val_f1']\n",
    "print(val_f1)\n",
    "\n",
    "min_val_loss = 1\n",
    "best = None\n",
    "for loss, accuracy, f1 in zip(val_loss, val_accuracy, val_f1):\n",
    "    if loss < min_val_loss:\n",
    "        min_val_loss = loss\n",
    "        best = { 'val_loss': loss, 'val_accuracy': accuracy, 'val_f1': f1 }\n",
    "print(\"best:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.5714481531963413, 0.5705330361685248, 0.5726435979885452, 0.5717445115587411, 0.5776157323207465, 0.5763675441310674, 0.5784062919152881, 0.5681992454943804, 0.5710791625057065, 0.5722369383829853], 'accuracies': [0.7098976151935069, 0.711233122967209, 0.7064104499263568, 0.7056685010727762, 0.705742698073794, 0.7009200229986536, 0.706039476109853, 0.7114557111222589, 0.7096008357334462, 0.709452444782843], 'f1': [0.7098975576231504, 0.7112330631591354, 0.7064103907285697, 0.7056684424852755, 0.7057426380622915, 0.7009199638008664, 0.7060394177257812, 0.7114556513141853, 0.7096007765356591, 0.7094523859919135]}\n",
      "Average accuracy: 0.7076420877980698\n"
     ]
    }
   ],
   "source": [
    "print(conv_wv_scores)\n",
    "print(\"Average accuracy:\", sum(conv_wv_scores['accuracies'])/len(conv_wv_scores['accuracies']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks perform significantly better than FFNNs and the results are not widely varying. This appears to be a confident benchmark for a simple convolutional network.\n",
    "\n",
    "The distribution of results is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(DataFrame(conv_wv_scores['accuracies'])).set_title(\"Convolutional Network Accuracies\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the model on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81186 samples, validate on 34794 samples\n",
      "Epoch 1/12\n",
      "81186/81186 [==============================] - 137s 2ms/step - loss: 0.6485 - acc: 0.6256 - val_loss: 0.6260 - val_acc: 0.6526\n",
      "Epoch 2/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.6242 - acc: 0.6541 - val_loss: 0.6272 - val_acc: 0.6517\n",
      "Epoch 3/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.6081 - acc: 0.6727 - val_loss: 0.6189 - val_acc: 0.6581\n",
      "Epoch 4/12\n",
      "81186/81186 [==============================] - 135s 2ms/step - loss: 0.5940 - acc: 0.6828 - val_loss: 0.6121 - val_acc: 0.6670\n",
      "Epoch 5/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.5795 - acc: 0.6974 - val_loss: 0.6121 - val_acc: 0.6675\n",
      "Epoch 6/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.5644 - acc: 0.7110 - val_loss: 0.6200 - val_acc: 0.6597\n",
      "Epoch 7/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.5502 - acc: 0.7193 - val_loss: 0.6173 - val_acc: 0.6630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc80748d320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "conv_model = get_conv_wv_model()\n",
    "conv_model.fit(training_vectors, to_categorical(training_labels), epochs=12, batch_size=128, validation_split=0.3,\n",
    "               callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_labels = to_categorical(held_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 609us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6657"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.evaluate(held_vectors, categorical_labels)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs very similarly on the unseen data as the data used to create the model. There was not a lot of tweaking, so it is not unreasonable that this should occur. If this can be maintained then this model could do well with slight changes in the domain, something very easy to do with product and service reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
