{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative_Model_Training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "R_3H_i-aTsJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deceptive Review Generation: LSTM Generative Model"
      ]
    },
    {
      "metadata": {
        "id": "wHe11mViUxss",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is our first Google Colab hosted notebook, so the formatting may be a bit strange. We are moving over to Google Colab due to Google AI making TPU's free to use via Colab! This opens the door to a lot of exciting possibilities, as hardware is no longer a limitation.\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, I will attempt to create a generative language model that learns how to generate realistic reviews. Let's get started.\n",
        "\n",
        "---\n",
        "\n",
        "The first thing we must do is check that our TPU is connected and working, and authenticate it with Google."
      ]
    },
    {
      "metadata": {
        "id": "xWvvKlIYTpY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_Z1LicYWd6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's begin our experimentation. Due to the nature of this new format of experiment, we will have to do things differently.\n",
        "\n",
        "I ran the NYC data through our Protobuffer processor locally and saved the output to a .txt file. In future, we can come up with a better solution (like actually using Protobuffer objects instead of the messy string stuff below) but right now I just want to test Colab."
      ]
    },
    {
      "metadata": {
        "id": "I_tGgxVJXBrq",
        "colab_type": "code",
        "outputId": "e07a2810-6769-42b4-9aa1-7b9e7c821844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers\n",
        "from keras import layers\n",
        "import keras\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_dataset(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"normalizedNYCYelp.txt\", \n",
        "      origin=\"https://storage.googleapis.com/lucas0/imdb_classification/normalizedNYCYelp.txt\", \n",
        "      extract=False)\n",
        "  dfile = open(dataset).read()\n",
        "  reviews = dfile.split('\\n, ')\n",
        "  return reviews\n",
        "\n",
        "reviews = download_and_load_dataset()\n",
        "\n",
        "data = {}\n",
        "data['review'] = []\n",
        "data['deceptive'] = []\n",
        "\n",
        "for x in reviews:\n",
        "  data['review'].append(x.split('\\n')[0].split(': ')[1].replace('\"', '').strip())\n",
        "  data['deceptive'].append(0 if 'label: ' in x else 1)\n",
        "\n",
        "dataDict = pd.DataFrame.from_dict(data)\n",
        "print(len(dataDict))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "160933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y3Z5YmOvdD4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1666
        },
        "outputId": "0e106307-635c-4ead-d16b-42deeadb70fb"
      },
      "cell_type": "code",
      "source": [
        "reviews = dataDict['review']\n",
        "mask = (dataDict['review'].str.len() < 251) \n",
        "shortReviews = dataDict.loc[mask]\n",
        "print(len(shortReviews))\n",
        "short_reviews=shortReviews.sample(frac=1).reset_index(drop=True)\n",
        "open('short_reviews.txt', 'w+')\n",
        "filename='/short_reviews.txt'\n",
        "short_reviews.to_csv(filename, header=None, index=None, sep=' ')\n",
        "text = open('/short_reviews.txt').read()\n",
        "print(len(text))\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "maxlen = 60\n",
        "step = 1\n",
        "char_indices"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54689\n",
            "7622574\n",
            "Unique characters: 94\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '#': 4,\n",
              " '$': 5,\n",
              " '%': 6,\n",
              " '&': 7,\n",
              " \"'\": 8,\n",
              " '(': 9,\n",
              " ')': 10,\n",
              " '*': 11,\n",
              " '+': 12,\n",
              " ',': 13,\n",
              " '-': 14,\n",
              " '.': 15,\n",
              " '/': 16,\n",
              " '0': 17,\n",
              " '1': 18,\n",
              " '2': 19,\n",
              " '3': 20,\n",
              " '4': 21,\n",
              " '5': 22,\n",
              " '6': 23,\n",
              " '7': 24,\n",
              " '8': 25,\n",
              " '9': 26,\n",
              " ':': 27,\n",
              " ';': 28,\n",
              " '=': 29,\n",
              " '?': 30,\n",
              " '@': 31,\n",
              " 'A': 32,\n",
              " 'B': 33,\n",
              " 'C': 34,\n",
              " 'D': 35,\n",
              " 'E': 36,\n",
              " 'F': 37,\n",
              " 'G': 38,\n",
              " 'H': 39,\n",
              " 'I': 40,\n",
              " 'J': 41,\n",
              " 'K': 42,\n",
              " 'L': 43,\n",
              " 'M': 44,\n",
              " 'N': 45,\n",
              " 'O': 46,\n",
              " 'P': 47,\n",
              " 'Q': 48,\n",
              " 'R': 49,\n",
              " 'S': 50,\n",
              " 'T': 51,\n",
              " 'U': 52,\n",
              " 'V': 53,\n",
              " 'W': 54,\n",
              " 'X': 55,\n",
              " 'Y': 56,\n",
              " 'Z': 57,\n",
              " '[': 58,\n",
              " '\\\\': 59,\n",
              " ']': 60,\n",
              " '^': 61,\n",
              " '_': 62,\n",
              " '`': 63,\n",
              " 'a': 64,\n",
              " 'b': 65,\n",
              " 'c': 66,\n",
              " 'd': 67,\n",
              " 'e': 68,\n",
              " 'f': 69,\n",
              " 'g': 70,\n",
              " 'h': 71,\n",
              " 'i': 72,\n",
              " 'j': 73,\n",
              " 'k': 74,\n",
              " 'l': 75,\n",
              " 'm': 76,\n",
              " 'n': 77,\n",
              " 'o': 78,\n",
              " 'p': 79,\n",
              " 'q': 80,\n",
              " 'r': 81,\n",
              " 's': 82,\n",
              " 't': 83,\n",
              " 'u': 84,\n",
              " 'v': 85,\n",
              " 'w': 86,\n",
              " 'x': 87,\n",
              " 'y': 88,\n",
              " 'z': 89,\n",
              " '{': 90,\n",
              " '|': 91,\n",
              " '}': 92,\n",
              " '~': 93}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "Q6l2pPmyfoNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This get Data From Chunk is necessary to process large data sets like the one we have\n",
        "#If you're using a sample less than 1 million characters you can train the whole thing at once\n",
        "\n",
        "def getDataFromChunk(txtChunk, maxlen=60, step=1):\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(txtChunk) - maxlen, step):\n",
        "        sentences.append(txtChunk[i : i + maxlen])\n",
        "        next_chars.append(txtChunk[i + maxlen])\n",
        "    print('nb sequences:', len(sentences))\n",
        "    print('Vectorization...')\n",
        "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for t, char in enumerate(sentence):\n",
        "            X[i, t, char_indices[char]] = 1\n",
        "            y[i, char_indices[next_chars[i]]] = 1\n",
        "    return [X, y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4ZQ69u5frX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "7a30a7d9-41bb-4df5-9307-0396af163ad3"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(1024, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
        "model.add(layers.LSTM(1024, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 60, 1024)          4583424   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 1024)              8392704   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 94)                96350     \n",
            "=================================================================\n",
            "Total params: 13,072,478\n",
            "Trainable params: 13,072,478\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ythVYwolgInx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CJQLnZm7gMbY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this saves the weights everytime they improve so you can let it train.  Also learning rate decay\n",
        "\n",
        "filepath=\"Mar-4-all-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
        "              patience=1, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=6, restore_best_weights=True)\n",
        "callbacks_list = [checkpoint, reduce_lr, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S513-D3XgSB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    '''\n",
        "    Generate some randomness with the given preds\n",
        "    which is a list of numbers, if the temperature\n",
        "    is very small, it will always pick the index\n",
        "    with highest pred value\n",
        "    '''\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUHA9qsbgVCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4624
        },
        "outputId": "9705f167-dd86-42d5-edd0-dcb4376bc760"
      },
      "cell_type": "code",
      "source": [
        "for iteration in range(1, 5):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    with open(\"/short_reviews.txt\") as f:\n",
        "        for review in iter(lambda: f.read(90000), \"\"):\n",
        "            X, y = getDataFromChunk(review)\n",
        "            model.fit(X, y, batch_size=128, epochs=1, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 2.3108\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.31078, saving model to Mar-4-all-01-2.3108.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 501s 6ms/step - loss: 1.7214\n",
            "\n",
            "Epoch 00001: loss improved from 2.31078 to 1.72143, saving model to Mar-4-all-01-1.7214.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.5260\n",
            "\n",
            "Epoch 00001: loss improved from 1.72143 to 1.52597, saving model to Mar-4-all-01-1.5260.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.4241\n",
            "\n",
            "Epoch 00001: loss improved from 1.52597 to 1.42405, saving model to Mar-4-all-01-1.4241.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.3662\n",
            "\n",
            "Epoch 00001: loss improved from 1.42405 to 1.36622, saving model to Mar-4-all-01-1.3662.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.3183\n",
            "\n",
            "Epoch 00001: loss improved from 1.36622 to 1.31831, saving model to Mar-4-all-01-1.3183.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.3333\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.31831\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.2607\n",
            "\n",
            "Epoch 00001: loss improved from 1.31831 to 1.26069, saving model to Mar-4-all-01-1.2607.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.2431\n",
            "\n",
            "Epoch 00001: loss improved from 1.26069 to 1.24306, saving model to Mar-4-all-01-1.2431.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.2153\n",
            "\n",
            "Epoch 00001: loss improved from 1.24306 to 1.21526, saving model to Mar-4-all-01-1.2153.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.2185\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.21526\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.2003\n",
            "\n",
            "Epoch 00001: loss improved from 1.21526 to 1.20030, saving model to Mar-4-all-01-1.2003.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.2022\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.20030\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1645\n",
            "\n",
            "Epoch 00001: loss improved from 1.20030 to 1.16451, saving model to Mar-4-all-01-1.1645.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1740\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.16451\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1611\n",
            "\n",
            "Epoch 00001: loss improved from 1.16451 to 1.16110, saving model to Mar-4-all-01-1.1611.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1270\n",
            "\n",
            "Epoch 00001: loss improved from 1.16110 to 1.12695, saving model to Mar-4-all-01-1.1270.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1683\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 498s 6ms/step - loss: 1.1422\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 498s 6ms/step - loss: 1.1452\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 498s 6ms/step - loss: 1.1312\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1323\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1458\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.1334\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.12695\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 1.1144\n",
            "\n",
            "Epoch 00001: loss improved from 1.12695 to 1.11436, saving model to Mar-4-all-01-1.1144.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.1096\n",
            "\n",
            "Epoch 00001: loss improved from 1.11436 to 1.10960, saving model to Mar-4-all-01-1.1096.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.0959\n",
            "\n",
            "Epoch 00001: loss improved from 1.10960 to 1.09593, saving model to Mar-4-all-01-1.0959.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.1062\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.09593\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 1.1167\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.09593\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 498s 6ms/step - loss: 1.0896\n",
            "\n",
            "Epoch 00001: loss improved from 1.09593 to 1.08964, saving model to Mar-4-all-01-1.0896.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 496s 6ms/step - loss: 1.0975\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08964\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 496s 6ms/step - loss: 1.0843\n",
            "\n",
            "Epoch 00001: loss improved from 1.08964 to 1.08432, saving model to Mar-4-all-01-1.0843.hdf5\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 496s 6ms/step - loss: 1.4088\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 498s 6ms/step - loss: 1.4252\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 2.7351\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 5.6603\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 8.3760\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 9.2854\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 500s 6ms/step - loss: 9.1890\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 497s 6ms/step - loss: 8.5603\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 581s 6ms/step - loss: 8.7311\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 600s 7ms/step - loss: 9.7080\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 499s 6ms/step - loss: 9.8588\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "89940/89940 [==============================] - 496s 6ms/step - loss: 9.9813\n",
            "\n",
            "Epoch 00001: loss did not improve from 1.08432\n",
            "nb sequences: 89940\n",
            "Vectorization...\n",
            "Epoch 1/1\n",
            "78080/89940 [=========================>....] - ETA: 1:05 - loss: 9.8297"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}