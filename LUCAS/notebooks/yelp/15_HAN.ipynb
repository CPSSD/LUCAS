{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Pgnn9n2HL4U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#HAN: Hierachical Attention Network for review classification"
      ]
    },
    {
      "metadata": {
        "id": "t5fndKwJpTkW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this paper I will be implementing a Hierachical Attention Network, a new architecture that has shown promising results in sequence classification tasks. \n",
        "\n",
        "It uses stacked BiLSTM networks on words followed by an attention model to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Then the same procedure applied to the derived sentence vectors which then generate a vector who conceives the meaning of the given document and that vector can be passed further for text classification.\n",
        "\n",
        "The illustration of this structure is below. ![](https://cdn-images-1.medium.com/max/1600/1*28XVtq2lOjOmZhcSgu1NmQ.png)\n",
        "\n",
        "The idea behind it is that words make sentences and sentences make reviews. The attention layer decides which words are important for sentences and which sentences are important for reviews. "
      ]
    },
    {
      "metadata": {
        "id": "yYR0sRUML7v0",
        "colab_type": "code",
        "outputId": "affc3a3c-ed2a-46fc-9dfe-4cdcf8524918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras==2.0.3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.0.3 in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==2.0.3) (1.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==2.0.3) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.3) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.3) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.3) (1.1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h1D2y32eMijN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_SENT_LENGTH = 100\n",
        "MAX_SENTS = 15\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4MXXLLuZNEtR",
        "colab_type": "code",
        "outputId": "bd657900-8def-4c79-957b-b4ef8b0d131e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def download_and_load_dataset(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"yelpZIP.txt\", \n",
        "      origin=\"https://storage.googleapis.com/lucas0/yelpZIP.txt\", \n",
        "      extract=False)\n",
        "  dfile = open(dataset).readlines()\n",
        "  return dfile\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "reviews = download_and_load_dataset()\n",
        "\n",
        "data = {}\n",
        "data['review'] = []\n",
        "data['deceptive'] = []\n",
        "\n",
        "for x in reviews:\n",
        "  x = eval(x)\n",
        "  data['review'].append(x[0])\n",
        "  data['deceptive'].append(1 if x[1] else 0)\n",
        "\n",
        "dataDict = pd.DataFrame.from_dict(data)\n",
        "print(dataDict.shape)\n",
        "\n",
        "reviews = []\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(dataDict.shape[0]):\n",
        "  text = dataDict.review[idx]\n",
        "  text = clean_str(text)\n",
        "  reviews.append(text)\n",
        "  sentence = tokenize.sent_tokenize(text)\n",
        "  sentences.append(sentence)\n",
        "\n",
        "  labels.append(dataDict.deceptive[idx])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(reviews)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(160933, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l7vbu2KJNMRM",
        "colab_type": "code",
        "outputId": "8da5e804-3fac-410c-c595-7c1b212f8aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "data = np.zeros((len(reviews), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for j, sent in enumerate(sentence):\n",
        "        if j < MAX_SENTS:\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            k = 0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
        "                    data[i, j, k] = tokenizer.word_index[word]\n",
        "                    k = k + 1\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Total %s unique tokens.' % len(word_index))\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n",
        "\n",
        "print('Number of genuine and deceptive reviews in traing and validation set')\n",
        "print(y_train.sum(axis=0))\n",
        "print(y_val.sum(axis=0))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 90929 unique tokens.\n",
            "Shape of data tensor: (160933, 15, 100)\n",
            "Shape of label tensor: (160933,)\n",
            "Number of genuine and deceptive reviews in traing and validation set\n",
            "64273\n",
            "16193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8LwjLRj6Se6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "tf.keras.utils.get_file(\n",
        "      fname=\"GoogleNews-vectors-negative300.bin\", \n",
        "      origin=\"https://storage.googleapis.com/lucas0/GoogleNews-vectors-negative300.bin\", \n",
        "      extract=False)\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"~/.keras/datasets/GoogleNews-vectors-negative300.bin\", binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4L3XHNwS5EQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_length = word_vectors.vector_size\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, index in word_index.items():\n",
        "  if word in word_vectors.vocab:\n",
        "    embedding_matrix[index] = np.array(word_vectors[word], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BPqZZbnXSwGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=False,\n",
        "                            mask_zero=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhLp5egJUI7E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttLayer(Layer):\n",
        "    def __init__(self, attention_dim):\n",
        "        self.init = initializers.get('normal')\n",
        "        self.supports_masking = True\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
        "        self.b = K.variable(self.init((self.attention_dim, )))\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
        "        self.trainable_weights = [self.W, self.b, self.u]\n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # size of x :[batch_size, sel_len, attention_dim]\n",
        "        # size of u :[batch_size, attention_dim]\n",
        "        # uit = tanh(xW+b)\n",
        "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
        "        ait = K.dot(uit, self.u)\n",
        "        ait = K.squeeze(ait, -1)\n",
        "\n",
        "        ait = K.exp(ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ekC8MHbcUN-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b8d98811-82bb-4095-be5e-0ee2ef935c81"
      },
      "cell_type": "code",
      "source": [
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
        "l_att = AttLayer(100)(l_lstm)\n",
        "sentEncoder = Model(sentence_input, l_att)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1046: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HIhBWU00UXxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
        "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
        "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
        "preds = Dense(1, activation='sigmoid')(l_att_sent)\n",
        "model = Model(review_input, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jU1kDZqsasWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "6e404798-e24c-4cf1-a4b9-e6a84fe40256"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 15, 100)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 15, 200)           27539800  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 15, 200)           180600    \n",
            "_________________________________________________________________\n",
            "att_layer_3 (AttLayer)       (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 27,740,801\n",
            "Trainable params: 461,801\n",
            "Non-trainable params: 27,279,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Ta5iJE4bA-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "903b12a3-8645-47c4-c53d-fa3e45758304"
      },
      "cell_type": "code",
      "source": [
        "print(\"model fitting - Hierachical attention network\")\n",
        "model.fit(x_train, np.array(y_train), validation_data=(x_val, np.array(y_val)),\n",
        "          epochs=20, batch_size=64)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model fitting - Hierachical attention network\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 128747 samples, validate on 32186 samples\n",
            "Epoch 1/20\n",
            "128747/128747 [==============================] - 1141s - loss: 0.6225 - acc: 0.6544 - val_loss: 0.6099 - val_acc: 0.6634\n",
            "Epoch 2/20\n",
            "128747/128747 [==============================] - 1140s - loss: 0.6039 - acc: 0.6719 - val_loss: 0.6039 - val_acc: 0.6711\n",
            "Epoch 3/20\n",
            "128747/128747 [==============================] - 1138s - loss: 0.5910 - acc: 0.6829 - val_loss: 0.5959 - val_acc: 0.6786\n",
            "Epoch 4/20\n",
            "128747/128747 [==============================] - 1135s - loss: 0.5783 - acc: 0.6947 - val_loss: 0.5979 - val_acc: 0.6759\n",
            "Epoch 5/20\n",
            "128747/128747 [==============================] - 1137s - loss: 0.5610 - acc: 0.7077 - val_loss: 0.5986 - val_acc: 0.6755\n",
            "Epoch 6/20\n",
            "128747/128747 [==============================] - 1138s - loss: 0.5370 - acc: 0.7256 - val_loss: 0.6102 - val_acc: 0.6690\n",
            "Epoch 7/20\n",
            "128747/128747 [==============================] - 1136s - loss: 0.5025 - acc: 0.7506 - val_loss: 0.6399 - val_acc: 0.6641\n",
            "Epoch 8/20\n",
            "128747/128747 [==============================] - 1135s - loss: 0.4570 - acc: 0.7794 - val_loss: 0.7077 - val_acc: 0.6596\n",
            "Epoch 9/20\n",
            "128747/128747 [==============================] - 1135s - loss: 0.4055 - acc: 0.8090 - val_loss: 0.7722 - val_acc: 0.6467\n",
            "Epoch 10/20\n",
            "128747/128747 [==============================] - 1133s - loss: 0.3499 - acc: 0.8402 - val_loss: 0.8848 - val_acc: 0.6427\n",
            "Epoch 11/20\n",
            "128747/128747 [==============================] - 1130s - loss: 0.2974 - acc: 0.8666 - val_loss: 0.9997 - val_acc: 0.6386\n",
            "Epoch 12/20\n",
            "128747/128747 [==============================] - 1134s - loss: 0.2538 - acc: 0.8877 - val_loss: 1.1460 - val_acc: 0.6294\n",
            "Epoch 13/20\n",
            "128747/128747 [==============================] - 1142s - loss: 0.2181 - acc: 0.9063 - val_loss: 1.2658 - val_acc: 0.6318\n",
            "Epoch 14/20\n",
            "128747/128747 [==============================] - 1141s - loss: 0.1872 - acc: 0.9201 - val_loss: 1.3935 - val_acc: 0.6300\n",
            "Epoch 15/20\n",
            "128747/128747 [==============================] - 1140s - loss: 0.1660 - acc: 0.9310 - val_loss: 1.4519 - val_acc: 0.6202\n",
            "Epoch 16/20\n",
            "128747/128747 [==============================] - 1141s - loss: 0.1484 - acc: 0.9393 - val_loss: 1.5956 - val_acc: 0.6247\n",
            "Epoch 17/20\n",
            "128747/128747 [==============================] - 1139s - loss: 0.1329 - acc: 0.9464 - val_loss: 1.6566 - val_acc: 0.6259\n",
            "Epoch 18/20\n",
            "128747/128747 [==============================] - 1144s - loss: 0.1181 - acc: 0.9529 - val_loss: 1.6848 - val_acc: 0.6215\n",
            "Epoch 19/20\n",
            "128747/128747 [==============================] - 1141s - loss: 0.1090 - acc: 0.9576 - val_loss: 1.7939 - val_acc: 0.6228\n",
            "Epoch 20/20\n",
            "128747/128747 [==============================] - 1145s - loss: 0.1008 - acc: 0.9602 - val_loss: 1.8381 - val_acc: 0.6221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3694ca2cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}