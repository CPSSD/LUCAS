{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network Benchmark\n",
    "\n",
    "To obtain a benchmark for a basic convolutional network, we will create a simple network that informs us what to expect when using these networks. This will not contain any novel specializations, it is done to find a baseline which we can improve upon.\n",
    "\n",
    "The architecture used here is inspired by the research in the form of the paper ['A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification'](https://arxiv.org/pdf/1510.03820.pdf), and is not complex.\n",
    "\n",
    "While previous experiments have been done to find applicability of convolutional networks, this is the first to run it over a large set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from exp8_feature_extraction import get_balanced_dataset\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "import pickle\n",
    "from seaborn import boxplot\n",
    "from pandas import DataFrame\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_record(record):\n",
    "    example = tf.parse_single_example(record, {\n",
    "        'feature': tf.FixedLenFeature([300*150], tf.float32),\n",
    "        'label': tf.FixedLenFeature([2], tf.int64),\n",
    "    })\n",
    "    example['feature'] = tf.reshape(example['feature'], [150, 300, 1])\n",
    "    return example['feature'], example['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding record  Tensor(\"ParseSingleExample/ParseSingleExample:1\", shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ds = (tf.data.TFRecordDataset('tmp.tfrecord').repeat(100).shuffle(buffer_size=100).map(_decode_record).make_one_shot_iterator().get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding record  Tensor(\"ParseSingleExample/ParseSingleExample:1\", shape=(2,), dtype=int64)\n",
      "52 50\n"
     ]
    }
   ],
   "source": [
    "sess = tf.keras.backend.get_session()\n",
    "iterator = tf.data.TFRecordDataset('tmp.tfrecord').map(_decode_record).make_one_shot_iterator()\n",
    "review_vecs = []\n",
    "count = 16000\n",
    "counta = 0\n",
    "countb = 0\n",
    "try:\n",
    "    nxt = iterator.get_next()\n",
    "    while True:\n",
    "        #review_vecs.append(sess.run(nxt['feature']))\n",
    "        val = sess.run(nxt[1])\n",
    "        if val[1] == 1:\n",
    "            countb += 1\n",
    "        elif val[0] == 1:\n",
    "            counta += 1\n",
    "        count -= 1\n",
    "        if count <= 0:\n",
    "            break\n",
    "except tf.errors.OutOfRangeError:\n",
    "    pass\n",
    "print(counta, countb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(training_vectors[:1000], review_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding record  Tensor(\"ParseSingleExample/ParseSingleExample:1\", shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "read_dataset = tf.data.TFRecordDataset('tmp.tfrecord').map(_decode_record).shuffle(buffer_size=100).repeat(100).batch(6299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = read_dataset.make_one_shot_iterator()\n",
    "inputs = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.data.ops.iterator_ops.Iterator at 0x7fbb4837e128>,\n",
       " (<tf.Tensor 'IteratorGetNext_1795:0' shape=(?, 150, 300, 1) dtype=float32>,\n",
       "  <tf.Tensor 'IteratorGetNext_1795:1' shape=(?, 2) dtype=int32>))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 170s 8s/step - loss: 0.6664 - acc: 0.5955\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 166s 8s/step - loss: 0.6396 - acc: 0.6377\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 165s 8s/step - loss: 0.6268 - acc: 0.6542\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 165s 8s/step - loss: 0.6188 - acc: 0.6626\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 166s 8s/step - loss: 0.6112 - acc: 0.6709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb08554c50>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = get_conv_wv_model()\n",
    "conv_model.fit(read_dataset, steps_per_epoch=20, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_word_feature(embedded_word):\n",
    "    feature_list = tf.train.Feature(float_list=tf.train.FloatList(value=embedded_word))\n",
    "    features = { \"features\": feature_list}\n",
    "    return tf.train.Features(feature=feature_list).SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(*embedded_values):\n",
    "  print(embedded_values)\n",
    "  tf_string = tf.py_func(\n",
    "      embedded_word_feature,\n",
    "      (embedded_values),\n",
    "      tf.string)\n",
    "  return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 300),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(max_review_words, 300, 1),\n",
    "          activation=relu),\n",
    "      tf.keras.layers.GlobalMaxPooling2D(data_format=\"channels_last\"),\n",
    "      Dropout(0.5),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125980"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "categorical_labels = to_categorical(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_placeholder = tf.placeholder(training_vectors.dtype, training_vectors.shape)\n",
    "labels_placeholder = tf.placeholder(categorical_labels.dtype, categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((vectors_placeholder, labels_placeholder)).repeat(5).batch(6299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.keras.backend.get_session()\n",
    "sess.run(iterator.initializer,\n",
    "         feed_dict={vectors_placeholder: training_vectors, labels_placeholder: categorical_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 169s 8s/step - loss: 0.6660 - acc: 0.5961\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 169s 8s/step - loss: 0.6401 - acc: 0.6382\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 169s 8s/step - loss: 0.6279 - acc: 0.6533\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 169s 8s/step - loss: 0.6192 - acc: 0.6626\n",
      "Epoch 5/5\n",
      "11/20 [===============>..............] - ETA: 1:15 - loss: 0.6142 - acc: 0.6675WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need touse the repeat() function when building your dataset.Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need touse the repeat() function when building your dataset.Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need touse the repeat() function when building your dataset.Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need touse the repeat() function when building your dataset.Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need touse the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd35b0d12b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = get_conv_wv_model()\n",
    "conv_model.fit(iterator, steps_per_epoch=20, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, training_vectors, training_labels, cv=6, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracies': [0.66613211939372, 0.6612177331746935, 0.6639077130029282, 0.6568368772311035, 0.654612240674634, 0.6592684567230587]}\n",
      "Average accuracy: 0.6603291900333562\n"
     ]
    }
   ],
   "source": [
    "print(conv_wv_scores)\n",
    "print(\"Average accuracy:\", sum(conv_wv_scores['accuracies'])/len(conv_wv_scores['accuracies']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks perform significantly better than FFNNs and the results are not widely varying. This appears to be a confident benchmark for a simple convolutional network.\n",
    "\n",
    "The distribution of results is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(DataFrame(conv_wv_scores)).set_title(\"Convolutional Network Accuracies\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the model on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81186 samples, validate on 34794 samples\n",
      "Epoch 1/12\n",
      "81186/81186 [==============================] - 137s 2ms/step - loss: 0.6485 - acc: 0.6256 - val_loss: 0.6260 - val_acc: 0.6526\n",
      "Epoch 2/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.6242 - acc: 0.6541 - val_loss: 0.6272 - val_acc: 0.6517\n",
      "Epoch 3/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.6081 - acc: 0.6727 - val_loss: 0.6189 - val_acc: 0.6581\n",
      "Epoch 4/12\n",
      "81186/81186 [==============================] - 135s 2ms/step - loss: 0.5940 - acc: 0.6828 - val_loss: 0.6121 - val_acc: 0.6670\n",
      "Epoch 5/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.5795 - acc: 0.6974 - val_loss: 0.6121 - val_acc: 0.6675\n",
      "Epoch 6/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.5644 - acc: 0.7110 - val_loss: 0.6200 - val_acc: 0.6597\n",
      "Epoch 7/12\n",
      "81186/81186 [==============================] - 136s 2ms/step - loss: 0.5502 - acc: 0.7193 - val_loss: 0.6173 - val_acc: 0.6630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc80748d320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "conv_model = get_conv_wv_model()\n",
    "conv_model.fit(training_vectors, to_categorical(training_labels), epochs=12, batch_size=128, validation_split=0.3,\n",
    "               callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_labels = to_categorical(held_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 609us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6657"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.evaluate(held_vectors, categorical_labels)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_metric = tf.metrics.auc(categorical_labels, conv_model.predict(held_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf auc: [9999999000.0, 0.72368735]\n"
     ]
    }
   ],
   "source": [
    "auc, update_op = auc_metric\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    print(\"tf auc: {}\".format(sess.run([auc, update_op])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs very similarly on the unseen data as the data used to create the model. There was not a lot of tweaking, so it is not unreasonable that this should occur. If this can be maintained then this model could do well with slight changes in the domain, something very easy to do with product and service reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
