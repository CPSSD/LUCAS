{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous experiment attempted to achieve high accuracy by using better features. Using the features that we found to work best we will increase the size of our dataset to investigate the impact. The previous experiment attempted to replicate the results of a paper, however this paper did not seem to use all of the data available to it. Here we will use all of the available data, while maintaining the balance of our two classes. The data used here will be about 10x bigger than in the last experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608598\n"
     ]
    }
   ],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpZip\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())\n",
    "print(len(review_set.reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our dataset into fake and genuine, but also store the reviews that we didn't use. We use all of the fake reviews in the dataset, so the only unused reviews are genuine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake: 80466\n",
      "real: 80467\n",
      "all: 160933\n",
      "unused real: 447665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "fake_reviews = list(filter(lambda x: x.label, review_set.reviews))\n",
    "count_fake = len(fake_reviews)\n",
    "genuine_reviews = []\n",
    "unused_genuine_reviews = []\n",
    "counter_genuine = 0\n",
    "for review in shuffle(review_set.reviews):\n",
    "  if review.label == True:\n",
    "    continue\n",
    "  if counter_genuine <= count_fake:\n",
    "    genuine_reviews.append(review)\n",
    "    counter_genuine += 1\n",
    "  else:\n",
    "    unused_genuine_reviews.append(review)\n",
    "  \n",
    "concatted_reviews = fake_reviews + genuine_reviews\n",
    "print(\"fake:\", len(fake_reviews))\n",
    "print(\"real:\", len(genuine_reviews))\n",
    "print(\"all:\", len(concatted_reviews))\n",
    "print(\"unused real:\", len(unused_genuine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.feature_extraction import get_balanced_dataset\n",
    "concatted_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in concatted_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp4_data_feature_extraction import get_features_maker\n",
    "get_features = get_features_maker(concatted_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'acc': 'accuracy',\n",
    "    'auroc': 'roc_auc',\n",
    "    'f1': 'f1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "predictor_features = get_features(concatted_reviews)\n",
    "naive_bayes = MultinomialNB()\n",
    "fold = 10\n",
    "results = cross_validate(naive_bayes, predictor_features, targets, cv=fold, scoring=scoring, return_train_score=False)\n",
    "#print(\"Average accuracy:\", sum([x for x in results['test_acc']])/fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.46036673, 0.45923424, 0.45915699, 0.45723844, 0.36908245,\n",
       "        0.37357187, 0.37405682, 0.37362266, 0.37323928, 0.37408853]),\n",
       " 'score_time': array([0.04839873, 0.04749274, 0.04627275, 0.04383707, 0.03976035,\n",
       "        0.04009771, 0.04002523, 0.03997731, 0.03998733, 0.04001665]),\n",
       " 'test_acc': array([0.68787492, 0.69463944, 0.69272495, 0.6854499 , 0.68672623,\n",
       "        0.6936822 , 0.69332397, 0.68387797, 0.69377074, 0.68713301]),\n",
       " 'test_auroc': array([0.75767943, 0.76391493, 0.76046234, 0.75584793, 0.75551675,\n",
       "        0.75623508, 0.75845146, 0.7520655 , 0.76099029, 0.7549497 ]),\n",
       " 'test_f1': array([0.70892103, 0.71566938, 0.71409061, 0.7076339 , 0.70528907,\n",
       "        0.71139971, 0.7143112 , 0.70624518, 0.71358644, 0.70583293])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is slightly better with the larger dataset, which shows the last experiment trained quite well on the smaller dataset, or at least that an increase of 10x does not improve the model dramatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
