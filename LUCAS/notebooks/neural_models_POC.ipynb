{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof-of-concept: Classification of review deception via neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to move on to a deep-learning approach to fake review opinion spam detection, shown in numerous papers and studies to be an effective method of classification. \n",
    "In this notebook, in a similar fashion to the other notebooks in this directory, we will use the small dataset with minimal features and set up some very basic neural network architectures, detailed in our Wiki, using Tensorflow and Keras.\n",
    "\n",
    "Let's start by importing the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start by getting our data. We use the get_data_frame function from the training helpers file, and split it into our X and Y (data and labels). \n",
    "\n",
    "It's been shown that a 70/30 train/test split is generally quite good, with a 70/30 train/validation split on the training data.\n",
    "\n",
    "Training data is used to adjust weights, validation is used to make sure that these weight changes result in an increase in accuracy. Test data is used to run the final set of weights over data it's never seen before for final validation and to produce an accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import training_helpers as th\n",
    "df = th.get_data_frame()\n",
    "\n",
    "X = df['review']\n",
    "y = df['deceptive']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the vocab size of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19309\n"
     ]
    }
   ],
   "source": [
    "def find_vocab_size(reviews):\n",
    "  words = set()\n",
    "  for review in reviews:\n",
    "    for word in review.split():\n",
    "      words.add(word)\n",
    "  return len(words)\n",
    "\n",
    "print(find_vocab_size(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tonne of features, so let's use the keras Tokenizer, which lets us choose a number of the most commonly occuring words in our vocab. We'll set this number to 2000, to get rid of the least important ones, and then fit it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 2000\n",
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that takes a list of reviews and converts it to a list of tfidf vectors based on the above dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return tokenizer.texts_to_matrix(data, mode='tfidf')\n",
    "\n",
    "train_data = tokenize(X_train)\n",
    "test_data = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at these tfidf vectors and the shape of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now that our data is prepared, lets create our neural network.\n",
    "\n",
    "Let's create a basic network with one hidden layer of 4 nodes, fully connected (Dense). \n",
    "\n",
    "The first layer needs to specify the size of the input in 'input_shape', which is the number of features in an input. Its output size will be the first parameter (4). In this case, we use a very small number of nodes as it's a very small dataset, so 4 is optimal to prevent overfitting.\n",
    "\n",
    "The following layers can infer the size.\n",
    "\n",
    "We use the Adam optimizer, which is a replacement optimizer for stochastic gradient descent, and the binary_crossentropy loss function, which is the binary log loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    " \n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train and validate the model. We fit it on the training data, using 70% for training (weight adjustment) and 30% for validation. Validation is important as it's data that hasn't been seen in the training, so we get a feel for how well the model generalizes and make sure overfitting does not occur. In general, how good a model is can be judged based on how low it's validation loss is.\n",
    "\n",
    "Batch size is how many chunks we want to split out data up into. We cant pass the entire dataset in one epoch, so we split it up into chunks of size n, where n is the batch size. The number of iterations it takes to complete one epoch is the training size / batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data,\n",
    "                      y_train,\n",
    "                      epochs=12,\n",
    "                      batch_size=16,\n",
    "                      validation_split=0.3,\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the validation loss consistently decreases as the epochs go on, and validation accuracy consistently increases. If the validation loss is decreasing up until the end, then increase the number of epochs or reduce the batch size.\n",
    "\n",
    "From the val_acc seen at the end of every epoch, we can predict the value of accuracy on the test data. Lets evaluate the model and give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, y_test)\n",
    "print(model.metrics_names, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, the accuracy is about as predicted! And we have a pretty low loss too. This indicates a good model that will generalize well.\n",
    "Let's plot the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, acc, 'ro', label='Training accuracy')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Model\n",
    "\n",
    "In this POC, we will still use BOW features, and look further into the use of word embeddings with a CNN in a seperate future experiment.\n",
    "\n",
    "One paper [Semi-supervised Convolutional Neural Networks for text categorization via region embedding](https://papers.nips.cc/paper/5849-semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding.pdf) builds a semi supervised convolutional network using bow by using unsupervised learning to produce two-view embeddings of the words. This however is a lot of work for a proof of concept, so we will not try to perform the same work.\n",
    "\n",
    "However this model is not expected to be very predictive on bag of words because the word order is not preserved. Word order is important for Convolutional Networks, using embeddings this would be preserved because each row would represent the next word in the sentence. By treating the bag of words as two dimensional input, with row number of one, we can drop in our embeddings with the same process as before, but with a greater number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our input data we have 1120 samples and a vocabulary of size 2000, which will be our number of columns. Each input or 'image' in the context of Convolutional networks will be an image with 1 row, 2000 columns and depth 1. We will have 1120 of these input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 1120\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in train_data]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on findings in a paper [A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1510.03820.pdf) we will make a number of decisions:\n",
    "\n",
    "* We will use the ReLU implementation as our activation function.\n",
    "* While working with only sliding one direction over the data, we will use a filter size of 10\n",
    "* We will try 50 filters for our POC\n",
    "* Our vocab size is magnitudes of order larger than our filter size. We'll pool the result of each convolution to 1 value.\n",
    "* We can start with 1 convolutional layer, like other research has, [Convolutional neural networks for sentence classification](https://www.aclweb.org/anthology/D14-1181)\n",
    "* We'll include a dropout layer to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.activations import relu\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(\n",
    "        filters=50,\n",
    "        kernel_size=(1, 10),\n",
    "        data_format=\"channels_last\",\n",
    "        input_shape=(1, 2000, 1),\n",
    "        activation=relu),\n",
    "    keras.layers.MaxPooling2D(pool_size=(1, 10)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert our targets to represent two classes, with arrays of length 2, where element 0 will be 1 for class 0, and element 1 will be 1 for class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional_targets = keras.utils.to_categorical(y_train)\n",
    "convolutional_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 12\n",
    "conv_history = model.fit(\n",
    "    convolutional_data,\n",
    "    convolutional_targets,\n",
    "    validation_split=0.2,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, num_epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, conv_history['loss'], 'bo', label='Training loss')\n",
    "plt.plot(epochs, conv_history['val_loss'], 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the both the training loss and validation loss start by decreasing, until the validation loss begins to rise again. This shows that after a short number of epochs we start to see overfitting, where the model is fit to well to our data. Adding dropout layers helps reduce this effect, but we are using such a small dataset that it is likely too easy to reproduce what the model is training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, conv_history['acc'], 'ro', label='Training accuracy')\n",
    "plt.plot(epochs, conv_history['val_acc'], 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training accuracy increases consistently, as the model fits more and more closely to the training set. The validation increases but quickly levels out, as the model is not becoming more predictive on unseen data.\n",
    "\n",
    "We should also note that with such a small dataset our results are highly variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Model\n",
    "\n",
    "We now wish to move on to recurrent neural networks. In this POC we will use our OpSpam word embeddings, as RNN networks rely heavily on the temporal nature of sentences and our Word2Vec model is a far better representation of this than a simple BOW model.\n",
    "\n",
    "The classic RNN in almost every case suffers from the 'vanishing gradient' problem, which is a byproduct of the chaining of activation functions. In a deep network, this becomes a far bigger problem, and RNN's are very deep networks. This results in RNN's not being able to remember words for a very long time, which is not what we want in a domain such as text classification. \n",
    "\n",
    "Therefore, in comes the LSTM (Long Short Term Memory). Basically, instead of each node being an activation function, each cell is a memory cell that can hold more info in the form of states. An LSTM has a forget state, an update state, and an output state. We will go into this in much more detail in the LSTM-specific experiment and in the wiki.\n",
    "\n",
    "A GRU (Gated Recurrent Unit) is a more computationally-efficient RNN but comes second in performance. It does well when we lack compute power necessary to do LSTM training. In this section I will demonstrate just the LSTM, as the results will be similarly bad on this small dataset.\n",
    "\n",
    "First, let's convert our reviews into a format usable by the embedding layer in Keras. We can bring in our pre-trained word2vec vectors to help us out too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load('../wordvec/opspam_w2v.kv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "embedding_dim = wv.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in wv.wv.vocab:\n",
    "        embedding_matrix[i] = np.array(wv.wv[word], dtype=np.float32)\n",
    "        \n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements/vocab_size)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good, as we have 94% coverage of the words in our training data by our embedding vectors. Now, we need to pad all of the sequences to be the length of the longest review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(x) for x in X_train_seq + X_test_seq])\n",
    "\n",
    "print(maxlen)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our reviews are now ready to be used by Keras. \n",
    "\n",
    "Let's define our LSTM model, using just 8 memory cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "lstm_model = keras.Sequential()\n",
    "lstm_model.add(keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "lstm_model.add(keras.layers.LSTM(8))\n",
    "lstm_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the LSTM network is simply a 50/50 guess right now. This is indicative that we need to adjust a couple of things, such as: The learning rate, the amount of input data, the embedding layer, among many other possibilities. We will dive into this in more detail in a seperated LSTM experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
