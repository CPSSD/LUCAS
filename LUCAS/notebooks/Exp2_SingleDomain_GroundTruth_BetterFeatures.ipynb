{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "data_file_path = 'data/yelpNYC'\n",
    "from protos import review_set_pb2, review_pb2\n",
    "import nltk\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(data_file_path, 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are length of the review, average word length, number of sentences, average sentence length, percentage of numerals, percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_avg_token_length, find_numerals_ratio\n",
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "\n",
    "find_words = lambda text: nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def structural_features(review):\n",
    "  review_content = review.review_content\n",
    "  length_of_review = len(review_content)\n",
    "  words = find_words(review_content)\n",
    "  avg_word_length = find_avg_token_length(words)\n",
    "  sentences = nltk.tokenize.sent_tokenize(review_content)\n",
    "  sentence_length_of_review = len(sentences)\n",
    "  avg_sentence_length = find_avg_token_length(sentences)\n",
    "  numerals_ratio = find_numerals_ratio(words)\n",
    "  capitalised_word_ratio = find_capitalised_word_ratio(words)\n",
    "  return (length_of_review, avg_word_length, sentence_length_of_review,\n",
    "          avg_sentence_length, numerals_ratio, capitalised_word_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = lambda text: nltk.pos_tag(words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import functools\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_features(review):\n",
    "  polarities = []\n",
    "  num_positive = 0\n",
    "  num_negative = 0\n",
    "  for word in find_words(review.review_content):\n",
    "    polarity = sentiment_analyzer.polarity_scores(word)['compound']\n",
    "    polarities.append(1 if polarity > 0 else -1 if polarity < 0 else 0)\n",
    "  reduce_func = lambda c, p: (c[0] + 1, c[1]) if p > 0 else (c[0], c[1] + 1) if p < 0 else (c[0], c[1])\n",
    "  num_positive, num_negative = functools.reduce(reduce_func, polarities, (0, 0))\n",
    "  total = num_positive + num_negative\n",
    "  if total == 0:\n",
    "    return (0, 0)\n",
    "  return (num_positive / total, num_negative / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes the maximum number of reviews in a day, *percentage of positive / negative reviews (MISSING)*, average review length, standard deviation of reviewer's ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "from sklearn.utils import shuffle\n",
    "reviews = shuffle(review_set.reviews)\n",
    "reviews_reviewer_map = reviews_by_reviewer(reviews)\n",
    "\n",
    "predictor_features = []\n",
    "i = 0\n",
    "for review in reviews:\n",
    "  next_entry = []\n",
    "  next_entry += list(structural_features(review))\n",
    "  next_entry += list(sentiment_features(review))\n",
    "  next_entry += list(reviewer_features(review, reviews_reviewer_map))\n",
    "  predictor_features.append(next_entry)\n",
    "  i+=1\n",
    "  if i % 1000 == 0:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1113, 3.9012875536480687, 15, 73.2, 0.0, 0.11587982832618025, 0.9166666666666666, 0.08333333333333333, 3, 802.0, 0.5773502691896257], [820, 3.4123711340206184, 8, 101.375, 0.015463917525773196, 0.12886597938144329, 0.6153846153846154, 0.38461538461538464, 1, 811.25, 0.5773502691896257], [232, 4.0638297872340425, 6, 37.666666666666664, 0.0, 0.1702127659574468, 0.7777777777777778, 0.2222222222222222, 1, 221.5, 0.5], [941, 4.085106382978723, 7, 132.28571428571428, 0.005319148936170213, 0.15425531914893617, 0.6923076923076923, 0.3076923076923077, 2, 878.25, 1.2817398889233114], [345, 4.0, 5, 68.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1, 563.5, 0.9574271077563381], [612, 4.235294117647059, 8, 75.5, 0.0, 0.09243697478991597, 0.9230769230769231, 0.07692307692307693, 1, 612.0, 0], [307, 4.080645161290323, 3, 101.0, 0.0, 0.04838709677419355, 0.6, 0.4, 1, 699.6666666666666, 0.5773502691896257], [556, 3.616, 6, 91.66666666666667, 0.0, 0.128, 0.8, 0.2, 3, 558.0, 0.6887372317211945], [1163, 4.067796610169491, 13, 88.46153846153847, 0.0, 0.11440677966101695, 0.7619047619047619, 0.23809523809523808, 2, 860.1935483870968, 0.8360171835451683], [448, 4.043956043956044, 7, 63.0, 0.0, 0.12087912087912088, 1.0, 0.0, 2, 1189.857142857143, 0.6900655593423543]]\n"
     ]
    }
   ],
   "source": [
    "print(predictor_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/stefan/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.021451  , 0.00132608, 0.0012238 , 0.00122094, 0.0011251 ,\n",
       "        0.00103354, 0.001127  , 0.00131989, 0.0012095 , 0.00104022]),\n",
       " 'score_time': array([0.00076175, 0.00043392, 0.00041699, 0.00041246, 0.00041604,\n",
       "        0.00036812, 0.00056982, 0.00045633, 0.0004456 , 0.00093961]),\n",
       " 'test_score': array([0.66666667, 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.5       , 1.        , 0.5       , 0.5       , 1.        ])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
