{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "data_file_path = 'data/yelpNYC'\n",
    "from protos import review_set_pb2, review_pb2\n",
    "import nltk\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(data_file_path, 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are length of the review, average word length, number of sentences, average sentence length, percentage of numerals, percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_avg_token_length, find_numerals_ratio\n",
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "\n",
    "find_words = lambda text: nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def structural_features(review):\n",
    "  review_content = review.review_content\n",
    "  length_of_review = len(review_content)\n",
    "  words = find_words(review_content)\n",
    "  avg_word_length = find_avg_token_length(words)\n",
    "  sentences = nltk.tokenize.sent_tokenize(review_content)\n",
    "  sentence_length_of_review = len(sentences)\n",
    "  avg_sentence_length = find_avg_token_length(sentences)\n",
    "  numerals_ratio = find_numerals_ratio(words)\n",
    "  capitalised_word_ratio = find_capitalised_word_ratio(words)\n",
    "  return (length_of_review, avg_word_length, sentence_length_of_review,\n",
    "          avg_sentence_length, numerals_ratio, capitalised_word_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = lambda text: nltk.pos_tag(words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import functools\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_features(review):\n",
    "  polarities = []\n",
    "  num_positive = 0\n",
    "  num_negative = 0\n",
    "  for word in find_words(review.review_content):\n",
    "    polarity = sentiment_analyzer.polarity_scores(word)['compound']\n",
    "    polarities.append(1 if polarity > 0 else -1 if polarity < 0 else 0)\n",
    "  reduce_func = lambda c, p: (c[0] + 1, c[1]) if p > 0 else (c[0], c[1] + 1) if p < 0 else (c[0], c[1])\n",
    "  num_positive, num_negative = functools.reduce(reduce_func, polarities, (0, 0))\n",
    "  total = num_positive + num_negative\n",
    "  if total == 0:\n",
    "    return (0, 0)\n",
    "  return (num_positive / total, num_negative / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the topic model features from LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "reviews_words = []\n",
    "for review in review_set.reviews:\n",
    "  reviews_words.append(find_words(review.review_content))\n",
    "dictionary = gensim.corpora.Dictionary(reviews_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in reviews_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=10, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_features(review):\n",
    "  words = find_words(review.review_content)\n",
    "  bow = dictionary.doc2bow(words)\n",
    "  topics = lda_model.get_document_topics(bow)\n",
    "  return [x[1] for x in topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes the maximum number of reviews in a day, *percentage of positive / negative reviews (MISSING)*, average review length, standard deviation of reviewer's ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "from sklearn.utils import shuffle\n",
    "reviews = shuffle(review_set.reviews)\n",
    "reviews_reviewer_map = reviews_by_reviewer(reviews)\n",
    "\n",
    "predictor_features = []\n",
    "i = 0\n",
    "for review in reviews:\n",
    "  next_entry = []\n",
    "  next_entry += list(structural_features(review))\n",
    "  next_entry += list(sentiment_features(review))\n",
    "  next_entry += topic_features(review)\n",
    "  next_entry += list(reviewer_features(review, reviews_reviewer_map))\n",
    "  predictor_features.append(next_entry)\n",
    "  i+=1\n",
    "  if i % 1000 == 0:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
