{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Neural Models on pretrained Word2Vec embeddings\n",
    "\n",
    "## Feedforward Neural Network\n",
    "\n",
    "Following an experiment to compare neural models, we discovered odd results showing that bag of words could outperform embeddings. This experiment attempts to tweak the embeddings to show the expected results under the assumption that the problem is not the amount of data. If the problem is the amount of data we will investigate this in another experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will show Bag of Words results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import training_helpers\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Embedding, MaxPooling2D, LSTM\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from pandas import DataFrame\n",
    "from seaborn import boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "read_existing_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = training_helpers.get_data_frame()\n",
    "raw_features = data_frame['review']\n",
    "labels = [x for x in data_frame['deceptive']]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_features)\n",
    "bow_features = tokenizer.texts_to_matrix(raw_features, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1\n",
    "maxlen = max([len(x) for x in raw_features])\n",
    "padded_reviews = pad_sequences(bow_features, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 129us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 115us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 130us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 204us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 121us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 108us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 130us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 108us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 139us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 115us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_bow_model():\n",
    "  model = Sequential([\n",
    "      Dense(16, activation=relu, input_shape=(corpus_vocab_size,), kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "ff_bow_scores = run_cross_validate(get_ff_bow_model, bow_features, labels, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will get results for embeddings. This time I will use pretrained Word2Vec. Although this was not trained directly on words from our dataset, the Word2Vec has a higher dimensionality (making it harder to run on our machines) and so may show better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = None\n",
    "embedding_length = 0\n",
    "if read_existing_embeddings:\n",
    "  embedding_length=300\n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  with open('data.json', 'r') as infile:\n",
    "      data = json.load(infile)\n",
    "      for i in range(len(data)):\n",
    "          embedding_matrix[i] = np.array(data[i], dtype=np.float32)\n",
    "else:\n",
    "  word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"../data/embeddings/GoogleNews-vectors-negative300.bin\",\n",
    "                                                                 binary=True)\n",
    "  embedding_length = word_vectors.vector_size\n",
    "    \n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  for word, index in corpus_words.items():\n",
    "    if word in word_vectors.vocab:\n",
    "      embedding_matrix[index] = np.array(word_vectors[word], dtype=np.float32)\n",
    "\n",
    "  with open('data.json', 'w') as outfile:\n",
    "      json.dump(embedding_matrix.tolist(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ff_wv_model():\n",
    "  model_ff_wv = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], trainable=False,\n",
    "                input_length=corpus_vocab_size),\n",
    "      Flatten(),\n",
    "      Dense(16, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model_ff_wv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model_ff_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff_wv_scores = run_cross_validate(get_ff_wv_model, bow_features, labels, cv=5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fitting with:  (1280, 9839) labels (1280,)\n",
    "Train on 896 samples, validate on 384 samples\n",
    "Epoch 1/12\n",
    "896/896 [==============================] - 64s 71ms/step - loss: 0.9597 - acc: 0.5413 - val_loss: 1.0838 - val_acc: 0.5859\n",
    "Epoch 2/12\n",
    "896/896 [==============================] - 42s 47ms/step - loss: 1.0716 - acc: 0.6696 - val_loss: 1.1920 - val_acc: 0.6693\n",
    "Epoch 3/12\n",
    "896/896 [==============================] - 42s 47ms/step - loss: 1.0781 - acc: 0.7277 - val_loss: 1.0938 - val_acc: 0.6823\n",
    "Epoch 4/12\n",
    "896/896 [==============================] - 44s 49ms/step - loss: 1.0370 - acc: 0.7277 - val_loss: 1.2011 - val_acc: 0.6536\n",
    "Epoch 5/12\n",
    "896/896 [==============================] - 42s 47ms/step - loss: 1.0845 - acc: 0.7779 - val_loss: 1.2248 - val_acc: 0.6979\n",
    "320/320 [==============================] - 2s 7ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Bag of words: \", ff_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", ff_wv_scores['accuracies'])\n",
    "\n",
    "ff_scores_entries =[('Bag of Words', x) for x in ff_bow_scores['accuracies']] + [('Word Vectors', x) for x in ff_wv_scores['accuracies']]\n",
    "ff_scores_data_frame = DataFrame(ff_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bag of words:  [0.875, 0.8625, 0.88125, 0.88125, 0.86875, 0.91875, 0.875, 0.8625, 0.88125, 0.8875]\n",
    "Word vectors:  [0.628125, 0.709375, 0.665625, 0.703125, 0.659375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(x='input type', y='accuracy', data=ff_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/aiWQUvv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like word vectors are doing better now! Although they should be more accurate than Bag of Words, unless this is an exceptional case. The next step here is to investigate how BoW and word vectors perform on more data, since a small amount of data is a case known to cause results like this. It is also very unlikely Bag of Words will perform as well on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research has shown that word embeddings perform better than Bag of Words (Convolutional Neural Networks for Sentence Classification, Yoon Kim 2014). We will use our convolutional network on pretrained Word2Vec embeddings to see if we obtain an improved accuracy. First we will obtain results for bag of words again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 9839, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in bow_features]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.4673 - acc: 0.7736 - val_loss: 0.6609 - val_acc: 0.6328\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.1416 - acc: 0.9543 - val_loss: 0.7442 - val_acc: 0.6374\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0840 - acc: 0.9682 - val_loss: 0.8427 - val_acc: 0.6212\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0469 - acc: 0.9930 - val_loss: 1.0039 - val_acc: 0.6143\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0275 - acc: 0.9970 - val_loss: 1.2231 - val_acc: 0.5866\n",
      "160/160 [==============================] - 1s 6ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4866 - acc: 0.7617 - val_loss: 0.6351 - val_acc: 0.6559\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.1448 - acc: 0.9573 - val_loss: 0.7271 - val_acc: 0.6490\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0770 - acc: 0.9791 - val_loss: 0.8389 - val_acc: 0.6513\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0442 - acc: 0.9930 - val_loss: 0.9812 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.0255 - acc: 0.9980 - val_loss: 1.1015 - val_acc: 0.6189\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.4150 - acc: 0.8034 - val_loss: 0.6728 - val_acc: 0.6397\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1272 - acc: 0.9652 - val_loss: 0.8141 - val_acc: 0.6305\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0675 - acc: 0.9861 - val_loss: 0.9793 - val_acc: 0.5866\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0365 - acc: 0.9950 - val_loss: 1.2924 - val_acc: 0.5935\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0231 - acc: 0.9970 - val_loss: 1.5945 - val_acc: 0.5774\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.4287 - acc: 0.7905 - val_loss: 0.6516 - val_acc: 0.6467\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.1335 - acc: 0.9623 - val_loss: 0.8195 - val_acc: 0.6259\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0775 - acc: 0.9772 - val_loss: 1.0004 - val_acc: 0.5820\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0407 - acc: 0.9930 - val_loss: 1.2430 - val_acc: 0.5797\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0245 - acc: 0.9970 - val_loss: 1.3674 - val_acc: 0.5727\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4416 - acc: 0.7855 - val_loss: 0.6441 - val_acc: 0.6582\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1297 - acc: 0.9662 - val_loss: 0.7761 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0685 - acc: 0.9861 - val_loss: 0.9527 - val_acc: 0.6305\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0392 - acc: 0.9930 - val_loss: 1.2719 - val_acc: 0.5889\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0239 - acc: 0.9980 - val_loss: 1.2118 - val_acc: 0.6166\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4231 - acc: 0.7865 - val_loss: 0.7315 - val_acc: 0.6189\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1220 - acc: 0.9613 - val_loss: 0.8145 - val_acc: 0.6351\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0730 - acc: 0.9791 - val_loss: 1.1843 - val_acc: 0.6074\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0422 - acc: 0.9901 - val_loss: 1.0451 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0211 - acc: 0.9980 - val_loss: 1.2156 - val_acc: 0.6005\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4392 - acc: 0.7964 - val_loss: 0.6573 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.1323 - acc: 0.9563 - val_loss: 0.8693 - val_acc: 0.6097\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0868 - acc: 0.9732 - val_loss: 0.8662 - val_acc: 0.6236\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0416 - acc: 0.9901 - val_loss: 1.0059 - val_acc: 0.6212\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0254 - acc: 0.9970 - val_loss: 1.1542 - val_acc: 0.6051\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.5041 - acc: 0.7507 - val_loss: 0.6621 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.1482 - acc: 0.9484 - val_loss: 0.7484 - val_acc: 0.6490\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.0783 - acc: 0.9811 - val_loss: 0.8698 - val_acc: 0.6397\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0416 - acc: 0.9940 - val_loss: 1.0058 - val_acc: 0.6513\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0255 - acc: 0.9980 - val_loss: 1.2594 - val_acc: 0.6189\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4309 - acc: 0.8044 - val_loss: 0.6791 - val_acc: 0.6467\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1267 - acc: 0.9603 - val_loss: 0.7612 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0685 - acc: 0.9841 - val_loss: 0.8830 - val_acc: 0.6305\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0371 - acc: 0.9921 - val_loss: 1.0377 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0226 - acc: 0.9960 - val_loss: 1.1543 - val_acc: 0.5958\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4766 - acc: 0.7607 - val_loss: 0.7455 - val_acc: 0.5982\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1364 - acc: 0.9553 - val_loss: 0.7625 - val_acc: 0.6374\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0727 - acc: 0.9811 - val_loss: 0.8296 - val_acc: 0.6467\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0428 - acc: 0.9921 - val_loss: 0.9457 - val_acc: 0.6351\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0253 - acc: 0.9960 - val_loss: 1.1094 - val_acc: 0.6236\n",
      "160/160 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_conv_bow_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(1, 10),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(1, corpus_vocab_size, 1),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(pool_size=(1, 10)),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "conv_bow_scores = run_cross_validate(get_conv_bow_model, convolutional_data, labels, cv=10, categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out reviews with more than 300 words because a small number have an exceptionally large number of words and dramatically increase the memory requirements. These reviews are rare and are not expected to provide much value, while also preventing this experiment from being run on a normal machine, so I will filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_words = []\n",
    "conv_wv_labels = []\n",
    "for i, raw_feature in enumerate(raw_features):\n",
    "    word_sequence = text_to_word_sequence(raw_feature)\n",
    "    if len(word_sequence) > 300:\n",
    "        continue\n",
    "    conv_wv_labels.append(labels[i])\n",
    "    reviews_words.append(word_sequence)\n",
    "max_review_len = max([len(x) for x in reviews_words])\n",
    "\n",
    "vectorized_reviews = np.zeros((len(reviews_words), 1, max_review_len, 300))\n",
    "for i, review in enumerate(reviews_words):\n",
    "    for j, word in enumerate(review):\n",
    "        vectorized_reviews[i][0][j] = [x for x in embedding_matrix[corpus_words[word]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1254, 1, 300, 300) labels (1254, 2)\n",
      "Train on 877 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "877/877 [==============================] - 17s 20ms/step - loss: 0.6448 - acc: 0.6192 - val_loss: 0.5773 - val_acc: 0.7029\n",
      "Epoch 2/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.4310 - acc: 0.7925 - val_loss: 0.5165 - val_acc: 0.7427\n",
      "Epoch 3/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.2388 - acc: 0.9065 - val_loss: 0.5490 - val_acc: 0.7241\n",
      "Epoch 4/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.1034 - acc: 0.9795 - val_loss: 0.5430 - val_acc: 0.7533\n",
      "Epoch 5/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0381 - acc: 0.9966 - val_loss: 0.5544 - val_acc: 0.7507\n",
      "Epoch 6/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.5754 - val_acc: 0.7586\n",
      "252/252 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1254, 1, 300, 300) labels (1254, 2)\n",
      "Train on 877 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "877/877 [==============================] - 17s 19ms/step - loss: 0.6701 - acc: 0.5621 - val_loss: 0.6454 - val_acc: 0.6207\n",
      "Epoch 2/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.4743 - acc: 0.7822 - val_loss: 0.5089 - val_acc: 0.7162\n",
      "Epoch 3/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.2522 - acc: 0.9111 - val_loss: 0.5087 - val_acc: 0.7427\n",
      "Epoch 4/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0949 - acc: 0.9829 - val_loss: 0.5494 - val_acc: 0.7374\n",
      "Epoch 5/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0325 - acc: 0.9977 - val_loss: 0.5930 - val_acc: 0.7586\n",
      "Epoch 6/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0136 - acc: 0.9989 - val_loss: 0.5995 - val_acc: 0.7507\n",
      "Epoch 7/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.6477 - val_acc: 0.7374\n",
      "252/252 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1255, 1, 300, 300) labels (1255, 2)\n",
      "Train on 878 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "878/878 [==============================] - 17s 19ms/step - loss: 0.6694 - acc: 0.5923 - val_loss: 0.6246 - val_acc: 0.6605\n",
      "Epoch 2/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.4484 - acc: 0.8052 - val_loss: 0.5542 - val_acc: 0.7082\n",
      "Epoch 3/12\n",
      "878/878 [==============================] - 14s 16ms/step - loss: 0.2252 - acc: 0.9271 - val_loss: 0.5563 - val_acc: 0.7294\n",
      "Epoch 4/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0857 - acc: 0.9909 - val_loss: 0.5321 - val_acc: 0.7294\n",
      "Epoch 5/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0282 - acc: 0.9977 - val_loss: 0.5418 - val_acc: 0.7480\n",
      "Epoch 6/12\n",
      "878/878 [==============================] - 15s 17ms/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 0.7560\n",
      "Epoch 7/12\n",
      "878/878 [==============================] - 15s 17ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 0.7586\n",
      "Epoch 8/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6039 - val_acc: 0.7401\n",
      "251/251 [==============================] - 2s 6ms/step\n",
      "Fitting with:  (1255, 1, 300, 300) labels (1255, 2)\n",
      "Train on 878 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "878/878 [==============================] - 17s 19ms/step - loss: 0.6625 - acc: 0.6002 - val_loss: 0.6225 - val_acc: 0.6419\n",
      "Epoch 2/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.4386 - acc: 0.7938 - val_loss: 0.5156 - val_acc: 0.7401\n",
      "Epoch 3/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.2044 - acc: 0.9362 - val_loss: 0.5069 - val_acc: 0.7613\n",
      "Epoch 4/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0628 - acc: 0.9932 - val_loss: 0.8429 - val_acc: 0.6976\n",
      "Epoch 5/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0446 - acc: 0.9909 - val_loss: 0.5959 - val_acc: 0.7586\n",
      "Epoch 6/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0115 - acc: 0.9989 - val_loss: 0.6381 - val_acc: 0.7666\n",
      "Epoch 7/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.6359 - val_acc: 0.7427\n",
      "251/251 [==============================] - 1s 6ms/step\n",
      "Fitting with:  (1256, 1, 300, 300) labels (1256, 2)\n",
      "Train on 879 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.6710 - acc: 0.5484 - val_loss: 0.6934 - val_acc: 0.5225\n",
      "Epoch 2/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.4827 - acc: 0.7679 - val_loss: 0.6038 - val_acc: 0.6737\n",
      "Epoch 3/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.2814 - acc: 0.8896 - val_loss: 0.5930 - val_acc: 0.7401\n",
      "Epoch 4/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.1293 - acc: 0.9625 - val_loss: 0.5974 - val_acc: 0.7294\n",
      "Epoch 5/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0403 - acc: 0.9977 - val_loss: 0.6317 - val_acc: 0.7507\n",
      "Epoch 6/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0174 - acc: 0.9989 - val_loss: 0.6468 - val_acc: 0.7374\n",
      "Epoch 7/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.7038 - val_acc: 0.7268\n",
      "250/250 [==============================] - 1s 6ms/step\n",
      "Fitting with:  (1256, 1, 300, 300) labels (1256, 2)\n",
      "Train on 879 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.6383 - acc: 0.6268 - val_loss: 0.6714 - val_acc: 0.6286\n",
      "Epoch 2/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.3930 - acc: 0.8271 - val_loss: 0.6001 - val_acc: 0.6870\n",
      "Epoch 3/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.1705 - acc: 0.9545 - val_loss: 0.5629 - val_acc: 0.7533\n",
      "Epoch 4/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0626 - acc: 0.9932 - val_loss: 0.5972 - val_acc: 0.7560\n",
      "Epoch 5/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.6161 - val_acc: 0.7560\n",
      "Epoch 6/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.6249 - val_acc: 0.7772\n",
      "Epoch 7/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6510 - val_acc: 0.7666\n",
      "250/250 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 300),\n",
    "          data_format=\"channels_first\",\n",
    "          input_shape=(1, 300, 300),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(strides=(1, 1), pool_size=(2, 1), data_format=\"channels_first\"),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, vectorized_reviews, conv_wv_labels, cv=6, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.825, 0.80625, 0.8, 0.8625, 0.81875, 0.75625, 0.8625, 0.7625, 0.79375, 0.8375]\n",
      "Word vectors:  [0.7658730168191213, 0.8055555527172391, 0.7888446222263503, 0.7729083672462707, 0.7839999980926514, 0.7400000014305115]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", conv_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", conv_wv_scores['accuracies'])\n",
    "\n",
    "conv_scores_entries =[('Bag of Words', x) for x in conv_bow_scores['accuracies']] + [('Word Vectors', x) for x in conv_wv_scores['accuracies']]\n",
    "conv_scores_data_frame = DataFrame(conv_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=conv_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again our results look better. If this continues to be consistent we know that Word2Vec has some property that is beneficial, which may be training on more data, or it may be the higher vector dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final experiment we will run to test the results of using pretrained Word2Vec embeddings will be our LSTM network. \n",
    "In the previous test we did with word embeddings, our accuracy was 50%, so simply a random guess. Following the trend above of pretrained embeddings giving us better accuracies, we will run a test and see if this can give any improvements for the LSTM network.\n",
    "Because LSTM networks only use word embeddings as inputs, we will take our 50% result from using the OpSpam embeddings as our benchmark and immediately test the pretrained ones to see if we can get an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1280, 4159) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/3\n",
      "896/896 [==============================] - 125s 140ms/step - loss: 0.7037 - acc: 0.4911 - val_loss: 0.6972 - val_acc: 0.4844\n",
      "Epoch 2/3\n",
      "896/896 [==============================] - 104s 117ms/step - loss: 0.6960 - acc: 0.4710 - val_loss: 0.6927 - val_acc: 0.5156\n",
      "Epoch 3/3\n",
      "896/896 [==============================] - 101s 113ms/step - loss: 0.6954 - acc: 0.5000 - val_loss: 0.6966 - val_acc: 0.4844\n",
      "320/320 [==============================] - 19s 60ms/step\n",
      "Fitting with:  (1280, 4159) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/3\n",
      "896/896 [==============================] - 106s 118ms/step - loss: 0.6954 - acc: 0.4933 - val_loss: 0.6946 - val_acc: 0.4844\n",
      "Epoch 2/3\n",
      "896/896 [==============================] - 103s 115ms/step - loss: 0.6933 - acc: 0.5067 - val_loss: 0.6927 - val_acc: 0.5182\n",
      "Epoch 3/3\n",
      "896/896 [==============================] - 99s 111ms/step - loss: 0.6949 - acc: 0.4821 - val_loss: 0.6937 - val_acc: 0.4818\n",
      "320/320 [==============================] - 20s 62ms/step\n",
      "Fitting with:  (1280, 4159) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/3\n",
      "896/896 [==============================] - 106s 119ms/step - loss: 0.6961 - acc: 0.5078 - val_loss: 0.6934 - val_acc: 0.5026\n",
      "Epoch 2/3\n",
      "896/896 [==============================] - 100s 112ms/step - loss: 0.6978 - acc: 0.4877 - val_loss: 0.6945 - val_acc: 0.5026\n",
      "Epoch 3/3\n",
      "896/896 [==============================] - 104s 116ms/step - loss: 0.6958 - acc: 0.4855 - val_loss: 0.6935 - val_acc: 0.5026\n",
      "320/320 [==============================] - 20s 64ms/step\n",
      "Fitting with:  (1280, 4159) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/3\n",
      "896/896 [==============================] - 112s 125ms/step - loss: 0.7029 - acc: 0.4978 - val_loss: 0.6949 - val_acc: 0.4896\n",
      "Epoch 2/3\n",
      "896/896 [==============================] - 128s 143ms/step - loss: 0.6947 - acc: 0.5033 - val_loss: 0.6934 - val_acc: 0.5104\n",
      "Epoch 3/3\n",
      "896/896 [==============================] - 119s 133ms/step - loss: 0.6956 - acc: 0.4911 - val_loss: 0.6977 - val_acc: 0.4896\n",
      "320/320 [==============================] - 22s 69ms/step\n",
      "Fitting with:  (1280, 4159) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/3\n",
      "896/896 [==============================] - 112s 124ms/step - loss: 0.7080 - acc: 0.4911 - val_loss: 0.6920 - val_acc: 0.5208\n",
      "Epoch 2/3\n",
      "896/896 [==============================] - 135s 151ms/step - loss: nan - acc: 0.5011 - val_loss: nan - val_acc: 0.5208\n",
      "Epoch 3/3\n",
      "896/896 [==============================] - 112s 125ms/step - loss: nan - acc: 0.4911 - val_loss: nan - val_acc: 0.5208\n",
      "320/320 [==============================] - 20s 62ms/step\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0.1)\n",
    "def get_lstm_wv_model():\n",
    "  model = Sequential([\n",
    "        Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        LSTM(24),\n",
    "        Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "lstm_wv_scores = run_cross_validate(get_lstm_wv_model, padded_reviews, labels, cv=5, verbose=1, epochs=3, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_scores_entries =[('Word Vectors', x) for x in lstm_bow_scores['accuracies']]\n",
    "lstm_scores_data_frame = DataFrame(lstm_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=lstm_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know. Pretty inconclusive, crappy graph. \n",
    "\n",
    "At this stage, we now know that LSTM's are not going to work with a dataset this small. \n",
    "After trying many configurations of learning rates, batch sizes, data normalization, LSTM cells, dropout, embedding dimensions and review truncated lengths we can conclude that we're going to have to use more data to start seeing some results from LSTM's. \n",
    "\n",
    "They also take an extremely long time to run, so it's hard to know if we're doing the right thing by running them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
