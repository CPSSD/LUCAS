{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Neural Models on pretrained Word2Vec embeddings\n",
    "\n",
    "## Feedforward Neural Network\n",
    "\n",
    "Following an experiment to compare neural models, we discovered odd results showing that bag of words could outperform embeddings. This experiment attempts to tweak the embeddings to show the expected results under the assumption that the problem is not the amount of data. If the problem is the amount of data we will investigate this in another experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will show Bag of Words results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import training_helpers\n",
    "from scripts.cross_validate import run_cross_validate\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Embedding, MaxPooling2D, LSTM\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from pandas import DataFrame\n",
    "from seaborn import boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "read_existing_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = training_helpers.get_data_frame()\n",
    "raw_features = data_frame['review']\n",
    "labels = [x for x in data_frame['deceptive']]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_features)\n",
    "bow_features = tokenizer.texts_to_matrix(raw_features, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = tokenizer.word_index\n",
    "corpus_vocab_size = len(corpus_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 129us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 115us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 130us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 204us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 121us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 108us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 130us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 108us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 139us/step\n",
      "Fitting with:  (1440, 9839) labels (1440,)\n",
      "160/160 [==============================] - 0s 115us/step\n"
     ]
    }
   ],
   "source": [
    "def get_ff_bow_model():\n",
    "  model = Sequential([\n",
    "      Dense(16, activation=relu, input_shape=(corpus_vocab_size,), kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "ff_bow_scores = run_cross_validate(get_ff_bow_model, bow_features, labels, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will get results for embeddings. This time I will use pretrained Word2Vec. Although this was not trained directly on words from our dataset, the Word2Vec has a higher dimensionality (making it harder to run on our machines) and so may show better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = None\n",
    "embedding_length = 0\n",
    "if read_existing_embeddings:\n",
    "  embedding_length=300\n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  with open('data.json', 'r') as infile:\n",
    "      data = json.load(infile)\n",
    "      for i in range(len(data)):\n",
    "          embedding_matrix[i] = np.array(data[i], dtype=np.float32)\n",
    "else:\n",
    "  word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"../data/embeddings/GoogleNews-vectors-negative300.bin\",\n",
    "                                                                 binary=True)\n",
    "  embedding_length = word_vectors.vector_size\n",
    "    \n",
    "  embedding_matrix = np.zeros((corpus_vocab_size, embedding_length))\n",
    "  for word, index in corpus_words.items():\n",
    "    if word in word_vectors.vocab:\n",
    "      embedding_matrix[index] = np.array(word_vectors[word], dtype=np.float32)\n",
    "\n",
    "  with open('data.json', 'w') as outfile:\n",
    "      json.dump(embedding_matrix.tolist(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ff_wv_model():\n",
    "  model_ff_wv = Sequential([\n",
    "      Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], trainable=False,\n",
    "                input_length=corpus_vocab_size),\n",
    "      Flatten(),\n",
    "      Dense(16, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dropout(0.25),\n",
    "      Dense(8, activation=relu, kernel_regularizer=l2(0.01)),\n",
    "      Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model_ff_wv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model_ff_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff_wv_scores = run_cross_validate(get_ff_wv_model, bow_features, labels, cv=5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fitting with:  (1280, 9839) labels (1280,)\n",
    "Train on 896 samples, validate on 384 samples\n",
    "Epoch 1/12\n",
    "896/896 [==============================] - 64s 71ms/step - loss: 0.9597 - acc: 0.5413 - val_loss: 1.0838 - val_acc: 0.5859\n",
    "Epoch 2/12\n",
    "896/896 [==============================] - 42s 47ms/step - loss: 1.0716 - acc: 0.6696 - val_loss: 1.1920 - val_acc: 0.6693\n",
    "Epoch 3/12\n",
    "896/896 [==============================] - 42s 47ms/step - loss: 1.0781 - acc: 0.7277 - val_loss: 1.0938 - val_acc: 0.6823\n",
    "Epoch 4/12\n",
    "896/896 [==============================] - 44s 49ms/step - loss: 1.0370 - acc: 0.7277 - val_loss: 1.2011 - val_acc: 0.6536\n",
    "Epoch 5/12\n",
    "896/896 [==============================] - 42s 47ms/step - loss: 1.0845 - acc: 0.7779 - val_loss: 1.2248 - val_acc: 0.6979\n",
    "320/320 [==============================] - 2s 7ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Bag of words: \", ff_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", ff_wv_scores['accuracies'])\n",
    "\n",
    "ff_scores_entries =[('Bag of Words', x) for x in ff_bow_scores['accuracies']] + [('Word Vectors', x) for x in ff_wv_scores['accuracies']]\n",
    "ff_scores_data_frame = DataFrame(ff_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bag of words:  [0.875, 0.8625, 0.88125, 0.88125, 0.86875, 0.91875, 0.875, 0.8625, 0.88125, 0.8875]\n",
    "Word vectors:  [0.628125, 0.709375, 0.665625, 0.703125, 0.659375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(x='input type', y='accuracy', data=ff_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/aiWQUvv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like word vectors are doing better now! Although they should be more accurate than Bag of Words, unless this is an exceptional case. The next step here is to investigate how BoW and word vectors perform on more data, since a small amount of data is a case known to cause results like this. It is also very unlikely Bag of Words will perform as well on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research has shown that word embeddings perform better than Bag of Words (Convolutional Neural Networks for Sentence Classification, Yoon Kim 2014). We will use our convolutional network on pretrained Word2Vec embeddings to see if we obtain an improved accuracy. First we will obtain results for bag of words again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1, 9839, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = 1600\n",
    "convolutional_data = np.array(np.split(np.array([[[y] for y in z] for z in bow_features]), batches))\n",
    "convolutional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.4673 - acc: 0.7736 - val_loss: 0.6609 - val_acc: 0.6328\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 18s 18ms/step - loss: 0.1416 - acc: 0.9543 - val_loss: 0.7442 - val_acc: 0.6374\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0840 - acc: 0.9682 - val_loss: 0.8427 - val_acc: 0.6212\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0469 - acc: 0.9930 - val_loss: 1.0039 - val_acc: 0.6143\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0275 - acc: 0.9970 - val_loss: 1.2231 - val_acc: 0.5866\n",
      "160/160 [==============================] - 1s 6ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4866 - acc: 0.7617 - val_loss: 0.6351 - val_acc: 0.6559\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.1448 - acc: 0.9573 - val_loss: 0.7271 - val_acc: 0.6490\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0770 - acc: 0.9791 - val_loss: 0.8389 - val_acc: 0.6513\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0442 - acc: 0.9930 - val_loss: 0.9812 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.0255 - acc: 0.9980 - val_loss: 1.1015 - val_acc: 0.6189\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 19s 19ms/step - loss: 0.4150 - acc: 0.8034 - val_loss: 0.6728 - val_acc: 0.6397\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1272 - acc: 0.9652 - val_loss: 0.8141 - val_acc: 0.6305\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0675 - acc: 0.9861 - val_loss: 0.9793 - val_acc: 0.5866\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0365 - acc: 0.9950 - val_loss: 1.2924 - val_acc: 0.5935\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0231 - acc: 0.9970 - val_loss: 1.5945 - val_acc: 0.5774\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.4287 - acc: 0.7905 - val_loss: 0.6516 - val_acc: 0.6467\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.1335 - acc: 0.9623 - val_loss: 0.8195 - val_acc: 0.6259\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0775 - acc: 0.9772 - val_loss: 1.0004 - val_acc: 0.5820\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0407 - acc: 0.9930 - val_loss: 1.2430 - val_acc: 0.5797\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0245 - acc: 0.9970 - val_loss: 1.3674 - val_acc: 0.5727\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4416 - acc: 0.7855 - val_loss: 0.6441 - val_acc: 0.6582\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1297 - acc: 0.9662 - val_loss: 0.7761 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0685 - acc: 0.9861 - val_loss: 0.9527 - val_acc: 0.6305\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.0392 - acc: 0.9930 - val_loss: 1.2719 - val_acc: 0.5889\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0239 - acc: 0.9980 - val_loss: 1.2118 - val_acc: 0.6166\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 16s 16ms/step - loss: 0.4231 - acc: 0.7865 - val_loss: 0.7315 - val_acc: 0.6189\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1220 - acc: 0.9613 - val_loss: 0.8145 - val_acc: 0.6351\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0730 - acc: 0.9791 - val_loss: 1.1843 - val_acc: 0.6074\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0422 - acc: 0.9901 - val_loss: 1.0451 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0211 - acc: 0.9980 - val_loss: 1.2156 - val_acc: 0.6005\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4392 - acc: 0.7964 - val_loss: 0.6573 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.1323 - acc: 0.9563 - val_loss: 0.8693 - val_acc: 0.6097\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0868 - acc: 0.9732 - val_loss: 0.8662 - val_acc: 0.6236\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0416 - acc: 0.9901 - val_loss: 1.0059 - val_acc: 0.6212\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0254 - acc: 0.9970 - val_loss: 1.1542 - val_acc: 0.6051\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 16ms/step - loss: 0.5041 - acc: 0.7507 - val_loss: 0.6621 - val_acc: 0.6513\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.1482 - acc: 0.9484 - val_loss: 0.7484 - val_acc: 0.6490\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 13ms/step - loss: 0.0783 - acc: 0.9811 - val_loss: 0.8698 - val_acc: 0.6397\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 13s 13ms/step - loss: 0.0416 - acc: 0.9940 - val_loss: 1.0058 - val_acc: 0.6513\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 15s 15ms/step - loss: 0.0255 - acc: 0.9980 - val_loss: 1.2594 - val_acc: 0.6189\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4309 - acc: 0.8044 - val_loss: 0.6791 - val_acc: 0.6467\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1267 - acc: 0.9603 - val_loss: 0.7612 - val_acc: 0.6467\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0685 - acc: 0.9841 - val_loss: 0.8830 - val_acc: 0.6305\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0371 - acc: 0.9921 - val_loss: 1.0377 - val_acc: 0.6120\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0226 - acc: 0.9960 - val_loss: 1.1543 - val_acc: 0.5958\n",
      "160/160 [==============================] - 1s 5ms/step\n",
      "Fitting with:  (1440, 1, 9839, 1) labels (1440, 2)\n",
      "Train on 1007 samples, validate on 433 samples\n",
      "Epoch 1/12\n",
      "1007/1007 [==============================] - 17s 17ms/step - loss: 0.4766 - acc: 0.7607 - val_loss: 0.7455 - val_acc: 0.5982\n",
      "Epoch 2/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.1364 - acc: 0.9553 - val_loss: 0.7625 - val_acc: 0.6374\n",
      "Epoch 3/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0727 - acc: 0.9811 - val_loss: 0.8296 - val_acc: 0.6467\n",
      "Epoch 4/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0428 - acc: 0.9921 - val_loss: 0.9457 - val_acc: 0.6351\n",
      "Epoch 5/12\n",
      "1007/1007 [==============================] - 14s 14ms/step - loss: 0.0253 - acc: 0.9960 - val_loss: 1.1094 - val_acc: 0.6236\n",
      "160/160 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_conv_bow_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(1, 10),\n",
    "          data_format=\"channels_last\",\n",
    "          input_shape=(1, corpus_vocab_size, 1),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(pool_size=(1, 10)),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "conv_bow_scores = run_cross_validate(get_conv_bow_model, convolutional_data, labels, cv=10, categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out reviews with more than 300 words because a small number have an exceptionally large number of words and dramatically increase the memory requirements. These reviews are rare and are not expected to provide much value, while also preventing this experiment from being run on a normal machine, so I will filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_words = []\n",
    "conv_wv_labels = []\n",
    "for i, raw_feature in enumerate(raw_features):\n",
    "    word_sequence = text_to_word_sequence(raw_feature)\n",
    "    if len(word_sequence) > 300:\n",
    "        continue\n",
    "    conv_wv_labels.append(labels[i])\n",
    "    reviews_words.append(word_sequence)\n",
    "max_review_len = max([len(x) for x in reviews_words])\n",
    "\n",
    "vectorized_reviews = np.zeros((len(reviews_words), 1, max_review_len, 300))\n",
    "for i, review in enumerate(reviews_words):\n",
    "    for j, word in enumerate(review):\n",
    "        vectorized_reviews[i][0][j] = [x for x in embedding_matrix[corpus_words[word]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1254, 1, 300, 300) labels (1254, 2)\n",
      "Train on 877 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "877/877 [==============================] - 17s 20ms/step - loss: 0.6448 - acc: 0.6192 - val_loss: 0.5773 - val_acc: 0.7029\n",
      "Epoch 2/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.4310 - acc: 0.7925 - val_loss: 0.5165 - val_acc: 0.7427\n",
      "Epoch 3/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.2388 - acc: 0.9065 - val_loss: 0.5490 - val_acc: 0.7241\n",
      "Epoch 4/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.1034 - acc: 0.9795 - val_loss: 0.5430 - val_acc: 0.7533\n",
      "Epoch 5/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0381 - acc: 0.9966 - val_loss: 0.5544 - val_acc: 0.7507\n",
      "Epoch 6/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.5754 - val_acc: 0.7586\n",
      "252/252 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1254, 1, 300, 300) labels (1254, 2)\n",
      "Train on 877 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "877/877 [==============================] - 17s 19ms/step - loss: 0.6701 - acc: 0.5621 - val_loss: 0.6454 - val_acc: 0.6207\n",
      "Epoch 2/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.4743 - acc: 0.7822 - val_loss: 0.5089 - val_acc: 0.7162\n",
      "Epoch 3/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.2522 - acc: 0.9111 - val_loss: 0.5087 - val_acc: 0.7427\n",
      "Epoch 4/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0949 - acc: 0.9829 - val_loss: 0.5494 - val_acc: 0.7374\n",
      "Epoch 5/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0325 - acc: 0.9977 - val_loss: 0.5930 - val_acc: 0.7586\n",
      "Epoch 6/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0136 - acc: 0.9989 - val_loss: 0.5995 - val_acc: 0.7507\n",
      "Epoch 7/12\n",
      "877/877 [==============================] - 13s 15ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.6477 - val_acc: 0.7374\n",
      "252/252 [==============================] - 2s 7ms/step\n",
      "Fitting with:  (1255, 1, 300, 300) labels (1255, 2)\n",
      "Train on 878 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "878/878 [==============================] - 17s 19ms/step - loss: 0.6694 - acc: 0.5923 - val_loss: 0.6246 - val_acc: 0.6605\n",
      "Epoch 2/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.4484 - acc: 0.8052 - val_loss: 0.5542 - val_acc: 0.7082\n",
      "Epoch 3/12\n",
      "878/878 [==============================] - 14s 16ms/step - loss: 0.2252 - acc: 0.9271 - val_loss: 0.5563 - val_acc: 0.7294\n",
      "Epoch 4/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0857 - acc: 0.9909 - val_loss: 0.5321 - val_acc: 0.7294\n",
      "Epoch 5/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0282 - acc: 0.9977 - val_loss: 0.5418 - val_acc: 0.7480\n",
      "Epoch 6/12\n",
      "878/878 [==============================] - 15s 17ms/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 0.7560\n",
      "Epoch 7/12\n",
      "878/878 [==============================] - 15s 17ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 0.7586\n",
      "Epoch 8/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6039 - val_acc: 0.7401\n",
      "251/251 [==============================] - 2s 6ms/step\n",
      "Fitting with:  (1255, 1, 300, 300) labels (1255, 2)\n",
      "Train on 878 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "878/878 [==============================] - 17s 19ms/step - loss: 0.6625 - acc: 0.6002 - val_loss: 0.6225 - val_acc: 0.6419\n",
      "Epoch 2/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.4386 - acc: 0.7938 - val_loss: 0.5156 - val_acc: 0.7401\n",
      "Epoch 3/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.2044 - acc: 0.9362 - val_loss: 0.5069 - val_acc: 0.7613\n",
      "Epoch 4/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0628 - acc: 0.9932 - val_loss: 0.8429 - val_acc: 0.6976\n",
      "Epoch 5/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0446 - acc: 0.9909 - val_loss: 0.5959 - val_acc: 0.7586\n",
      "Epoch 6/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0115 - acc: 0.9989 - val_loss: 0.6381 - val_acc: 0.7666\n",
      "Epoch 7/12\n",
      "878/878 [==============================] - 13s 15ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.6359 - val_acc: 0.7427\n",
      "251/251 [==============================] - 1s 6ms/step\n",
      "Fitting with:  (1256, 1, 300, 300) labels (1256, 2)\n",
      "Train on 879 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.6710 - acc: 0.5484 - val_loss: 0.6934 - val_acc: 0.5225\n",
      "Epoch 2/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.4827 - acc: 0.7679 - val_loss: 0.6038 - val_acc: 0.6737\n",
      "Epoch 3/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.2814 - acc: 0.8896 - val_loss: 0.5930 - val_acc: 0.7401\n",
      "Epoch 4/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.1293 - acc: 0.9625 - val_loss: 0.5974 - val_acc: 0.7294\n",
      "Epoch 5/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0403 - acc: 0.9977 - val_loss: 0.6317 - val_acc: 0.7507\n",
      "Epoch 6/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0174 - acc: 0.9989 - val_loss: 0.6468 - val_acc: 0.7374\n",
      "Epoch 7/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.7038 - val_acc: 0.7268\n",
      "250/250 [==============================] - 1s 6ms/step\n",
      "Fitting with:  (1256, 1, 300, 300) labels (1256, 2)\n",
      "Train on 879 samples, validate on 377 samples\n",
      "Epoch 1/12\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.6383 - acc: 0.6268 - val_loss: 0.6714 - val_acc: 0.6286\n",
      "Epoch 2/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.3930 - acc: 0.8271 - val_loss: 0.6001 - val_acc: 0.6870\n",
      "Epoch 3/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.1705 - acc: 0.9545 - val_loss: 0.5629 - val_acc: 0.7533\n",
      "Epoch 4/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0626 - acc: 0.9932 - val_loss: 0.5972 - val_acc: 0.7560\n",
      "Epoch 5/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.6161 - val_acc: 0.7560\n",
      "Epoch 6/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.6249 - val_acc: 0.7772\n",
      "Epoch 7/12\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6510 - val_acc: 0.7666\n",
      "250/250 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_conv_wv_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(\n",
    "          filters=50,\n",
    "          kernel_size=(10, 300),\n",
    "          data_format=\"channels_first\",\n",
    "          input_shape=(1, 300, 300),\n",
    "          activation=relu),\n",
    "      MaxPooling2D(strides=(1, 1), pool_size=(2, 1), data_format=\"channels_first\"),\n",
    "      Dropout(0.2),\n",
    "      Flatten(),\n",
    "      Dense(2, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "conv_wv_scores = run_cross_validate(get_conv_wv_model, vectorized_reviews, conv_wv_labels, cv=6, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.825, 0.80625, 0.8, 0.8625, 0.81875, 0.75625, 0.8625, 0.7625, 0.79375, 0.8375]\n",
      "Word vectors:  [0.7658730168191213, 0.8055555527172391, 0.7888446222263503, 0.7729083672462707, 0.7839999980926514, 0.7400000014305115]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", conv_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", conv_wv_scores['accuracies'])\n",
    "\n",
    "conv_scores_entries =[('Bag of Words', x) for x in conv_bow_scores['accuracies']] + [('Word Vectors', x) for x in conv_wv_scores['accuracies']]\n",
    "conv_scores_data_frame = DataFrame(conv_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=conv_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again our results look better. If this continues to be consistent we know that Word2Vec has some property that is beneficial, which may be training on more data, or it may be the higher vector dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final experiment we will run to test the results of using pretrained Word2Vec embeddings will be our LSTM network. \n",
    "In the previous test we did with word embeddings, our accuracy was 50%, so simply a random guess. Following the trend above of pretrained embeddings giving us better accuracies, we will run a test and see if this can give any improvements for the LSTM network.\n",
    "\n",
    "First, let's get our BOW accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1280, 1, 9839) labels (1280, 1)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 0.5607 - acc: 0.7277 - val_loss: 0.3866 - val_acc: 0.8646\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.1078 - acc: 0.9810 - val_loss: 0.3294 - val_acc: 0.8750\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0323 - acc: 0.9967 - val_loss: 0.3371 - val_acc: 0.8776\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.3449 - val_acc: 0.8828\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 4s 4ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.3536 - val_acc: 0.8776\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.3600 - val_acc: 0.8750\n",
      "320/320 [==============================] - 0s 354us/step\n",
      "Fitting with:  (1280, 1, 9839) labels (1280, 1)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 15s 16ms/step - loss: 0.5472 - acc: 0.7545 - val_loss: 0.3841 - val_acc: 0.8724\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0976 - acc: 0.9911 - val_loss: 0.3413 - val_acc: 0.8542\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0296 - acc: 0.9989 - val_loss: 0.3284 - val_acc: 0.8646\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 4s 5ms/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.3394 - val_acc: 0.8568\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 5s 5ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.3461 - val_acc: 0.8542\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 5s 6ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3513 - val_acc: 0.8568\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.3585 - val_acc: 0.8542\n",
      "320/320 [==============================] - 0s 204us/step\n",
      "Fitting with:  (1280, 1, 9839) labels (1280, 1)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 0.5624 - acc: 0.7377 - val_loss: 0.3653 - val_acc: 0.8906\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.1026 - acc: 0.9844 - val_loss: 0.3133 - val_acc: 0.8776\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0293 - acc: 0.9989 - val_loss: 0.2929 - val_acc: 0.8880\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 0.8828\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.3076 - val_acc: 0.8828\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.8802\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.3191 - val_acc: 0.8828\n",
      "320/320 [==============================] - 0s 233us/step\n",
      "Fitting with:  (1280, 1, 9839) labels (1280, 1)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 15s 16ms/step - loss: 0.5462 - acc: 0.7645 - val_loss: 0.3781 - val_acc: 0.8802\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.1127 - acc: 0.9799 - val_loss: 0.3121 - val_acc: 0.8672\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0357 - acc: 0.9989 - val_loss: 0.3150 - val_acc: 0.8698\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.3166 - val_acc: 0.8620\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.3207 - val_acc: 0.8698\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.3267 - val_acc: 0.8672\n",
      "320/320 [==============================] - 0s 222us/step\n",
      "Fitting with:  (1280, 1, 9839) labels (1280, 1)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 0.5444 - acc: 0.7478 - val_loss: 0.3838 - val_acc: 0.8672\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0966 - acc: 0.9877 - val_loss: 0.3191 - val_acc: 0.8620\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0274 - acc: 1.0000 - val_loss: 0.3064 - val_acc: 0.8802\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.3066 - val_acc: 0.8828\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.8828\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.3201 - val_acc: 0.8750\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.3249 - val_acc: 0.8776\n",
      "320/320 [==============================] - 0s 218us/step\n"
     ]
    }
   ],
   "source": [
    "batches = 1600\n",
    "padded = pad_sequences(bow_features)\n",
    "rnn_bow = np.array(np.split(padded, batches))\n",
    "max_len_bow = max([len(x) for x in padded])\n",
    "rnn_bow_targets = np.array([[x] for x in labels])\n",
    "\n",
    "def get_rnn_bow_model():\n",
    "  model = Sequential([\n",
    "      LSTM(20, input_shape=(1, max_len_bow)),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "rnn_bow_scores = run_cross_validate(get_rnn_bow_model, rnn_bow, rnn_bow_targets, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW again gives us astoundingly good accuracies, but of course, because it's over a tiny amount of data.\n",
    "\n",
    "Now let's try our pretrained Word2Vec embeddings, and compare it to our previous OpSpam embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with:  (1280, 784) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 45s 50ms/step - loss: 0.6931 - acc: 0.5078 - val_loss: 0.6876 - val_acc: 0.5599\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 31s 34ms/step - loss: 0.6858 - acc: 0.5647 - val_loss: 0.6814 - val_acc: 0.5703\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 0.6773 - acc: 0.6027 - val_loss: 0.6700 - val_acc: 0.6068\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 0.6606 - acc: 0.6373 - val_loss: 0.6542 - val_acc: 0.6250\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.6486 - acc: 0.6161 - val_loss: 0.6439 - val_acc: 0.6250\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.6370 - acc: 0.6306 - val_loss: 0.6120 - val_acc: 0.7031\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.5851 - acc: 0.7109 - val_loss: 0.5922 - val_acc: 0.7057\n",
      "Epoch 8/12\n",
      "896/896 [==============================] - 30s 34ms/step - loss: 0.6543 - acc: 0.6228 - val_loss: 0.7128 - val_acc: 0.5365\n",
      "Epoch 9/12\n",
      "896/896 [==============================] - 30s 34ms/step - loss: 0.6674 - acc: 0.5837 - val_loss: 0.6175 - val_acc: 0.6849\n",
      "Epoch 10/12\n",
      "896/896 [==============================] - 38s 43ms/step - loss: 0.5966 - acc: 0.6987 - val_loss: 0.6216 - val_acc: 0.6745\n",
      "Epoch 11/12\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 0.5716 - acc: 0.7266 - val_loss: 0.5999 - val_acc: 0.6823\n",
      "320/320 [==============================] - 4s 12ms/step\n",
      "Fitting with:  (1280, 784) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 49s 54ms/step - loss: 0.6936 - acc: 0.5201 - val_loss: 0.6904 - val_acc: 0.5443\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 36s 41ms/step - loss: 0.6855 - acc: 0.5592 - val_loss: 0.6864 - val_acc: 0.5677\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.6785 - acc: 0.5982 - val_loss: 0.6817 - val_acc: 0.5859\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.6695 - acc: 0.6429 - val_loss: 0.6724 - val_acc: 0.6354\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.6544 - acc: 0.6674 - val_loss: 0.6554 - val_acc: 0.6484\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6262 - acc: 0.6719 - val_loss: 0.6350 - val_acc: 0.6484\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6080 - acc: 0.6752 - val_loss: 0.6245 - val_acc: 0.6536\n",
      "Epoch 8/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.6597 - acc: 0.6172 - val_loss: 0.6720 - val_acc: 0.5833\n",
      "Epoch 9/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6373 - acc: 0.6272 - val_loss: 0.6292 - val_acc: 0.6615\n",
      "Epoch 10/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6093 - acc: 0.6819 - val_loss: 0.6268 - val_acc: 0.6849\n",
      "Epoch 11/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.6012 - acc: 0.6786 - val_loss: 0.6171 - val_acc: 0.6693\n",
      "Epoch 12/12\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.5862 - acc: 0.6953 - val_loss: 0.6056 - val_acc: 0.6797\n",
      "320/320 [==============================] - 4s 11ms/step\n",
      "Fitting with:  (1280, 784) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 44s 49ms/step - loss: 0.6915 - acc: 0.5324 - val_loss: 0.6902 - val_acc: 0.5417\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6832 - acc: 0.5826 - val_loss: 0.6836 - val_acc: 0.5755\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.6741 - acc: 0.6194 - val_loss: 0.6768 - val_acc: 0.5938\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6617 - acc: 0.6696 - val_loss: 0.6645 - val_acc: 0.6172\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6382 - acc: 0.6551 - val_loss: 0.6510 - val_acc: 0.5990\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.6200 - acc: 0.6574 - val_loss: 0.6349 - val_acc: 0.6432\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.5956 - acc: 0.6842 - val_loss: 0.6131 - val_acc: 0.6562\n",
      "Epoch 8/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.5734 - acc: 0.6987 - val_loss: 0.6608 - val_acc: 0.6276\n",
      "Epoch 9/12\n",
      "896/896 [==============================] - 28s 32ms/step - loss: 0.6320 - acc: 0.6272 - val_loss: 0.6725 - val_acc: 0.5807\n",
      "Epoch 10/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6235 - acc: 0.6596 - val_loss: 0.6425 - val_acc: 0.6094\n",
      "Epoch 11/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6045 - acc: 0.6763 - val_loss: 0.6175 - val_acc: 0.6927\n",
      "320/320 [==============================] - 3s 9ms/step\n",
      "Fitting with:  (1280, 784) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 40s 44ms/step - loss: 0.6887 - acc: 0.5379 - val_loss: 0.6857 - val_acc: 0.5260\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6773 - acc: 0.5815 - val_loss: 0.6727 - val_acc: 0.6172\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6611 - acc: 0.6060 - val_loss: 0.6527 - val_acc: 0.6432\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6349 - acc: 0.6518 - val_loss: 0.6141 - val_acc: 0.6719\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6010 - acc: 0.6931 - val_loss: 0.5845 - val_acc: 0.7031\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6367 - acc: 0.6451 - val_loss: 0.6554 - val_acc: 0.6302\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.5855 - acc: 0.6975 - val_loss: 0.5718 - val_acc: 0.7188\n",
      "Epoch 8/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.5395 - acc: 0.7533 - val_loss: 0.5462 - val_acc: 0.7448\n",
      "Epoch 9/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.5513 - acc: 0.7355 - val_loss: 0.5540 - val_acc: 0.7344\n",
      "Epoch 10/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6095 - acc: 0.6931 - val_loss: 0.8901 - val_acc: 0.5130\n",
      "Epoch 11/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.7468 - acc: 0.5915 - val_loss: 0.5953 - val_acc: 0.7057\n",
      "Epoch 12/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.5670 - acc: 0.7266 - val_loss: 0.6591 - val_acc: 0.6328\n",
      "320/320 [==============================] - 3s 9ms/step\n",
      "Fitting with:  (1280, 784) labels (1280,)\n",
      "Train on 896 samples, validate on 384 samples\n",
      "Epoch 1/12\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.6940 - acc: 0.4967 - val_loss: 0.6854 - val_acc: 0.5781\n",
      "Epoch 2/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6866 - acc: 0.5748 - val_loss: 0.6801 - val_acc: 0.5885\n",
      "Epoch 3/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6776 - acc: 0.6083 - val_loss: 0.6690 - val_acc: 0.6120\n",
      "Epoch 4/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6638 - acc: 0.6529 - val_loss: 0.6434 - val_acc: 0.6432\n",
      "Epoch 5/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.6265 - acc: 0.6607 - val_loss: 0.5905 - val_acc: 0.7031\n",
      "Epoch 6/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.5756 - acc: 0.7143 - val_loss: 0.5656 - val_acc: 0.7240\n",
      "Epoch 7/12\n",
      "896/896 [==============================] - 28s 31ms/step - loss: 0.6165 - acc: 0.6775 - val_loss: 0.8136 - val_acc: 0.5286\n",
      "Epoch 8/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.8327 - acc: 0.5156 - val_loss: 0.8240 - val_acc: 0.5078\n",
      "Epoch 9/12\n",
      "896/896 [==============================] - 27s 30ms/step - loss: 0.7979 - acc: 0.5045 - val_loss: 0.7678 - val_acc: 0.5078\n",
      "Epoch 10/12\n",
      "896/896 [==============================] - 27s 31ms/step - loss: 0.7479 - acc: 0.5067 - val_loss: 0.7331 - val_acc: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 3s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "predictors_sequences = pad_sequences(tokenizer.texts_to_sequences(raw_features))\n",
    "max_len = max([len(x) for x in predictors_sequences])\n",
    "\n",
    "def get_lstm_wv_model():\n",
    "  model = Sequential([\n",
    "        Embedding(corpus_vocab_size, embedding_length, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
    "        LSTM(10),\n",
    "        Dense(1, activation=sigmoid)\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "rnn_wv_scores = run_cross_validate(get_lstm_wv_model, predictors_sequences, labels, cv=5, verbose=1, epochs=12, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  [0.88125, 0.8375, 0.8625, 0.915625, 0.846875]\n",
      "Word vectors:  [0.721875, 0.64375, 0.696875, 0.621875, 0.509375]\n"
     ]
    }
   ],
   "source": [
    "print (\"Bag of words: \", rnn_bow_scores['accuracies'])\n",
    "print (\"Word vectors: \", rnn_wv_scores['accuracies'])\n",
    "\n",
    "rnn_scores_entries =[('Bag of Words', x) for x in rnn_bow_scores['accuracies']] + [('Word Vectors', x) for x in rnn_wv_scores['accuracies']]\n",
    "rnn_scores_data_frame = DataFrame(rnn_scores_entries, columns=['input type', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(x='input type', y='accuracy', data=rnn_scores_data_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still seeing much higher results for BOW, however our pretrained vectors are doing better than the custom ones trained on the OpSpam dataset. This shows us conclusively that word embeddings do have some value, though perhaps not on a dataset as small as this. When we test over our full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
