{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept of different binary text classifiers on our data\n",
    "\n",
    "In the world of ML, there is a vast amount of choice when it comes to which classification method to use.\n",
    "In this notebook, I will be demonstrating how 4 of the most popular classifiers perform:\n",
    "\n",
    "- <b>Multinomial Naive Bayes</b>, the 'punching bag' benchmark classifier of the ML world, using class membership probabilities found by feature vector weights, to predict the membership of a new data point,\n",
    "\n",
    "- <b>Logistic Regression</b>, a method that uses the sigmoid function to transform a representation of how far a new data point lies from a decision boundary found via gradient descent to a class probability,\n",
    "\n",
    "- <b>K-nearest-neighbours</b>, where the classification of X is a vote of the K nearest items to X,\n",
    "\n",
    "- <b>SVM</b>, a method that tries to find a hyperplane to seperate classes by treating them as coordinates in an m dimensional space, m being the number of features.\n",
    "\n",
    "The dataset that we are using in this notebook is very small, containing only 1600 data points, 800 of each class.\n",
    "\n",
    "However, it is a good dataset to use to produce a POC in this notebook, because the feature extraction is fast, it is balanced, and the labels belong to the gold standard. This means we can cross examine multiple classifiers with multiple features in a fast manner to get a feel for how they perform.\n",
    "\n",
    "In terms of producing a reliable model to serve on our API, it is not a good choice, as it does not generalize well.\n",
    "\n",
    "Without further ado, let us begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing some modules for data processing and manipulation, Numpy and Pandas.\n",
    "\n",
    "I also import a helper function to process the raw data into a frame for us, for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from get_df import get_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>deceptive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We stayed at the Schicago Hilton for 4 days an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hotel is located 1/2 mile from the train stati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I made my reservation at the Hilton Chicago be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>When most people think Hilton, they think luxu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>My husband and I recently stayed stayed at the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review deceptive\n",
       "0         0  We stayed at the Schicago Hilton for 4 days an...         1\n",
       "1         0  Hotel is located 1/2 mile from the train stati...         1\n",
       "2         0  I made my reservation at the Hilton Chicago be...         1\n",
       "3         0  When most people think Hilton, they think luxu...         1\n",
       "4         0  My husband and I recently stayed stayed at the...         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data_frame()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. We have 3 columns: \n",
    "- Sentiment (0 is negative, 1 is positive)\n",
    "- Review, our review text,\n",
    "- Deceptive (0 is genuine, 1 is deceptive)\n",
    "\n",
    "Sentiment and Deceptive are pre-labelled for us.\n",
    "We will be focusing on the deceptive column, the label we wish to predict.\n",
    "\n",
    "Let's seperate our data from the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "y = np.asarray(df['sentiment'], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classifiers only work on numeric features, not the strings that our reviews are currently represented by. To represent our reviews as numeric features, we use a Bag of Words model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer takes our review and returns a $m$-dimensional array, where $m$ is our vocabularly size and $m_i$ is 1 if the word $i$ appears in the review, 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the shape of our data after this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9571)\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, 1600 represents the number of reviews in our data, and 9571 the number of words (size of vocab.)\n",
    "Let's see what happens if we remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9284)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "print(cv.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will help our classifier slightly. Stop words generally don't contribute anything to class membership, and add noise.\n",
    "\n",
    "Here's what our first review looks like in count vector format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3266)\t1\n",
      "  (0, 337)\t1\n",
      "  (0, 7482)\t1\n",
      "  (0, 9210)\t1\n",
      "  (0, 1754)\t1\n",
      "  (0, 9023)\t1\n",
      "  (0, 4085)\t1\n",
      "  (0, 3744)\t1\n",
      "  (0, 4770)\t1\n",
      "  (0, 2829)\t1\n",
      "  (0, 6507)\t1\n",
      "  (0, 2535)\t1\n",
      "  (0, 4343)\t1\n",
      "  (0, 5252)\t1\n",
      "  (0, 2172)\t1\n",
      "  (0, 699)\t1\n",
      "  (0, 4902)\t1\n",
      "  (0, 2859)\t1\n",
      "  (0, 8777)\t1\n",
      "  (0, 1514)\t1\n",
      "  (0, 6982)\t1\n",
      "  (0, 5775)\t1\n",
      "  (0, 2603)\t1\n",
      "  (0, 3518)\t1\n",
      "  (0, 4727)\t1\n",
      "  :\t:\n",
      "  (0, 6979)\t2\n",
      "  (0, 731)\t1\n",
      "  (0, 4143)\t4\n",
      "  (0, 7848)\t2\n",
      "  (0, 8566)\t1\n",
      "  (0, 6886)\t1\n",
      "  (0, 9231)\t1\n",
      "  (0, 387)\t1\n",
      "  (0, 8374)\t1\n",
      "  (0, 8174)\t1\n",
      "  (0, 889)\t2\n",
      "  (0, 3154)\t1\n",
      "  (0, 4839)\t1\n",
      "  (0, 1719)\t1\n",
      "  (0, 571)\t1\n",
      "  (0, 3728)\t1\n",
      "  (0, 2849)\t1\n",
      "  (0, 5563)\t1\n",
      "  (0, 7104)\t1\n",
      "  (0, 1943)\t1\n",
      "  (0, 5528)\t1\n",
      "  (0, 2311)\t1\n",
      "  (0, 4050)\t3\n",
      "  (0, 7137)\t1\n",
      "  (0, 7849)\t1\n"
     ]
    }
   ],
   "source": [
    "print((cv.fit_transform(X)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ie. the 4050'th word in the vocab appeared in the first review 3 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply another transformation. \n",
    "\n",
    "$Tf$ is term frequency, and corresponds to how many times a word appears in a review. The higher, more chances are that this particular review is relevant to this word.  \n",
    "\n",
    "$df$ is the number of reviews the word occured in. The higher, the less we should weight the review because of the word. (If a word occurs in a large number of reviews , it wont play important role in finding out reviews relevant to  the word).\n",
    "\n",
    "Therefore, we use the calculation $tf * \\frac {1} {df} $.\n",
    "\n",
    "This is known as $Tf-idf$, and provides a better representation of our data than just word counts. It weights words that are important to a review's classification higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our first count vector and see what it looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9246)\t0.07738232325341368\n",
      "  (0, 9231)\t0.07738232325341368\n",
      "  (0, 9210)\t0.07738232325341368\n",
      "  (0, 9201)\t0.07738232325341368\n",
      "  (0, 9023)\t0.07738232325341368\n",
      "  (0, 9014)\t0.07738232325341368\n",
      "  (0, 8848)\t0.07738232325341368\n",
      "  (0, 8813)\t0.07738232325341368\n",
      "  (0, 8777)\t0.07738232325341368\n",
      "  (0, 8566)\t0.07738232325341368\n",
      "  (0, 8419)\t0.07738232325341368\n",
      "  (0, 8401)\t0.15476464650682736\n",
      "  (0, 8374)\t0.07738232325341368\n",
      "  (0, 8316)\t0.15476464650682736\n",
      "  (0, 8174)\t0.07738232325341368\n",
      "  (0, 8059)\t0.07738232325341368\n",
      "  (0, 7849)\t0.07738232325341368\n",
      "  (0, 7848)\t0.15476464650682736\n",
      "  (0, 7761)\t0.07738232325341368\n",
      "  (0, 7580)\t0.07738232325341368\n",
      "  (0, 7482)\t0.07738232325341368\n",
      "  (0, 7249)\t0.07738232325341368\n",
      "  (0, 7201)\t0.07738232325341368\n",
      "  (0, 7137)\t0.07738232325341368\n",
      "  (0, 7104)\t0.07738232325341368\n",
      "  :\t:\n",
      "  (0, 2311)\t0.07738232325341368\n",
      "  (0, 2172)\t0.07738232325341368\n",
      "  (0, 1943)\t0.07738232325341368\n",
      "  (0, 1879)\t0.07738232325341368\n",
      "  (0, 1754)\t0.07738232325341368\n",
      "  (0, 1721)\t0.15476464650682736\n",
      "  (0, 1719)\t0.07738232325341368\n",
      "  (0, 1717)\t0.07738232325341368\n",
      "  (0, 1712)\t0.07738232325341368\n",
      "  (0, 1711)\t0.15476464650682736\n",
      "  (0, 1564)\t0.07738232325341368\n",
      "  (0, 1514)\t0.07738232325341368\n",
      "  (0, 1474)\t0.15476464650682736\n",
      "  (0, 1448)\t0.07738232325341368\n",
      "  (0, 1410)\t0.07738232325341368\n",
      "  (0, 990)\t0.15476464650682736\n",
      "  (0, 939)\t0.07738232325341368\n",
      "  (0, 889)\t0.15476464650682736\n",
      "  (0, 731)\t0.07738232325341368\n",
      "  (0, 699)\t0.07738232325341368\n",
      "  (0, 658)\t0.07738232325341368\n",
      "  (0, 571)\t0.07738232325341368\n",
      "  (0, 387)\t0.07738232325341368\n",
      "  (0, 386)\t0.07738232325341368\n",
      "  (0, 337)\t0.07738232325341368\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.fit_transform(cv.fit_transform(X)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Classifier Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun stuff! Let's import all of our classifiers listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.svm as svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines allow us to combine feature extractors with classifiers to make life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't much different things we can try with Naive Bayes, however we can see the difference of unigrams to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_unigram = CountVectorizer(stop_words='english', ngram_range = (0, 1))\n",
    "cv_bigram = CountVectorizer(stop_words='english', ngram_range = (0, 2))\n",
    "mnb = MultinomialNB(alpha=2)\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(('Bigram MNB tfidf', Pipeline([ ('cv', cv_bigram), ('tfidf', tfidf), ('mnb', mnb) ])))\n",
    "models.append(('Unigram MNB tfidf', Pipeline([ ('cv', cv_unigram), ('tfidf', tfidf), ('mnb', mnb) ])))\n",
    "\n",
    "\n",
    "CV = 6\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model[0]\n",
    "  accuracies = cross_val_score(model[1], X, y, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our Bigram-trained MNB is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many neighbors we should use for our k-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cv_knn = CountVectorizer(stop_words='english', ngram_range = (0, 1))\n",
    "\n",
    "# Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(5, 22)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "# Split up the data into train and test for this problem\n",
    "X_train = tfidf.fit_transform(cv_knn.fit_transform(X_train))\n",
    "X_test = tfidf.transform(cv_knn.transform(X_test))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate plot\n",
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 seems to be our optimal number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple of different types of SVM with a bunch of different parameters. NuSVC is the classic SVC except you can tweak the number of support vectors. Let's compare the different types, using bigram tf-idf vectors as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8c49af4a002d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SVC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LinearSVC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NuSVC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNuSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup arrays to store models and param sets\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (1, 2))\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(('SVC', Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.SVC(kernel=\"sigmoid\", gamma='scale', random_state=42)) ])))\n",
    "models.append(('LinearSVC', Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.LinearSVC(random_state=42)) ])))\n",
    "models.append(('NuSVC', Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.NuSVC(kernel='sigmoid', gamma='scale', random_state=42)) ])))\n",
    "\n",
    "CV = 6\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model[0]\n",
    "  accuracies = cross_val_score(model[1], X, y, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the LinearSVC is our best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Classifier Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create all of our pipelines, one for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbayes = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('nbayes', MultinomialNB()) ])\n",
    "logreg = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('logreg', LogisticRegression(random_state=42, solver='liblinear')) ])\n",
    "knn = Pipeline([ ('cv', cv_knn), ('tfidf', tfidf), ('knn', KNeighborsClassifier(n_neighbors=18)) ])\n",
    "svc = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.LinearSVC(random_state=42)) ])\n",
    "\n",
    "models = {'Naive Bayes': nbayes, 'Log. Regression':logreg, 'k-NN':knn, 'SVC':svc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting an accurate test/train split, we split our data up using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get our accuracy scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 6\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for name, model in models.items():\n",
    "  model_name = name\n",
    "  accuracies = cross_val_score(model, X, y, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 4 classifier accuracies against each other using the matplotlib library Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the SVM worked best, whereas the k-NN algorithm does relatively poorly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lucas)",
   "language": "python",
   "name": "lucas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
