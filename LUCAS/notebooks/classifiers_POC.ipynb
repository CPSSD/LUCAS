{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept of different binary text classifiers on our data\n",
    "\n",
    "In the world of ML, there is a vast amount of choice when it comes to which classification method to use.\n",
    "In this notebook, I will be demonstrating how 4 of the most popular classifiers perform:\n",
    "\n",
    "- <b>Multinomial Naive Bayes</b>, the 'punching bag' benchmark classifier of the ML world, using class membership probabilities found by feature vector weights, to predict the membership of a new data point,\n",
    "\n",
    "- <b>Logistic Regression</b>, a method that uses the sigmoid function to transform a representation of how far a new data point lies from a decision boundary found via gradient descent to a class probability,\n",
    "\n",
    "- <b>K-nearest-neighbours</b>, where the classification of X is a vote of the K nearest items to X,\n",
    "\n",
    "- <b>SVM</b>, a method that tries to find a hyperplane to seperate classes by treating them as coordinates in an m dimensional space, m being the number of features.\n",
    "\n",
    "The dataset that we are using in this notebook is very small, containing only 1600 data points, 800 of each class.\n",
    "\n",
    "However, it is a good dataset to use to produce a POC in this notebook, because the feature extraction is fast, it is balanced, and the labels belong to the gold standard. This means we can cross examine multiple classifiers with multiple features in a fast manner to get a feel for how they perform.\n",
    "\n",
    "In terms of producing a reliable model to serve on our API, it is not a good choice, as it does not generalize well.\n",
    "\n",
    "Without further ado, let us begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing some modules for data processing and manipulation, Numpy and Pandas.\n",
    "\n",
    "I also import a helper function to process the raw data into a frame for us, for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from get_df import get_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      sentiment                                             review deceptive\n",
       "0            0  We stayed at the Schicago Hilton for 4 days an...         1\n",
       "1            0  Hotel is located 1/2 mile from the train stati...         1\n",
       "2            0  I made my reservation at the Hilton Chicago be...         1\n",
       "3            0  When most people think Hilton, they think luxu...         1\n",
       "4            0  My husband and I recently stayed stayed at the...         1\n",
       "5            0  My wife and I booked a room at the Hilton Chic...         1\n",
       "6            0  For a hotel rated with four diamonds by AAA, o...         1\n",
       "7            0  I had high hopes for the Hilton Chicago, but I...         1\n",
       "8            0  We booked a room at the Hilton Chicago for two...         1\n",
       "9            0  I've stayed at other hotels in Chicago, but th...         1\n",
       "10           0  During my stay at the Hilton Chicago it has be...         1\n",
       "11           0  I stayed two nights at the Hilton Chicago. Tha...         1\n",
       "12           0  The Hilton Chicago, located on prime real esta...         1\n",
       "13           0  I stayed at the Hilton Chicago back in July. F...         1\n",
       "14           0  The Hilton Chicago boasts to be one of the gre...         1\n",
       "15           0  My husband and me reserved a room online for t...         1\n",
       "16           0  I stayed at the Hilton Chicago during a recent...         1\n",
       "17           0  I really wish I could say that I liked this ho...         1\n",
       "18           0  My family stayed at this hotel and walking int...         1\n",
       "19           0  I was very disappointed with this hotel. The f...         1\n",
       "20           0  When I went to The James hotel in Chicago I tr...         1\n",
       "21           0  I recently stayed in The James Hotel in Chicag...         1\n",
       "22           0  I was disappointed in my stay at The James. I ...         1\n",
       "23           0  I was initially excited when I found out that ...         1\n",
       "24           0  James Chicago would be one of the worst hotels...         1\n",
       "25           0  James Chicago; the luxurious nice hotel as it ...         1\n",
       "26           0  DO NOT STAY HERE!! My wife and I were visiting...         1\n",
       "27           0  A recent stay at the James Hotel-Chicago, reve...         1\n",
       "28           0  The hotel was not one of the better ones I sta...         1\n",
       "29           0  I had a business trip coming up in Chicago and...         1\n",
       "...        ...                                                ...       ...\n",
       "1570         1  If you are looking for a hotel that is truly i...         0\n",
       "1571         1  Stayed for 3 nights. booked via hotwire, upgra...         0\n",
       "1572         1  I was attending a training conference in Chica...         0\n",
       "1573         1  Stayed at the InterContinental for an entire w...         0\n",
       "1574         1  I stayed at ICH Chicago for business in Novemb...         0\n",
       "1575         1  After reading some of the most recent reviews ...         0\n",
       "1576         1  Stayed here for two days while attending Lollo...         0\n",
       "1577         1  What a fabulous hotel! From its location direc...         0\n",
       "1578         1  We stayed at the Intercontinental for three ni...         0\n",
       "1579         1  We stayed here through Hotwire and got an amaz...         0\n",
       "1580         1  Stayed here for 5 nights while visiting Chicag...         0\n",
       "1581         1  My wife and I spent several nights here on a g...         0\n",
       "1582         1  Had to bring my new puppy with me on a busines...         0\n",
       "1583         1  Just returned from a week in Chicago with the ...         0\n",
       "1584         1  My wife and I stayed for 2 nights and loved it...         0\n",
       "1585         1  My wife and I redeemed some of my Hilton Rewar...         0\n",
       "1586         1  We stayed at the Palmer House Hilton in early ...         0\n",
       "1587         1  We just got back from a great weekend in Chica...         0\n",
       "1588         1  Guys trip to Chicago, scored this hotel on Pri...         0\n",
       "1589         1  We just returned from a weekend at the Palmer ...         0\n",
       "1590         1  My wife and I decided to spend three days in C...         0\n",
       "1591         1  I booked this hotel for $75 on Hotwire with mi...         0\n",
       "1592         1  We booked this hotel using points on our credi...         0\n",
       "1593         1  I stayed at the Palmer House for @ $150/night ...         0\n",
       "1594         1  Attended a conference at the Palmer House and ...         0\n",
       "1595         1  beautiful place with European charm. No compla...         0\n",
       "1596         1  Spent three nights at the hotel for a girls we...         0\n",
       "1597         1  It has been a couple of years since I stayed h...         0\n",
       "1598         1  Stayed here October 31 through November 5 for ...         0\n",
       "1599         1  We just returned from a girls shopping /sight-...         0\n",
       "\n",
       "[1600 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data_frame()\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. We have 3 columns: \n",
    "- Sentiment (0 is negative, 1 is positive)\n",
    "- Review, our review text,\n",
    "- Deceptive (0 is genuine, 1 is deceptive)\n",
    "\n",
    "Sentiment and Deceptive are pre-labelled for us.\n",
    "We will be focusing on the deceptive column, the label we wish to predict.\n",
    "\n",
    "Let's seperate our data from the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "y = np.asarray(df['deceptive'], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classifiers only work on numeric features, not the strings that our reviews are currently represented by. To represent our reviews as numeric features, we use a Bag of Words model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer takes our review and returns a m-dimensional array, where $m$ is our vocabularly size and $m_i$ is 1 if the word $i$ appears in the review, 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9571)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the shape of our data after this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9571)\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, 1600 represents the number of reviews in our data, and 9571 the number of words (size of vocab.)\n",
    "Let's see what happens if we remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9284)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "print(cv.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will help our classifier slightly. Stop words generally don't contribute anything to class, and add noise.\n",
    "\n",
    "Here's what our first review looks like in count vector format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3266)\t1\n",
      "  (0, 337)\t1\n",
      "  (0, 7482)\t1\n",
      "  (0, 9210)\t1\n",
      "  (0, 1754)\t1\n",
      "  (0, 9023)\t1\n",
      "  (0, 4085)\t1\n",
      "  (0, 3744)\t1\n",
      "  (0, 4770)\t1\n",
      "  (0, 2829)\t1\n",
      "  (0, 6507)\t1\n",
      "  (0, 2535)\t1\n",
      "  (0, 4343)\t1\n",
      "  (0, 5252)\t1\n",
      "  (0, 2172)\t1\n",
      "  (0, 699)\t1\n",
      "  (0, 4902)\t1\n",
      "  (0, 2859)\t1\n",
      "  (0, 8777)\t1\n",
      "  (0, 1514)\t1\n",
      "  (0, 6982)\t1\n",
      "  (0, 5775)\t1\n",
      "  (0, 2603)\t1\n",
      "  (0, 3518)\t1\n",
      "  (0, 4727)\t1\n",
      "  :\t:\n",
      "  (0, 6979)\t2\n",
      "  (0, 731)\t1\n",
      "  (0, 4143)\t4\n",
      "  (0, 7848)\t2\n",
      "  (0, 8566)\t1\n",
      "  (0, 6886)\t1\n",
      "  (0, 9231)\t1\n",
      "  (0, 387)\t1\n",
      "  (0, 8374)\t1\n",
      "  (0, 8174)\t1\n",
      "  (0, 889)\t2\n",
      "  (0, 3154)\t1\n",
      "  (0, 4839)\t1\n",
      "  (0, 1719)\t1\n",
      "  (0, 571)\t1\n",
      "  (0, 3728)\t1\n",
      "  (0, 2849)\t1\n",
      "  (0, 5563)\t1\n",
      "  (0, 7104)\t1\n",
      "  (0, 1943)\t1\n",
      "  (0, 5528)\t1\n",
      "  (0, 2311)\t1\n",
      "  (0, 4050)\t3\n",
      "  (0, 7137)\t1\n",
      "  (0, 7849)\t1\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit_transform(X)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ie. the 4050'th word in the vocab appeared in the first review 3 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply another transformation. \n",
    "\n",
    "$Tf$ is term frequency, and corresponds to how many times a word appears in our vocab. \n",
    "This will weight words in longer reviews greater however, so we discount it by dividing by the number of times it appears in a particular review, with $idf$, inverse document-frequency. \n",
    "Together, this is $Tf-idf$, and provides a better representation of our data than just word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our first count vector and see what it looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9246)\t0.07738232325341368\n",
      "  (0, 9231)\t0.07738232325341368\n",
      "  (0, 9210)\t0.07738232325341368\n",
      "  (0, 9201)\t0.07738232325341368\n",
      "  (0, 9023)\t0.07738232325341368\n",
      "  (0, 9014)\t0.07738232325341368\n",
      "  (0, 8848)\t0.07738232325341368\n",
      "  (0, 8813)\t0.07738232325341368\n",
      "  (0, 8777)\t0.07738232325341368\n",
      "  (0, 8566)\t0.07738232325341368\n",
      "  (0, 8419)\t0.07738232325341368\n",
      "  (0, 8401)\t0.15476464650682736\n",
      "  (0, 8374)\t0.07738232325341368\n",
      "  (0, 8316)\t0.15476464650682736\n",
      "  (0, 8174)\t0.07738232325341368\n",
      "  (0, 8059)\t0.07738232325341368\n",
      "  (0, 7849)\t0.07738232325341368\n",
      "  (0, 7848)\t0.15476464650682736\n",
      "  (0, 7761)\t0.07738232325341368\n",
      "  (0, 7580)\t0.07738232325341368\n",
      "  (0, 7482)\t0.07738232325341368\n",
      "  (0, 7249)\t0.07738232325341368\n",
      "  (0, 7201)\t0.07738232325341368\n",
      "  (0, 7137)\t0.07738232325341368\n",
      "  (0, 7104)\t0.07738232325341368\n",
      "  :\t:\n",
      "  (0, 2311)\t0.07738232325341368\n",
      "  (0, 2172)\t0.07738232325341368\n",
      "  (0, 1943)\t0.07738232325341368\n",
      "  (0, 1879)\t0.07738232325341368\n",
      "  (0, 1754)\t0.07738232325341368\n",
      "  (0, 1721)\t0.15476464650682736\n",
      "  (0, 1719)\t0.07738232325341368\n",
      "  (0, 1717)\t0.07738232325341368\n",
      "  (0, 1712)\t0.07738232325341368\n",
      "  (0, 1711)\t0.15476464650682736\n",
      "  (0, 1564)\t0.07738232325341368\n",
      "  (0, 1514)\t0.07738232325341368\n",
      "  (0, 1474)\t0.15476464650682736\n",
      "  (0, 1448)\t0.07738232325341368\n",
      "  (0, 1410)\t0.07738232325341368\n",
      "  (0, 990)\t0.15476464650682736\n",
      "  (0, 939)\t0.07738232325341368\n",
      "  (0, 889)\t0.15476464650682736\n",
      "  (0, 731)\t0.07738232325341368\n",
      "  (0, 699)\t0.07738232325341368\n",
      "  (0, 658)\t0.07738232325341368\n",
      "  (0, 571)\t0.07738232325341368\n",
      "  (0, 387)\t0.07738232325341368\n",
      "  (0, 386)\t0.07738232325341368\n",
      "  (0, 337)\t0.07738232325341368\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.fit_transform(cv.fit_transform(X)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight of every word now will sum to 1 but is distributed among reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun stuff! Let's import all of our classifiers listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.svm as svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines allow us to combine feature extractors and classifiers to make life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to use the same features for every classifier, we initialize our CountVectorizer and TfIdf transformer beforehand, and pass it into every pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', ngram_range = (0, 2))\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create all of our pipelines, one for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbayes = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('nbayes', MultinomialNB()) ])\n",
    "logreg = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('logreg', LogisticRegression(random_state=42, solver='lbfgs')) ])\n",
    "knn = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('knn', KNeighborsClassifier(n_neighbors=6)) ])\n",
    "svc = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.LinearSVC(random_state=42)) ])\n",
    "\n",
    "models = {'nbayes': nbayes, 'logreg':logreg, 'knn':knn, 'svm':svc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting an accurate test/train split, we split our data up using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get our accuracy scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbayes Accuracy: 0.86 (+/- 0.06)\n",
      "logreg Accuracy: 0.87 (+/- 0.06)\n",
      "knn Accuracy: 0.52 (+/- 0.09)\n",
      "svm Accuracy: 0.88 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=10)\n",
    "    print(\"%s Accuracy: %0.2f (+/- %0.2f)\" % (name, scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, logistic regression and the linear SVM work best, whereas the k-NN algorithm does very poorly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lucas)",
   "language": "python",
   "name": "lucas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
