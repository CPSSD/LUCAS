{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: MLP network with Bag of Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first deep learning experiment following the proof of concept in the parent directory, this notebook will experiment with the most basic form of neural network, which is simply a multi-layer feedforward network (multilayer perceptron) with all nodes connected to every other node. However, in this one we will use a much bigger dataset, because the previous notebook is deceptive(lol) in its accuracy figures because it's such a small, similar dataset.\n",
    "\n",
    "First, let's import and split our data. We'll use 20,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers as th\n",
    "\n",
    "reviews_set, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()\n",
    "reviews = reviews_set[:20000]\n",
    "X = [x.review_content for x in reviews]\n",
    "y = np.array([x.label for x in reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets limit the number of words we use from our reviews, to filter out some of the nonsense. 10,000 is as good a number as any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a function that takes a bunch of reviews and returns them as tfidf count vectors of size 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return tokenizer.texts_to_matrix(data, mode='tfidf')\n",
    "\n",
    "X = np.array(tokenize(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a review to make sure it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.28969329 1.32839609 ... 0.         0.         0.        ]\n",
      " [0.         1.5985416  1.32839609 ... 0.         0.         0.        ]\n",
      " [0.         2.34565261 2.95986798 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         1.98764444 1.64651272 ... 0.         0.         0.        ]\n",
      " [0.         1.5985416  0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.32839609 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial fully connected FF network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train and validate a model to see what our accuracies look like. \n",
    "This time, to help prevent overfitting, we're going to use k-fold cross validation, with k=5, to make sure our model isn't biased to a particular chunk of the dataset.\n",
    "We wont use any regularization methods this time around so we can see how they affect it when we add them in to our layers later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu,),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=6, batch_size=2048, validation_split=0.3, verbose=0)\n",
    "    scores = model.evaluate(X[test], y[test], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train on 11199 samples, validate on 4800 samples\n",
    "Epoch 1/8\n",
    "11199/11199 [==============================] - 7s 656us/step - loss: 0.6903 - acc: 0.5249 - val_loss: 0.6790 - val_acc: 0.5369\n",
    "Epoch 2/8\n",
    "11199/11199 [==============================] - 2s 147us/step - loss: 0.6544 - acc: 0.6086 - val_loss: 0.6632 - val_acc: 0.5929\n",
    "Epoch 3/8\n",
    "11199/11199 [==============================] - 2s 140us/step - loss: 0.6088 - acc: 0.6952 - val_loss: 0.6457 - val_acc: 0.6335\n",
    "Epoch 4/8\n",
    "11199/11199 [==============================] - 2s 143us/step - loss: 0.5596 - acc: 0.7268 - val_loss: 0.6464 - val_acc: 0.6402\n",
    "Epoch 5/8\n",
    "11199/11199 [==============================] - 2s 137us/step - loss: 0.5094 - acc: 0.7613 - val_loss: 0.6517 - val_acc: 0.6490\n",
    " \n",
    " ...\n",
    " \n",
    " Epoch 2/6\n",
    "11200/11200 [==============================] - 2s 150us/step - loss: 0.6523 - acc: 0.5967 - val_loss: 0.6623 - val_acc: 0.5924\n",
    "Epoch 3/6\n",
    "11200/11200 [==============================] - 2s 146us/step - loss: 0.6055 - acc: 0.6954 - val_loss: 0.6474 - val_acc: 0.6299\n",
    "Epoch 4/6\n",
    "11200/11200 [==============================] - 2s 170us/step - loss: 0.5542 - acc: 0.7282 - val_loss: 0.6487 - val_acc: 0.6434\n",
    "Epoch 5/6\n",
    "11200/11200 [==============================] - 2s 145us/step - loss: 0.5034 - acc: 0.7662 - val_loss: 0.6546 - val_acc: 0.6444\n",
    "Epoch 6/6\n",
    "11200/11200 [==============================] - 2s 134us/step - loss: 0.4579 - acc: 0.8015 - val_loss: 0.6631 - val_acc: 0.6417\n",
    "acc: 64.24%\n",
    "64.29% (+/- 0.82%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. With only one hidden layer, 8 nodes, simple word count embeddings as features and no dropout or regularization, we get 64.3% accuracy. Not as good as our POC, but with over 30x the data, definitley a good first step.\n",
    "\n",
    "We can see above that validation loss begins to decrease, but then starts to increase, while training loss and accuracy continues to increase. \n",
    "This indicates that the model is overfitting. It continues to get better and better at fitting the data that it sees (training data) while getting worse and worse at fitting the data that it does not see (validation data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tackling overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's multiple methods of tackling overfitting. We can look at:\n",
    "\n",
    "- Regularization functions (L1, L2)\n",
    "- Dropout layers (Sets random fraction of inputs to 0 during training)\n",
    "- Early stopping callbacks, to stop training when validation loss is not improving\n",
    "\n",
    "We'll try all of these, as well as a couple of different architectures.\n",
    "\n",
    "First, let's define our early stopping callback function, so we don't waste time training with diminishing returns. This will stop training when validation loss doesn't decrease after 3 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our previous model, with early stopping and regularization.\n",
    "\n",
    "There are three types of layer regularization in Keras: kernel, bias, and activity.\n",
    "\n",
    "Kernel: this applies to actual weights of the layer, in Dense it is the W of Wx+b.\n",
    "\n",
    "Bias: this is the bias vector of the weights, so you can apply a different regulariser for it, the b in Wx+b.\n",
    "\n",
    "Activity: is applied to the output vector, the y in y = f(Wx + b).\n",
    "\n",
    "To use regularization to prevent overfitting, we want to apply it to the kernel weights.\n",
    "\n",
    "Let's start with adding L2 regularization with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 63.38%\n",
      "acc: 65.20%\n",
      "acc: 64.05%\n",
      "acc: 65.12%\n",
      "acc: 65.69%\n",
      "64.69% (+/- 0.84%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=30, batch_size=1024, validation_split=0.3, verbose=0, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already a slight improvement, and the models validation loss decreases far better. Let's add a dropout layer to the mix before we change up the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4001/4001 [==============================] - 1s 177us/step\n",
      "acc: 63.06%\n",
      "4000/4000 [==============================] - 1s 169us/step\n",
      "acc: 65.33%\n",
      "4000/4000 [==============================] - 1s 207us/step\n",
      "acc: 64.83%\n",
      "4000/4000 [==============================] - 1s 166us/step\n",
      "acc: 64.78%\n",
      "3999/3999 [==============================] - 1s 199us/step\n",
      "acc: 65.14%\n",
      "64.63% (+/- 0.81%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=6, batch_size=512, validation_split=0.3, verbose=0, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference. Let's change up the number of nodes and see if we can get any increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6667/6667 [==============================] - 1s 196us/step\n",
      "acc: 64.57%\n",
      "6667/6667 [==============================] - 1s 180us/step\n",
      "acc: 65.20%\n",
      "6666/6666 [==============================] - 1s 195us/step\n",
      "acc: 61.54%\n",
      "63.77% (+/- 1.60%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=7, batch_size=2048, validation_split=0.3, verbose=0, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we've hit a plateau with BOW features on this type of neural network. On the next experiment, we'll use a far more complex method of representing words known as word embeddings, which is sure to help us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
