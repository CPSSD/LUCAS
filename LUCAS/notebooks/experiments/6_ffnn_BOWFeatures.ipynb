{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: MLP network with Bag of Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first deep learning experiment following the proof of concept in the parent directory, this notebook will experiment with the most basic form of neural network, which is simply a multi-layer feedforward network (multilayer perceptron) with all nodes connected to every other node. However, in this one we will use a much bigger dataset, because the previous notebook is deceptive(lol) in its accuracy figures because it's such a small, similar dataset.\n",
    "\n",
    "First, let's import and split our data. We'll use 50,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers as th\n",
    "\n",
    "reviews_set, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()\n",
    "reviews = reviews_set[:50000]\n",
    "X = [x.review_content for x in reviews]\n",
    "y = np.array([x.label for x in reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets limit the number of words we use from our reviews, to filter out some of the nonsense. 10,000 is as good a number as any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a function that takes a bunch of reviews and returns them as word count vectors of size 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return tokenizer.texts_to_matrix(data, mode='count')\n",
    "\n",
    "X = np.array(tokenize(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a review to make sure it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial fully connected FF network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train and validate a model to see what our accuracies look like. \n",
    "This time, to help prevent overfitting, we're going to use k-fold cross validation, with k=10, to make sure our model isn't biased to a particular chunk of the dataset.\n",
    "We wont use any regularization methods this time around so we can see how they affect it when we add them in to our layers later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 15s - loss: 0.6676 - acc: 0.5913 - val_loss: 0.6390 - val_acc: 0.6581\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.6079 - acc: 0.6834 - val_loss: 0.6196 - val_acc: 0.6719\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.5725 - acc: 0.7050 - val_loss: 0.6158 - val_acc: 0.6741\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.5457 - acc: 0.7224 - val_loss: 0.6182 - val_acc: 0.6729\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.5223 - acc: 0.7393 - val_loss: 0.6282 - val_acc: 0.6690\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.5005 - acc: 0.7534 - val_loss: 0.6374 - val_acc: 0.6656\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.4796 - acc: 0.7675 - val_loss: 0.6505 - val_acc: 0.6603\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4597 - acc: 0.7818 - val_loss: 0.6662 - val_acc: 0.6584\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4428 - acc: 0.7924 - val_loss: 0.6807 - val_acc: 0.6556\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.4232 - acc: 0.8061 - val_loss: 0.7014 - val_acc: 0.6526\n",
      "acc: 65.49%\n",
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 15s - loss: 0.6731 - acc: 0.5609 - val_loss: 0.6495 - val_acc: 0.6472\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.6209 - acc: 0.6752 - val_loss: 0.6260 - val_acc: 0.6662\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.5858 - acc: 0.7009 - val_loss: 0.6196 - val_acc: 0.6709\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.5603 - acc: 0.7188 - val_loss: 0.6187 - val_acc: 0.6675\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.5380 - acc: 0.7331 - val_loss: 0.6247 - val_acc: 0.6659\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.5196 - acc: 0.7444 - val_loss: 0.6310 - val_acc: 0.6666\n",
      "Epoch 7/10\n",
      " - 9s - loss: 0.5018 - acc: 0.7562 - val_loss: 0.6447 - val_acc: 0.6681\n",
      "Epoch 8/10\n",
      " - 11s - loss: 0.4853 - acc: 0.7675 - val_loss: 0.6499 - val_acc: 0.6610\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.4686 - acc: 0.7781 - val_loss: 0.6619 - val_acc: 0.6607\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.4529 - acc: 0.7870 - val_loss: 0.6749 - val_acc: 0.6555\n",
      "acc: 64.47%\n",
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 0.6672 - acc: 0.5879 - val_loss: 0.6443 - val_acc: 0.6564\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6142 - acc: 0.6800 - val_loss: 0.6219 - val_acc: 0.6690\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.5824 - acc: 0.7007 - val_loss: 0.6147 - val_acc: 0.6724\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5584 - acc: 0.7153 - val_loss: 0.6159 - val_acc: 0.6724\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.5381 - acc: 0.7324 - val_loss: 0.6216 - val_acc: 0.6707\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5188 - acc: 0.7435 - val_loss: 0.6296 - val_acc: 0.6670\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.4991 - acc: 0.7568 - val_loss: 0.6415 - val_acc: 0.6636\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4802 - acc: 0.7695 - val_loss: 0.6525 - val_acc: 0.6583\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4589 - acc: 0.7861 - val_loss: 0.6666 - val_acc: 0.6603\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.4378 - acc: 0.8013 - val_loss: 0.6845 - val_acc: 0.6544\n",
      "acc: 66.13%\n",
      "Train on 31499 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 0.6667 - acc: 0.5872 - val_loss: 0.6419 - val_acc: 0.6537\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.6102 - acc: 0.6810 - val_loss: 0.6213 - val_acc: 0.6674\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.5771 - acc: 0.7021 - val_loss: 0.6157 - val_acc: 0.6702\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.5526 - acc: 0.7187 - val_loss: 0.6180 - val_acc: 0.6665\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.5307 - acc: 0.7333 - val_loss: 0.6256 - val_acc: 0.6637\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5107 - acc: 0.7470 - val_loss: 0.6375 - val_acc: 0.6600\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.4925 - acc: 0.7572 - val_loss: 0.6523 - val_acc: 0.6578\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4761 - acc: 0.7667 - val_loss: 0.6670 - val_acc: 0.6594\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4583 - acc: 0.7789 - val_loss: 0.6870 - val_acc: 0.6504\n",
      "Epoch 10/10\n",
      " - 9s - loss: 0.4441 - acc: 0.7873 - val_loss: 0.7022 - val_acc: 0.6519\n",
      "acc: 65.66%\n",
      "Train on 31499 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 16s - loss: 0.6677 - acc: 0.5824 - val_loss: 0.6429 - val_acc: 0.6544\n",
      "Epoch 2/10\n",
      " - 11s - loss: 0.6130 - acc: 0.6803 - val_loss: 0.6200 - val_acc: 0.6728\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.5811 - acc: 0.7018 - val_loss: 0.6158 - val_acc: 0.6775\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.5594 - acc: 0.7173 - val_loss: 0.6151 - val_acc: 0.6761\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.5403 - acc: 0.7315 - val_loss: 0.6199 - val_acc: 0.6729\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5243 - acc: 0.7402 - val_loss: 0.6256 - val_acc: 0.6705\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.5089 - acc: 0.7502 - val_loss: 0.6348 - val_acc: 0.6665\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4940 - acc: 0.7574 - val_loss: 0.6505 - val_acc: 0.6637\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4783 - acc: 0.7656 - val_loss: 0.6689 - val_acc: 0.6593\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.4633 - acc: 0.7802 - val_loss: 0.6830 - val_acc: 0.6535\n",
      "acc: 64.02%\n",
      "Train on 31499 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 17s - loss: 0.6724 - acc: 0.5738 - val_loss: 0.6450 - val_acc: 0.6539\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.6173 - acc: 0.6780 - val_loss: 0.6217 - val_acc: 0.6668\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.5832 - acc: 0.6997 - val_loss: 0.6146 - val_acc: 0.6701\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.5578 - acc: 0.7174 - val_loss: 0.6172 - val_acc: 0.6688\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.5363 - acc: 0.7329 - val_loss: 0.6231 - val_acc: 0.6652\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5160 - acc: 0.7452 - val_loss: 0.6323 - val_acc: 0.6624\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.4976 - acc: 0.7565 - val_loss: 0.6444 - val_acc: 0.6607\n",
      "Epoch 8/10\n",
      " - 8s - loss: 0.4813 - acc: 0.7663 - val_loss: 0.6595 - val_acc: 0.6578\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.4654 - acc: 0.7779 - val_loss: 0.6792 - val_acc: 0.6557\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.4502 - acc: 0.7869 - val_loss: 0.6915 - val_acc: 0.6536\n",
      "acc: 65.54%\n",
      "Train on 31499 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 15s - loss: 0.6709 - acc: 0.5289 - val_loss: 0.6550 - val_acc: 0.6149\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.6330 - acc: 0.6673 - val_loss: 0.6360 - val_acc: 0.6628\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.6020 - acc: 0.6992 - val_loss: 0.6242 - val_acc: 0.6657\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.5753 - acc: 0.7173 - val_loss: 0.6219 - val_acc: 0.6639\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.5519 - acc: 0.7329 - val_loss: 0.6214 - val_acc: 0.6679\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5311 - acc: 0.7466 - val_loss: 0.6268 - val_acc: 0.6672\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.5105 - acc: 0.7595 - val_loss: 0.6371 - val_acc: 0.6645\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4918 - acc: 0.7693 - val_loss: 0.6490 - val_acc: 0.6630\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4711 - acc: 0.7822 - val_loss: 0.6592 - val_acc: 0.6603\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.4514 - acc: 0.7943 - val_loss: 0.6764 - val_acc: 0.6568\n",
      "acc: 65.28%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 15s - loss: 0.6787 - acc: 0.5464 - val_loss: 0.6603 - val_acc: 0.6394\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.6372 - acc: 0.6662 - val_loss: 0.6339 - val_acc: 0.6581\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.6039 - acc: 0.6912 - val_loss: 0.6209 - val_acc: 0.6683\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5784 - acc: 0.7047 - val_loss: 0.6152 - val_acc: 0.6733\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.5544 - acc: 0.7221 - val_loss: 0.6166 - val_acc: 0.6725\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5333 - acc: 0.7360 - val_loss: 0.6231 - val_acc: 0.6691\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.5133 - acc: 0.7483 - val_loss: 0.6305 - val_acc: 0.6652\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4948 - acc: 0.7600 - val_loss: 0.6431 - val_acc: 0.6642\n",
      "Epoch 9/10\n",
      " - 9s - loss: 0.4760 - acc: 0.7736 - val_loss: 0.6567 - val_acc: 0.6601\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.4572 - acc: 0.7829 - val_loss: 0.6719 - val_acc: 0.6575\n",
      "acc: 65.79%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 0.6782 - acc: 0.5577 - val_loss: 0.6563 - val_acc: 0.6455\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.6301 - acc: 0.6742 - val_loss: 0.6251 - val_acc: 0.6654\n",
      "Epoch 3/10\n",
      " - 11s - loss: 0.5936 - acc: 0.6933 - val_loss: 0.6141 - val_acc: 0.6734\n",
      "Epoch 4/10\n",
      " - 14s - loss: 0.5680 - acc: 0.7123 - val_loss: 0.6158 - val_acc: 0.6680\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.5443 - acc: 0.7289 - val_loss: 0.6168 - val_acc: 0.6717\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5233 - acc: 0.7428 - val_loss: 0.6236 - val_acc: 0.6708\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.5024 - acc: 0.7576 - val_loss: 0.6348 - val_acc: 0.6668\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.4837 - acc: 0.7698 - val_loss: 0.6475 - val_acc: 0.6626\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.4645 - acc: 0.7820 - val_loss: 0.6636 - val_acc: 0.6598\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.4443 - acc: 0.7950 - val_loss: 0.6801 - val_acc: 0.6580\n",
      "acc: 64.99%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 0.6824 - acc: 0.5541 - val_loss: 0.6628 - val_acc: 0.6387\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 7s - loss: 0.6293 - acc: 0.6724 - val_loss: 0.6247 - val_acc: 0.6648\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.5866 - acc: 0.6990 - val_loss: 0.6156 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5570 - acc: 0.7197 - val_loss: 0.6154 - val_acc: 0.6685\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.5304 - acc: 0.7378 - val_loss: 0.6211 - val_acc: 0.6682\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5052 - acc: 0.7539 - val_loss: 0.6304 - val_acc: 0.6646\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.4791 - acc: 0.7718 - val_loss: 0.6457 - val_acc: 0.6608\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.4525 - acc: 0.7890 - val_loss: 0.6603 - val_acc: 0.6577\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4252 - acc: 0.8079 - val_loss: 0.6836 - val_acc: 0.6544\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.3997 - acc: 0.8249 - val_loss: 0.7010 - val_acc: 0.6536\n",
      "acc: 63.21%\n",
      "65.06% (+/- 0.86%)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu,),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=10, batch_size=2048, validation_split=0.3, verbose=1)\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. With only one hidden layer, 8 nodes, simple word count embeddings as features and no dropout or regularization, we get 65% accuracy. Not as good as our POC, but with over 30x the data, definitley a good first step.\n",
    "\n",
    "We can see above that validation loss begins to decrease, but then starts to increase, while training loss and accuracy continues to increase. \n",
    "This indicates that the model is overfitting. It continues to get better and better at fitting the data that it sees (training data) while getting worse and worse at fitting the data that it does not see (validation data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tackling overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's multiple methods of tackling overfitting. We can look at:\n",
    "\n",
    "- Regularization functions (L1, L2)\n",
    "- Dropout layers (Sets random fraction of inputs to 0 during training)\n",
    "- Early stopping callbacks, to stop training when validation loss is not improving\n",
    "\n",
    "We'll try all of these, as well as a couple of different architectures.\n",
    "\n",
    "First, let's define our early stopping callback function, so we don't waste time training with diminishing returns. This will stop training when validation loss doesn't decrease after 3 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our previous model, with early stopping and regularization.\n",
    "\n",
    "There are three types of layer regularization in Keras: kernel, bias, and activity.\n",
    "\n",
    "Kernel: this applies to actual weights of the layer, in Dense it is the W of Wx+b.\n",
    "\n",
    "Bias: this is the bias vector of the weights, so you can apply a different regulariser for it, the b in Wx+b.\n",
    "\n",
    "Activity: is applied to the output vector, the y in y = f(Wx + b).\n",
    "\n",
    "To use regularization to prevent overfitting, we want to apply it to the kernel weights.\n",
    "\n",
    "Let's start with adding L2 regularization with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27999 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "27999/27999 [==============================] - 12s 433us/step - loss: 0.9126 - acc: 0.5849 - val_loss: 0.7914 - val_acc: 0.6602\n",
      "Epoch 2/30\n",
      "27999/27999 [==============================] - 3s 109us/step - loss: 0.7609 - acc: 0.6698 - val_loss: 0.7547 - val_acc: 0.6668\n",
      "Epoch 3/30\n",
      "27999/27999 [==============================] - 3s 102us/step - loss: 0.7272 - acc: 0.6794 - val_loss: 0.7363 - val_acc: 0.6658\n",
      "Epoch 4/30\n",
      "27999/27999 [==============================] - 3s 98us/step - loss: 0.7073 - acc: 0.6826 - val_loss: 0.7212 - val_acc: 0.6683\n",
      "Epoch 5/30\n",
      "27999/27999 [==============================] - 3s 113us/step - loss: 0.6915 - acc: 0.6847 - val_loss: 0.7083 - val_acc: 0.6652\n",
      "Epoch 6/30\n",
      "27999/27999 [==============================] - 3s 106us/step - loss: 0.6801 - acc: 0.6833 - val_loss: 0.6997 - val_acc: 0.6643\n",
      "Epoch 7/30\n",
      "27999/27999 [==============================] - 3s 102us/step - loss: 0.6691 - acc: 0.6863 - val_loss: 0.6903 - val_acc: 0.6666\n",
      "Epoch 8/30\n",
      "27999/27999 [==============================] - 3s 108us/step - loss: 0.6610 - acc: 0.6849 - val_loss: 0.6825 - val_acc: 0.6653\n",
      "Epoch 9/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.6554 - acc: 0.6874 - val_loss: 0.6835 - val_acc: 0.6566\n",
      "Epoch 10/30\n",
      "27999/27999 [==============================] - 3s 121us/step - loss: 0.6467 - acc: 0.6884 - val_loss: 0.6721 - val_acc: 0.6673\n",
      "Epoch 11/30\n",
      "27999/27999 [==============================] - 3s 106us/step - loss: 0.6423 - acc: 0.6872 - val_loss: 0.6692 - val_acc: 0.6668\n",
      "Epoch 12/30\n",
      "27999/27999 [==============================] - 3s 115us/step - loss: 0.6375 - acc: 0.6905 - val_loss: 0.6661 - val_acc: 0.6678\n",
      "Epoch 13/30\n",
      "27999/27999 [==============================] - 3s 96us/step - loss: 0.6343 - acc: 0.6911 - val_loss: 0.6645 - val_acc: 0.6683\n",
      "Epoch 14/30\n",
      "27999/27999 [==============================] - 3s 97us/step - loss: 0.6320 - acc: 0.6906 - val_loss: 0.6621 - val_acc: 0.6668\n",
      "Epoch 15/30\n",
      "27999/27999 [==============================] - 3s 113us/step - loss: 0.6289 - acc: 0.6920 - val_loss: 0.6613 - val_acc: 0.6658\n",
      "Epoch 16/30\n",
      "27999/27999 [==============================] - 3s 110us/step - loss: 0.6268 - acc: 0.6941 - val_loss: 0.6594 - val_acc: 0.6669\n",
      "Epoch 17/30\n",
      "27999/27999 [==============================] - 3s 113us/step - loss: 0.6254 - acc: 0.6952 - val_loss: 0.6583 - val_acc: 0.6676\n",
      "Epoch 18/30\n",
      "27999/27999 [==============================] - 3s 103us/step - loss: 0.6228 - acc: 0.6947 - val_loss: 0.6584 - val_acc: 0.6678\n",
      "Epoch 19/30\n",
      "27999/27999 [==============================] - 3s 103us/step - loss: 0.6220 - acc: 0.6976 - val_loss: 0.6582 - val_acc: 0.6676\n",
      "Epoch 20/30\n",
      "27999/27999 [==============================] - 3s 103us/step - loss: 0.6207 - acc: 0.6978 - val_loss: 0.6579 - val_acc: 0.6675\n",
      "Epoch 21/30\n",
      "27999/27999 [==============================] - 4s 143us/step - loss: 0.6193 - acc: 0.6989 - val_loss: 0.6586 - val_acc: 0.6669\n",
      "Epoch 22/30\n",
      "27999/27999 [==============================] - 3s 114us/step - loss: 0.6185 - acc: 0.6975 - val_loss: 0.6581 - val_acc: 0.6662\n",
      "Epoch 23/30\n",
      "27999/27999 [==============================] - 3s 111us/step - loss: 0.6179 - acc: 0.6993 - val_loss: 0.6571 - val_acc: 0.6659\n",
      "Epoch 24/30\n",
      "27999/27999 [==============================] - 4s 143us/step - loss: 0.6143 - acc: 0.7030 - val_loss: 0.6563 - val_acc: 0.6664\n",
      "Epoch 25/30\n",
      "27999/27999 [==============================] - 4s 132us/step - loss: 0.6166 - acc: 0.7003 - val_loss: 0.6589 - val_acc: 0.6663\n",
      "Epoch 26/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.6129 - acc: 0.7048 - val_loss: 0.6576 - val_acc: 0.6667\n",
      "Epoch 27/30\n",
      "27999/27999 [==============================] - 3s 104us/step - loss: 0.6120 - acc: 0.7051 - val_loss: 0.6593 - val_acc: 0.6665\n",
      "acc: 66.66%\n",
      "Train on 27999 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "27999/27999 [==============================] - 12s 432us/step - loss: 0.8986 - acc: 0.5929 - val_loss: 0.7759 - val_acc: 0.6584\n",
      "Epoch 2/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.7498 - acc: 0.6728 - val_loss: 0.7437 - val_acc: 0.6667\n",
      "Epoch 3/30\n",
      "27999/27999 [==============================] - 3s 115us/step - loss: 0.7203 - acc: 0.6799 - val_loss: 0.7262 - val_acc: 0.6630\n",
      "Epoch 4/30\n",
      "27999/27999 [==============================] - 3s 108us/step - loss: 0.7033 - acc: 0.6820 - val_loss: 0.7125 - val_acc: 0.6640\n",
      "Epoch 5/30\n",
      "27999/27999 [==============================] - 3s 110us/step - loss: 0.6887 - acc: 0.6828 - val_loss: 0.7014 - val_acc: 0.6633\n",
      "Epoch 6/30\n",
      "27999/27999 [==============================] - 3s 108us/step - loss: 0.6782 - acc: 0.6820 - val_loss: 0.6985 - val_acc: 0.6604\n",
      "Epoch 7/30\n",
      "27999/27999 [==============================] - 3s 110us/step - loss: 0.6704 - acc: 0.6815 - val_loss: 0.6849 - val_acc: 0.6617\n",
      "Epoch 8/30\n",
      "27999/27999 [==============================] - 3s 117us/step - loss: 0.6599 - acc: 0.6838 - val_loss: 0.6790 - val_acc: 0.6648\n",
      "Epoch 9/30\n",
      "27999/27999 [==============================] - 4s 129us/step - loss: 0.6546 - acc: 0.6851 - val_loss: 0.6736 - val_acc: 0.6623\n",
      "Epoch 10/30\n",
      "27999/27999 [==============================] - 3s 117us/step - loss: 0.6486 - acc: 0.6840 - val_loss: 0.6701 - val_acc: 0.6622\n",
      "Epoch 11/30\n",
      "27999/27999 [==============================] - 3s 117us/step - loss: 0.6439 - acc: 0.6852 - val_loss: 0.6672 - val_acc: 0.6645\n",
      "Epoch 12/30\n",
      "27999/27999 [==============================] - 4s 151us/step - loss: 0.6413 - acc: 0.6853 - val_loss: 0.6620 - val_acc: 0.6642\n",
      "Epoch 13/30\n",
      "27999/27999 [==============================] - 5s 173us/step - loss: 0.6363 - acc: 0.6856 - val_loss: 0.6592 - val_acc: 0.6633\n",
      "Epoch 14/30\n",
      "27999/27999 [==============================] - 4s 160us/step - loss: 0.6340 - acc: 0.6847 - val_loss: 0.6603 - val_acc: 0.6632\n",
      "Epoch 15/30\n",
      "27999/27999 [==============================] - 4s 145us/step - loss: 0.6332 - acc: 0.6850 - val_loss: 0.6563 - val_acc: 0.6649\n",
      "Epoch 16/30\n",
      "27999/27999 [==============================] - 4s 156us/step - loss: 0.6279 - acc: 0.6905 - val_loss: 0.6548 - val_acc: 0.6643\n",
      "Epoch 17/30\n",
      "27999/27999 [==============================] - 5s 186us/step - loss: 0.6260 - acc: 0.6895 - val_loss: 0.6544 - val_acc: 0.6650\n",
      "Epoch 18/30\n",
      "27999/27999 [==============================] - 4s 143us/step - loss: 0.6245 - acc: 0.6891 - val_loss: 0.6538 - val_acc: 0.6655\n",
      "Epoch 19/30\n",
      "27999/27999 [==============================] - 3s 121us/step - loss: 0.6236 - acc: 0.6914 - val_loss: 0.6555 - val_acc: 0.6602\n",
      "Epoch 20/30\n",
      "27999/27999 [==============================] - 3s 117us/step - loss: 0.6212 - acc: 0.6922 - val_loss: 0.6517 - val_acc: 0.6631\n",
      "Epoch 21/30\n",
      "27999/27999 [==============================] - 3s 115us/step - loss: 0.6204 - acc: 0.6931 - val_loss: 0.6550 - val_acc: 0.6638\n",
      "Epoch 22/30\n",
      "27999/27999 [==============================] - 5s 161us/step - loss: 0.6206 - acc: 0.6935 - val_loss: 0.6506 - val_acc: 0.6637\n",
      "Epoch 23/30\n",
      "27999/27999 [==============================] - 4s 153us/step - loss: 0.6186 - acc: 0.6935 - val_loss: 0.6501 - val_acc: 0.6667\n",
      "Epoch 24/30\n",
      "27999/27999 [==============================] - 3s 118us/step - loss: 0.6169 - acc: 0.6956 - val_loss: 0.6508 - val_acc: 0.6634\n",
      "Epoch 25/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.6189 - acc: 0.6950 - val_loss: 0.6522 - val_acc: 0.6657\n",
      "Epoch 26/30\n",
      "27999/27999 [==============================] - 3s 106us/step - loss: 0.6156 - acc: 0.6962 - val_loss: 0.6504 - val_acc: 0.6640\n",
      "acc: 68.02%\n",
      "Train on 28000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "28000/28000 [==============================] - 15s 527us/step - loss: 0.9242 - acc: 0.5944 - val_loss: 0.8009 - val_acc: 0.6550\n",
      "Epoch 2/30\n",
      "28000/28000 [==============================] - 4s 133us/step - loss: 0.7673 - acc: 0.6759 - val_loss: 0.7603 - val_acc: 0.6703\n",
      "Epoch 3/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.7307 - acc: 0.6883 - val_loss: 0.7406 - val_acc: 0.6718\n",
      "Epoch 4/30\n",
      "28000/28000 [==============================] - 4s 133us/step - loss: 0.7088 - acc: 0.6910 - val_loss: 0.7294 - val_acc: 0.6672\n",
      "Epoch 5/30\n",
      "28000/28000 [==============================] - 4s 136us/step - loss: 0.6928 - acc: 0.6926 - val_loss: 0.7144 - val_acc: 0.6713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "28000/28000 [==============================] - 6s 217us/step - loss: 0.6786 - acc: 0.6944 - val_loss: 0.7041 - val_acc: 0.6679\n",
      "Epoch 7/30\n",
      "28000/28000 [==============================] - 6s 220us/step - loss: 0.6687 - acc: 0.6948 - val_loss: 0.6965 - val_acc: 0.6673\n",
      "Epoch 8/30\n",
      "28000/28000 [==============================] - 5s 180us/step - loss: 0.6583 - acc: 0.6971 - val_loss: 0.6910 - val_acc: 0.6665\n",
      "Epoch 9/30\n",
      "28000/28000 [==============================] - 4s 153us/step - loss: 0.6528 - acc: 0.6972 - val_loss: 0.6905 - val_acc: 0.6582\n",
      "Epoch 10/30\n",
      "28000/28000 [==============================] - 4s 143us/step - loss: 0.6462 - acc: 0.6986 - val_loss: 0.6789 - val_acc: 0.6673\n",
      "Epoch 11/30\n",
      "28000/28000 [==============================] - 5s 164us/step - loss: 0.6394 - acc: 0.6982 - val_loss: 0.6745 - val_acc: 0.6670\n",
      "Epoch 12/30\n",
      "28000/28000 [==============================] - 5s 163us/step - loss: 0.6354 - acc: 0.6985 - val_loss: 0.6711 - val_acc: 0.6672\n",
      "Epoch 13/30\n",
      "28000/28000 [==============================] - 3s 122us/step - loss: 0.6314 - acc: 0.6996 - val_loss: 0.6693 - val_acc: 0.6648\n",
      "Epoch 14/30\n",
      "28000/28000 [==============================] - 3s 99us/step - loss: 0.6274 - acc: 0.7006 - val_loss: 0.6668 - val_acc: 0.6659\n",
      "Epoch 15/30\n",
      "28000/28000 [==============================] - 3s 106us/step - loss: 0.6247 - acc: 0.7032 - val_loss: 0.6646 - val_acc: 0.6700\n",
      "Epoch 16/30\n",
      "28000/28000 [==============================] - 3s 112us/step - loss: 0.6214 - acc: 0.7019 - val_loss: 0.6639 - val_acc: 0.6677\n",
      "Epoch 17/30\n",
      "28000/28000 [==============================] - 3s 110us/step - loss: 0.6208 - acc: 0.7023 - val_loss: 0.6618 - val_acc: 0.6673\n",
      "Epoch 18/30\n",
      "28000/28000 [==============================] - 3s 103us/step - loss: 0.6168 - acc: 0.7066 - val_loss: 0.6620 - val_acc: 0.6674\n",
      "Epoch 19/30\n",
      "28000/28000 [==============================] - 3s 106us/step - loss: 0.6153 - acc: 0.7070 - val_loss: 0.6632 - val_acc: 0.6685\n",
      "Epoch 20/30\n",
      "28000/28000 [==============================] - 3s 109us/step - loss: 0.6132 - acc: 0.7090 - val_loss: 0.6620 - val_acc: 0.6688\n",
      "acc: 66.38%\n",
      "Train on 28000 samples, validate on 12001 samples\n",
      "Epoch 1/30\n",
      "28000/28000 [==============================] - 11s 404us/step - loss: 0.9044 - acc: 0.6070 - val_loss: 0.7853 - val_acc: 0.6554\n",
      "Epoch 2/30\n",
      "28000/28000 [==============================] - 3s 99us/step - loss: 0.7565 - acc: 0.6737 - val_loss: 0.7499 - val_acc: 0.6645\n",
      "Epoch 3/30\n",
      "28000/28000 [==============================] - 3s 105us/step - loss: 0.7240 - acc: 0.6836 - val_loss: 0.7300 - val_acc: 0.6637\n",
      "Epoch 4/30\n",
      "28000/28000 [==============================] - 3s 107us/step - loss: 0.7045 - acc: 0.6884 - val_loss: 0.7158 - val_acc: 0.6641\n",
      "Epoch 5/30\n",
      "28000/28000 [==============================] - 3s 110us/step - loss: 0.6895 - acc: 0.6849 - val_loss: 0.7032 - val_acc: 0.6650\n",
      "Epoch 6/30\n",
      "28000/28000 [==============================] - 3s 107us/step - loss: 0.6760 - acc: 0.6882 - val_loss: 0.6947 - val_acc: 0.6639\n",
      "Epoch 7/30\n",
      "28000/28000 [==============================] - 3s 113us/step - loss: 0.6672 - acc: 0.6878 - val_loss: 0.6862 - val_acc: 0.6637\n",
      "Epoch 8/30\n",
      "28000/28000 [==============================] - 3s 100us/step - loss: 0.6585 - acc: 0.6883 - val_loss: 0.6813 - val_acc: 0.6621\n",
      "Epoch 9/30\n",
      "28000/28000 [==============================] - 3s 106us/step - loss: 0.6506 - acc: 0.6924 - val_loss: 0.6733 - val_acc: 0.6650\n",
      "Epoch 10/30\n",
      "28000/28000 [==============================] - 4s 128us/step - loss: 0.6445 - acc: 0.6912 - val_loss: 0.6693 - val_acc: 0.6634\n",
      "Epoch 11/30\n",
      "28000/28000 [==============================] - 4s 142us/step - loss: 0.6392 - acc: 0.6927 - val_loss: 0.6673 - val_acc: 0.6633\n",
      "Epoch 12/30\n",
      "28000/28000 [==============================] - 5s 167us/step - loss: 0.6369 - acc: 0.6901 - val_loss: 0.6653 - val_acc: 0.6616\n",
      "Epoch 13/30\n",
      "28000/28000 [==============================] - 4s 127us/step - loss: 0.6334 - acc: 0.6923 - val_loss: 0.6650 - val_acc: 0.6624\n",
      "Epoch 14/30\n",
      "28000/28000 [==============================] - 3s 109us/step - loss: 0.6302 - acc: 0.6932 - val_loss: 0.6588 - val_acc: 0.6636\n",
      "Epoch 15/30\n",
      "28000/28000 [==============================] - 3s 107us/step - loss: 0.6280 - acc: 0.6925 - val_loss: 0.6604 - val_acc: 0.6626\n",
      "Epoch 16/30\n",
      "28000/28000 [==============================] - 3s 112us/step - loss: 0.6249 - acc: 0.6961 - val_loss: 0.6554 - val_acc: 0.6658\n",
      "Epoch 17/30\n",
      "28000/28000 [==============================] - 4s 130us/step - loss: 0.6220 - acc: 0.6953 - val_loss: 0.6552 - val_acc: 0.6642\n",
      "Epoch 18/30\n",
      "28000/28000 [==============================] - 4s 131us/step - loss: 0.6217 - acc: 0.6954 - val_loss: 0.6537 - val_acc: 0.6656\n",
      "Epoch 19/30\n",
      "28000/28000 [==============================] - 4s 129us/step - loss: 0.6210 - acc: 0.6963 - val_loss: 0.6577 - val_acc: 0.6650\n",
      "Epoch 20/30\n",
      "28000/28000 [==============================] - 3s 118us/step - loss: 0.6181 - acc: 0.6970 - val_loss: 0.6543 - val_acc: 0.6633\n",
      "Epoch 21/30\n",
      "28000/28000 [==============================] - 4s 126us/step - loss: 0.6171 - acc: 0.6985 - val_loss: 0.6556 - val_acc: 0.6608\n",
      "acc: 66.77%\n",
      "Train on 28000 samples, validate on 12001 samples\n",
      "Epoch 1/30\n",
      "28000/28000 [==============================] - 14s 517us/step - loss: 0.9204 - acc: 0.5899 - val_loss: 0.7923 - val_acc: 0.6519\n",
      "Epoch 2/30\n",
      "28000/28000 [==============================] - 8s 290us/step - loss: 0.7637 - acc: 0.6711 - val_loss: 0.7531 - val_acc: 0.6629\n",
      "Epoch 3/30\n",
      "28000/28000 [==============================] - 5s 183us/step - loss: 0.7273 - acc: 0.6844 - val_loss: 0.7339 - val_acc: 0.6614\n",
      "Epoch 4/30\n",
      "28000/28000 [==============================] - 4s 150us/step - loss: 0.7071 - acc: 0.6861 - val_loss: 0.7184 - val_acc: 0.6619\n",
      "Epoch 5/30\n",
      "28000/28000 [==============================] - 4s 126us/step - loss: 0.6928 - acc: 0.6881 - val_loss: 0.7064 - val_acc: 0.6641\n",
      "Epoch 6/30\n",
      "28000/28000 [==============================] - 3s 124us/step - loss: 0.6804 - acc: 0.6878 - val_loss: 0.6970 - val_acc: 0.6636\n",
      "Epoch 7/30\n",
      "28000/28000 [==============================] - 3s 107us/step - loss: 0.6701 - acc: 0.6905 - val_loss: 0.6905 - val_acc: 0.6635\n",
      "Epoch 8/30\n",
      "28000/28000 [==============================] - 3s 119us/step - loss: 0.6616 - acc: 0.6905 - val_loss: 0.6880 - val_acc: 0.6579\n",
      "Epoch 9/30\n",
      "28000/28000 [==============================] - 6s 207us/step - loss: 0.6552 - acc: 0.6915 - val_loss: 0.6784 - val_acc: 0.6634\n",
      "Epoch 10/30\n",
      "28000/28000 [==============================] - 5s 180us/step - loss: 0.6481 - acc: 0.6912 - val_loss: 0.6732 - val_acc: 0.6624\n",
      "Epoch 11/30\n",
      "28000/28000 [==============================] - 5s 179us/step - loss: 0.6423 - acc: 0.6934 - val_loss: 0.6670 - val_acc: 0.6669\n",
      "Epoch 12/30\n",
      "28000/28000 [==============================] - 4s 127us/step - loss: 0.6379 - acc: 0.6949 - val_loss: 0.6650 - val_acc: 0.6659\n",
      "Epoch 13/30\n",
      "28000/28000 [==============================] - 4s 127us/step - loss: 0.6347 - acc: 0.6943 - val_loss: 0.6622 - val_acc: 0.6649\n",
      "Epoch 14/30\n",
      "28000/28000 [==============================] - 3s 122us/step - loss: 0.6313 - acc: 0.6954 - val_loss: 0.6634 - val_acc: 0.6647\n",
      "Epoch 15/30\n",
      "28000/28000 [==============================] - 3s 116us/step - loss: 0.6283 - acc: 0.6967 - val_loss: 0.6588 - val_acc: 0.6652\n",
      "Epoch 16/30\n",
      "28000/28000 [==============================] - 3s 97us/step - loss: 0.6256 - acc: 0.6980 - val_loss: 0.6580 - val_acc: 0.6639\n",
      "Epoch 17/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.6245 - acc: 0.6971 - val_loss: 0.6560 - val_acc: 0.6664\n",
      "Epoch 18/30\n",
      "28000/28000 [==============================] - 6s 217us/step - loss: 0.6224 - acc: 0.7000 - val_loss: 0.6557 - val_acc: 0.6644\n",
      "Epoch 19/30\n",
      "28000/28000 [==============================] - 6s 222us/step - loss: 0.6197 - acc: 0.6998 - val_loss: 0.6557 - val_acc: 0.6663\n",
      "Epoch 20/30\n",
      "28000/28000 [==============================] - 5s 190us/step - loss: 0.6177 - acc: 0.7022 - val_loss: 0.6547 - val_acc: 0.6649\n",
      "Epoch 21/30\n",
      "28000/28000 [==============================] - 7s 238us/step - loss: 0.6157 - acc: 0.7018 - val_loss: 0.6551 - val_acc: 0.6669\n",
      "Epoch 22/30\n",
      "28000/28000 [==============================] - 6s 212us/step - loss: 0.6140 - acc: 0.7043 - val_loss: 0.6571 - val_acc: 0.6643\n",
      "Epoch 23/30\n",
      "28000/28000 [==============================] - 6s 215us/step - loss: 0.6138 - acc: 0.7051 - val_loss: 0.6553 - val_acc: 0.6666\n",
      "acc: 66.29%\n",
      "66.82% (+/- 0.62%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=30, batch_size=1024, validation_split=0.3, verbose=0, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already a slight improvement, and the models validation loss decreases far better. Let's add a dropout layer to the mix before adding more dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27999 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "27999/27999 [==============================] - 15s 546us/step - loss: 0.9434 - acc: 0.5746 - val_loss: 0.8203 - val_acc: 0.6516\n",
      "Epoch 2/30\n",
      "27999/27999 [==============================] - 5s 175us/step - loss: 0.7986 - acc: 0.6503 - val_loss: 0.7770 - val_acc: 0.6642\n",
      "Epoch 3/30\n",
      "27999/27999 [==============================] - 4s 147us/step - loss: 0.7621 - acc: 0.6633 - val_loss: 0.7518 - val_acc: 0.6644\n",
      "Epoch 4/30\n",
      "27999/27999 [==============================] - 4s 137us/step - loss: 0.7363 - acc: 0.6669 - val_loss: 0.7339 - val_acc: 0.6656\n",
      "Epoch 5/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.7166 - acc: 0.6706 - val_loss: 0.7178 - val_acc: 0.6625\n",
      "Epoch 6/30\n",
      "27999/27999 [==============================] - 3s 111us/step - loss: 0.7008 - acc: 0.6743 - val_loss: 0.7043 - val_acc: 0.6682\n",
      "Epoch 7/30\n",
      "27999/27999 [==============================] - 3s 113us/step - loss: 0.6869 - acc: 0.6769 - val_loss: 0.6950 - val_acc: 0.6649\n",
      "Epoch 8/30\n",
      "27999/27999 [==============================] - 3s 120us/step - loss: 0.6759 - acc: 0.6813 - val_loss: 0.6859 - val_acc: 0.6662\n",
      "Epoch 9/30\n",
      "27999/27999 [==============================] - 3s 115us/step - loss: 0.6662 - acc: 0.6812 - val_loss: 0.6804 - val_acc: 0.6651\n",
      "Epoch 10/30\n",
      "27999/27999 [==============================] - 3s 110us/step - loss: 0.6613 - acc: 0.6772 - val_loss: 0.6737 - val_acc: 0.6644\n",
      "Epoch 11/30\n",
      "27999/27999 [==============================] - 3s 111us/step - loss: 0.6533 - acc: 0.6848 - val_loss: 0.6683 - val_acc: 0.6665\n",
      "Epoch 12/30\n",
      "27999/27999 [==============================] - 3s 107us/step - loss: 0.6465 - acc: 0.6860 - val_loss: 0.6647 - val_acc: 0.6688\n",
      "Epoch 13/30\n",
      "27999/27999 [==============================] - 3s 107us/step - loss: 0.6444 - acc: 0.6871 - val_loss: 0.6635 - val_acc: 0.6674\n",
      "Epoch 14/30\n",
      "27999/27999 [==============================] - 3s 106us/step - loss: 0.6385 - acc: 0.6875 - val_loss: 0.6613 - val_acc: 0.6684\n",
      "Epoch 15/30\n",
      "27999/27999 [==============================] - 3s 108us/step - loss: 0.6354 - acc: 0.6882 - val_loss: 0.6599 - val_acc: 0.6667\n",
      "Epoch 16/30\n",
      "27999/27999 [==============================] - 3s 113us/step - loss: 0.6325 - acc: 0.6920 - val_loss: 0.6604 - val_acc: 0.6656\n",
      "Epoch 17/30\n",
      "27999/27999 [==============================] - 3s 104us/step - loss: 0.6308 - acc: 0.6930 - val_loss: 0.6587 - val_acc: 0.6688\n",
      "Epoch 18/30\n",
      "27999/27999 [==============================] - 3s 110us/step - loss: 0.6288 - acc: 0.6940 - val_loss: 0.6584 - val_acc: 0.6680\n",
      "Epoch 19/30\n",
      "27999/27999 [==============================] - 3s 110us/step - loss: 0.6281 - acc: 0.6960 - val_loss: 0.6578 - val_acc: 0.6677\n",
      "Epoch 20/30\n",
      "27999/27999 [==============================] - 3s 116us/step - loss: 0.6263 - acc: 0.6968 - val_loss: 0.6569 - val_acc: 0.6692\n",
      "Epoch 21/30\n",
      "27999/27999 [==============================] - 3s 105us/step - loss: 0.6246 - acc: 0.6980 - val_loss: 0.6643 - val_acc: 0.6628\n",
      "Epoch 22/30\n",
      "27999/27999 [==============================] - 3s 106us/step - loss: 0.6240 - acc: 0.6955 - val_loss: 0.6581 - val_acc: 0.6693\n",
      "Epoch 23/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.6227 - acc: 0.7007 - val_loss: 0.6608 - val_acc: 0.6672\n",
      "acc: 66.63%\n",
      "Train on 27999 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "27999/27999 [==============================] - 17s 610us/step - loss: 0.9256 - acc: 0.5887 - val_loss: 0.8038 - val_acc: 0.6528\n",
      "Epoch 2/30\n",
      "27999/27999 [==============================] - 4s 142us/step - loss: 0.7848 - acc: 0.6544 - val_loss: 0.7678 - val_acc: 0.6610\n",
      "Epoch 3/30\n",
      "27999/27999 [==============================] - 4s 134us/step - loss: 0.7494 - acc: 0.6672 - val_loss: 0.7443 - val_acc: 0.6644\n",
      "Epoch 4/30\n",
      "27999/27999 [==============================] - 4s 128us/step - loss: 0.7260 - acc: 0.6700 - val_loss: 0.7245 - val_acc: 0.6658\n",
      "Epoch 5/30\n",
      "27999/27999 [==============================] - 5s 172us/step - loss: 0.7092 - acc: 0.6746 - val_loss: 0.7094 - val_acc: 0.6619\n",
      "Epoch 6/30\n",
      "27999/27999 [==============================] - 4s 137us/step - loss: 0.6926 - acc: 0.6773 - val_loss: 0.6975 - val_acc: 0.6626\n",
      "Epoch 7/30\n",
      "27999/27999 [==============================] - 3s 112us/step - loss: 0.6818 - acc: 0.6767 - val_loss: 0.6888 - val_acc: 0.6613\n",
      "Epoch 8/30\n",
      "27999/27999 [==============================] - 3s 108us/step - loss: 0.6717 - acc: 0.6783 - val_loss: 0.6840 - val_acc: 0.6621\n",
      "Epoch 9/30\n",
      "27999/27999 [==============================] - 3s 107us/step - loss: 0.6610 - acc: 0.6832 - val_loss: 0.6745 - val_acc: 0.6632\n",
      "Epoch 10/30\n",
      "27999/27999 [==============================] - 3s 106us/step - loss: 0.6550 - acc: 0.6816 - val_loss: 0.6691 - val_acc: 0.6667\n",
      "Epoch 11/30\n",
      "27999/27999 [==============================] - 3s 108us/step - loss: 0.6504 - acc: 0.6864 - val_loss: 0.6658 - val_acc: 0.6660\n",
      "Epoch 12/30\n",
      "27999/27999 [==============================] - 3s 121us/step - loss: 0.6453 - acc: 0.6839 - val_loss: 0.6635 - val_acc: 0.6645\n",
      "Epoch 13/30\n",
      "27999/27999 [==============================] - 4s 125us/step - loss: 0.6419 - acc: 0.6845 - val_loss: 0.6618 - val_acc: 0.6633\n",
      "Epoch 14/30\n",
      "27999/27999 [==============================] - 3s 107us/step - loss: 0.6380 - acc: 0.6870 - val_loss: 0.6592 - val_acc: 0.6645\n",
      "Epoch 15/30\n",
      "27999/27999 [==============================] - 3s 94us/step - loss: 0.6367 - acc: 0.6880 - val_loss: 0.6565 - val_acc: 0.6679\n",
      "Epoch 16/30\n",
      "27999/27999 [==============================] - 3s 117us/step - loss: 0.6339 - acc: 0.6845 - val_loss: 0.6557 - val_acc: 0.6678\n",
      "Epoch 17/30\n",
      "27999/27999 [==============================] - 3s 120us/step - loss: 0.6306 - acc: 0.6859 - val_loss: 0.6544 - val_acc: 0.6638\n",
      "Epoch 18/30\n",
      "27999/27999 [==============================] - 3s 124us/step - loss: 0.6295 - acc: 0.6886 - val_loss: 0.6548 - val_acc: 0.6653\n",
      "Epoch 19/30\n",
      "27999/27999 [==============================] - 3s 119us/step - loss: 0.6262 - acc: 0.6936 - val_loss: 0.6548 - val_acc: 0.6657\n",
      "Epoch 20/30\n",
      "27999/27999 [==============================] - 4s 128us/step - loss: 0.6270 - acc: 0.6910 - val_loss: 0.6544 - val_acc: 0.6672\n",
      "acc: 67.80%\n",
      "Train on 28000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "28000/28000 [==============================] - 15s 521us/step - loss: 0.9182 - acc: 0.5977 - val_loss: 0.8007 - val_acc: 0.6527\n",
      "Epoch 2/30\n",
      "28000/28000 [==============================] - 4s 135us/step - loss: 0.7723 - acc: 0.6665 - val_loss: 0.7598 - val_acc: 0.6643\n",
      "Epoch 3/30\n",
      "28000/28000 [==============================] - 4s 129us/step - loss: 0.7380 - acc: 0.6765 - val_loss: 0.7393 - val_acc: 0.6597\n",
      "Epoch 4/30\n",
      "28000/28000 [==============================] - 3s 122us/step - loss: 0.7172 - acc: 0.6779 - val_loss: 0.7218 - val_acc: 0.6638\n",
      "Epoch 5/30\n",
      "28000/28000 [==============================] - 5s 161us/step - loss: 0.7012 - acc: 0.6773 - val_loss: 0.7077 - val_acc: 0.6659\n",
      "Epoch 6/30\n",
      "28000/28000 [==============================] - 5s 170us/step - loss: 0.6872 - acc: 0.6839 - val_loss: 0.6986 - val_acc: 0.6654\n",
      "Epoch 7/30\n",
      "28000/28000 [==============================] - 4s 127us/step - loss: 0.6765 - acc: 0.6846 - val_loss: 0.6888 - val_acc: 0.6663\n",
      "Epoch 8/30\n",
      "28000/28000 [==============================] - 4s 126us/step - loss: 0.6663 - acc: 0.6859 - val_loss: 0.6838 - val_acc: 0.6653\n",
      "Epoch 9/30\n",
      "28000/28000 [==============================] - 3s 117us/step - loss: 0.6589 - acc: 0.6873 - val_loss: 0.6770 - val_acc: 0.6670\n",
      "Epoch 10/30\n",
      "28000/28000 [==============================] - 4s 125us/step - loss: 0.6525 - acc: 0.6874 - val_loss: 0.6718 - val_acc: 0.6693\n",
      "Epoch 11/30\n",
      "28000/28000 [==============================] - 4s 135us/step - loss: 0.6462 - acc: 0.6906 - val_loss: 0.6751 - val_acc: 0.6603\n",
      "Epoch 12/30\n",
      "28000/28000 [==============================] - 3s 119us/step - loss: 0.6429 - acc: 0.6930 - val_loss: 0.6668 - val_acc: 0.6690\n",
      "Epoch 13/30\n",
      "28000/28000 [==============================] - 3s 125us/step - loss: 0.6384 - acc: 0.6957 - val_loss: 0.6654 - val_acc: 0.6683\n",
      "Epoch 14/30\n",
      "28000/28000 [==============================] - 4s 125us/step - loss: 0.6345 - acc: 0.6951 - val_loss: 0.6626 - val_acc: 0.6703\n",
      "Epoch 15/30\n",
      "28000/28000 [==============================] - 4s 137us/step - loss: 0.6316 - acc: 0.7002 - val_loss: 0.6612 - val_acc: 0.6685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "28000/28000 [==============================] - 3s 119us/step - loss: 0.6298 - acc: 0.6982 - val_loss: 0.6617 - val_acc: 0.6688\n",
      "Epoch 17/30\n",
      "28000/28000 [==============================] - 4s 128us/step - loss: 0.6275 - acc: 0.6993 - val_loss: 0.6613 - val_acc: 0.6716\n",
      "Epoch 18/30\n",
      "28000/28000 [==============================] - 5s 181us/step - loss: 0.6255 - acc: 0.7007 - val_loss: 0.6625 - val_acc: 0.6687\n",
      "acc: 66.52%\n",
      "Train on 28000 samples, validate on 12001 samples\n",
      "Epoch 1/30\n",
      "28000/28000 [==============================] - 16s 580us/step - loss: 0.9132 - acc: 0.5877 - val_loss: 0.7930 - val_acc: 0.6513\n",
      "Epoch 2/30\n",
      "28000/28000 [==============================] - 4s 132us/step - loss: 0.7706 - acc: 0.6584 - val_loss: 0.7557 - val_acc: 0.6613\n",
      "Epoch 3/30\n",
      "28000/28000 [==============================] - 3s 117us/step - loss: 0.7391 - acc: 0.6675 - val_loss: 0.7341 - val_acc: 0.6587\n",
      "Epoch 4/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.7181 - acc: 0.6728 - val_loss: 0.7163 - val_acc: 0.6631\n",
      "Epoch 5/30\n",
      "28000/28000 [==============================] - 3s 117us/step - loss: 0.7004 - acc: 0.6738 - val_loss: 0.7037 - val_acc: 0.6620\n",
      "Epoch 6/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.6867 - acc: 0.6758 - val_loss: 0.6917 - val_acc: 0.6628\n",
      "Epoch 7/30\n",
      "28000/28000 [==============================] - 3s 119us/step - loss: 0.6734 - acc: 0.6807 - val_loss: 0.6838 - val_acc: 0.6629\n",
      "Epoch 8/30\n",
      "28000/28000 [==============================] - 4s 143us/step - loss: 0.6658 - acc: 0.6808 - val_loss: 0.6793 - val_acc: 0.6605\n",
      "Epoch 9/30\n",
      "28000/28000 [==============================] - 4s 150us/step - loss: 0.6581 - acc: 0.6837 - val_loss: 0.6722 - val_acc: 0.6639\n",
      "Epoch 10/30\n",
      "28000/28000 [==============================] - 4s 144us/step - loss: 0.6520 - acc: 0.6846 - val_loss: 0.6673 - val_acc: 0.6639\n",
      "Epoch 11/30\n",
      "28000/28000 [==============================] - 4s 147us/step - loss: 0.6462 - acc: 0.6862 - val_loss: 0.6629 - val_acc: 0.6655\n",
      "Epoch 12/30\n",
      "28000/28000 [==============================] - 4s 128us/step - loss: 0.6419 - acc: 0.6845 - val_loss: 0.6598 - val_acc: 0.6638\n",
      "Epoch 13/30\n",
      "28000/28000 [==============================] - 3s 118us/step - loss: 0.6383 - acc: 0.6899 - val_loss: 0.6591 - val_acc: 0.6622\n",
      "Epoch 14/30\n",
      "28000/28000 [==============================] - 3s 106us/step - loss: 0.6357 - acc: 0.6893 - val_loss: 0.6576 - val_acc: 0.6644\n",
      "Epoch 15/30\n",
      "28000/28000 [==============================] - 3s 113us/step - loss: 0.6339 - acc: 0.6915 - val_loss: 0.6551 - val_acc: 0.6648\n",
      "Epoch 16/30\n",
      "28000/28000 [==============================] - 4s 138us/step - loss: 0.6290 - acc: 0.6908 - val_loss: 0.6579 - val_acc: 0.6626\n",
      "Epoch 17/30\n",
      "28000/28000 [==============================] - 4s 147us/step - loss: 0.6294 - acc: 0.6910 - val_loss: 0.6560 - val_acc: 0.6635\n",
      "Epoch 18/30\n",
      "28000/28000 [==============================] - 4s 143us/step - loss: 0.6289 - acc: 0.6924 - val_loss: 0.6548 - val_acc: 0.6633\n",
      "Epoch 19/30\n",
      "28000/28000 [==============================] - 4s 126us/step - loss: 0.6268 - acc: 0.6943 - val_loss: 0.6534 - val_acc: 0.6661\n",
      "Epoch 20/30\n",
      "28000/28000 [==============================] - 3s 118us/step - loss: 0.6247 - acc: 0.6960 - val_loss: 0.6541 - val_acc: 0.6658\n",
      "Epoch 21/30\n",
      "28000/28000 [==============================] - 4s 128us/step - loss: 0.6241 - acc: 0.6972 - val_loss: 0.6548 - val_acc: 0.6639\n",
      "Epoch 22/30\n",
      "28000/28000 [==============================] - 3s 120us/step - loss: 0.6248 - acc: 0.6952 - val_loss: 0.6563 - val_acc: 0.6659\n",
      "acc: 66.84%\n",
      "Train on 28000 samples, validate on 12001 samples\n",
      "Epoch 1/30\n",
      "28000/28000 [==============================] - 21s 767us/step - loss: 0.9131 - acc: 0.5587 - val_loss: 0.7954 - val_acc: 0.6454\n",
      "Epoch 2/30\n",
      "28000/28000 [==============================] - 5s 162us/step - loss: 0.7743 - acc: 0.6553 - val_loss: 0.7532 - val_acc: 0.6584\n",
      "Epoch 3/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.7420 - acc: 0.6617 - val_loss: 0.7307 - val_acc: 0.6595\n",
      "Epoch 4/30\n",
      "28000/28000 [==============================] - 3s 122us/step - loss: 0.7222 - acc: 0.6672 - val_loss: 0.7150 - val_acc: 0.6589\n",
      "Epoch 5/30\n",
      "28000/28000 [==============================] - 3s 116us/step - loss: 0.7042 - acc: 0.6690 - val_loss: 0.7021 - val_acc: 0.6628\n",
      "Epoch 6/30\n",
      "28000/28000 [==============================] - 3s 117us/step - loss: 0.6918 - acc: 0.6701 - val_loss: 0.6938 - val_acc: 0.6609\n",
      "Epoch 7/30\n",
      "28000/28000 [==============================] - 3s 116us/step - loss: 0.6799 - acc: 0.6726 - val_loss: 0.6827 - val_acc: 0.6566\n",
      "Epoch 8/30\n",
      "28000/28000 [==============================] - 3s 119us/step - loss: 0.6703 - acc: 0.6746 - val_loss: 0.6757 - val_acc: 0.6635\n",
      "Epoch 9/30\n",
      "28000/28000 [==============================] - 3s 122us/step - loss: 0.6636 - acc: 0.6772 - val_loss: 0.6706 - val_acc: 0.6616\n",
      "Epoch 10/30\n",
      "28000/28000 [==============================] - 3s 117us/step - loss: 0.6583 - acc: 0.6780 - val_loss: 0.6651 - val_acc: 0.6624\n",
      "Epoch 11/30\n",
      "28000/28000 [==============================] - 3s 120us/step - loss: 0.6520 - acc: 0.6778 - val_loss: 0.6615 - val_acc: 0.6612\n",
      "Epoch 12/30\n",
      "28000/28000 [==============================] - 3s 124us/step - loss: 0.6481 - acc: 0.6810 - val_loss: 0.6600 - val_acc: 0.6602\n",
      "Epoch 13/30\n",
      "28000/28000 [==============================] - 3s 101us/step - loss: 0.6456 - acc: 0.6786 - val_loss: 0.6568 - val_acc: 0.6647\n",
      "Epoch 14/30\n",
      "28000/28000 [==============================] - 3s 96us/step - loss: 0.6427 - acc: 0.6816 - val_loss: 0.6580 - val_acc: 0.6602\n",
      "Epoch 15/30\n",
      "28000/28000 [==============================] - 3s 111us/step - loss: 0.6408 - acc: 0.6797 - val_loss: 0.6537 - val_acc: 0.6616\n",
      "Epoch 16/30\n",
      "28000/28000 [==============================] - 4s 127us/step - loss: 0.6368 - acc: 0.6857 - val_loss: 0.6528 - val_acc: 0.6654\n",
      "Epoch 17/30\n",
      "28000/28000 [==============================] - 4s 133us/step - loss: 0.6350 - acc: 0.6847 - val_loss: 0.6519 - val_acc: 0.6608\n",
      "Epoch 18/30\n",
      "28000/28000 [==============================] - 4s 157us/step - loss: 0.6344 - acc: 0.6826 - val_loss: 0.6562 - val_acc: 0.6641\n",
      "Epoch 19/30\n",
      "28000/28000 [==============================] - 4s 127us/step - loss: 0.6321 - acc: 0.6848 - val_loss: 0.6501 - val_acc: 0.6638\n",
      "Epoch 20/30\n",
      "28000/28000 [==============================] - 4s 134us/step - loss: 0.6317 - acc: 0.6890 - val_loss: 0.6500 - val_acc: 0.6647\n",
      "Epoch 21/30\n",
      "28000/28000 [==============================] - 3s 111us/step - loss: 0.6292 - acc: 0.6883 - val_loss: 0.6495 - val_acc: 0.6652\n",
      "Epoch 22/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.6301 - acc: 0.6866 - val_loss: 0.6500 - val_acc: 0.6666\n",
      "Epoch 23/30\n",
      "28000/28000 [==============================] - 3s 121us/step - loss: 0.6299 - acc: 0.6901 - val_loss: 0.6496 - val_acc: 0.6644\n",
      "Epoch 24/30\n",
      "28000/28000 [==============================] - 3s 120us/step - loss: 0.6285 - acc: 0.6885 - val_loss: 0.6503 - val_acc: 0.6651\n",
      "acc: 66.64%\n",
      "66.89% (+/- 0.47%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=30, batch_size=1024, validation_split=0.3, verbose=1, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but with only one hidden layer that's not overly surpising. Let's increase the number of nodes in the first layer to 32, add another hidden layer of size 16 with another dropout layer before it, and see what happens. Also, let's reduce cross validation to 3 folds to save some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23332 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "23332/23332 [==============================] - 15s 639us/step - loss: 0.9114 - acc: 0.5734 - val_loss: 0.8184 - val_acc: 0.6517\n",
      "Epoch 2/30\n",
      "23332/23332 [==============================] - 3s 116us/step - loss: 0.8002 - acc: 0.6284 - val_loss: 0.7734 - val_acc: 0.6593\n",
      "Epoch 3/30\n",
      "23332/23332 [==============================] - 3s 112us/step - loss: 0.7658 - acc: 0.6448 - val_loss: 0.7474 - val_acc: 0.6592\n",
      "Epoch 4/30\n",
      "23332/23332 [==============================] - 3s 117us/step - loss: 0.7409 - acc: 0.6511 - val_loss: 0.7278 - val_acc: 0.6610\n",
      "Epoch 5/30\n",
      "23332/23332 [==============================] - 3s 115us/step - loss: 0.7199 - acc: 0.6579 - val_loss: 0.7137 - val_acc: 0.6533\n",
      "Epoch 6/30\n",
      "23332/23332 [==============================] - 3s 114us/step - loss: 0.7055 - acc: 0.6606 - val_loss: 0.7005 - val_acc: 0.6607\n",
      "Epoch 7/30\n",
      "23332/23332 [==============================] - 3s 116us/step - loss: 0.6931 - acc: 0.6625 - val_loss: 0.6923 - val_acc: 0.6618\n",
      "Epoch 8/30\n",
      "23332/23332 [==============================] - 3s 139us/step - loss: 0.6841 - acc: 0.6670 - val_loss: 0.6866 - val_acc: 0.6588\n",
      "Epoch 9/30\n",
      "23332/23332 [==============================] - 3s 127us/step - loss: 0.6780 - acc: 0.6700 - val_loss: 0.6800 - val_acc: 0.6614\n",
      "Epoch 10/30\n",
      "23332/23332 [==============================] - 3s 119us/step - loss: 0.6725 - acc: 0.6705 - val_loss: 0.6764 - val_acc: 0.6626\n",
      "Epoch 11/30\n",
      "23332/23332 [==============================] - 3s 133us/step - loss: 0.6672 - acc: 0.6738 - val_loss: 0.6740 - val_acc: 0.6610\n",
      "Epoch 12/30\n",
      "23332/23332 [==============================] - 3s 117us/step - loss: 0.6663 - acc: 0.6691 - val_loss: 0.6710 - val_acc: 0.6607\n",
      "Epoch 13/30\n",
      "23332/23332 [==============================] - 3s 111us/step - loss: 0.6598 - acc: 0.6759 - val_loss: 0.6709 - val_acc: 0.6606\n",
      "Epoch 14/30\n",
      "23332/23332 [==============================] - 3s 111us/step - loss: 0.6591 - acc: 0.6747 - val_loss: 0.6694 - val_acc: 0.6619\n",
      "Epoch 15/30\n",
      "23332/23332 [==============================] - 3s 117us/step - loss: 0.6564 - acc: 0.6786 - val_loss: 0.6684 - val_acc: 0.6615\n",
      "Epoch 16/30\n",
      "23332/23332 [==============================] - 3s 122us/step - loss: 0.6569 - acc: 0.6759 - val_loss: 0.6693 - val_acc: 0.6628\n",
      "Epoch 17/30\n",
      "23332/23332 [==============================] - 3s 118us/step - loss: 0.6551 - acc: 0.6776 - val_loss: 0.6689 - val_acc: 0.6606\n",
      "Epoch 18/30\n",
      "23332/23332 [==============================] - 3s 133us/step - loss: 0.6513 - acc: 0.6787 - val_loss: 0.6676 - val_acc: 0.6668\n",
      "Epoch 19/30\n",
      "23332/23332 [==============================] - 4s 164us/step - loss: 0.6524 - acc: 0.6803 - val_loss: 0.6666 - val_acc: 0.6663\n",
      "Epoch 20/30\n",
      "23332/23332 [==============================] - 4s 151us/step - loss: 0.6515 - acc: 0.6796 - val_loss: 0.6657 - val_acc: 0.6640\n",
      "Epoch 21/30\n",
      "23332/23332 [==============================] - 3s 120us/step - loss: 0.6497 - acc: 0.6834 - val_loss: 0.6665 - val_acc: 0.6633\n",
      "Epoch 22/30\n",
      "23332/23332 [==============================] - 3s 118us/step - loss: 0.6486 - acc: 0.6871 - val_loss: 0.6671 - val_acc: 0.6672\n",
      "Epoch 23/30\n",
      "23332/23332 [==============================] - 3s 116us/step - loss: 0.6492 - acc: 0.6836 - val_loss: 0.6696 - val_acc: 0.6620\n",
      "acc: 66.43%\n",
      "Train on 23333 samples, validate on 10001 samples\n",
      "Epoch 1/30\n",
      "23333/23333 [==============================] - 14s 611us/step - loss: 0.9086 - acc: 0.5883 - val_loss: 0.8178 - val_acc: 0.6530\n",
      "Epoch 2/30\n",
      "23333/23333 [==============================] - 3s 114us/step - loss: 0.7975 - acc: 0.6375 - val_loss: 0.7746 - val_acc: 0.6583\n",
      "Epoch 3/30\n",
      "23333/23333 [==============================] - 3s 112us/step - loss: 0.7602 - acc: 0.6452 - val_loss: 0.7460 - val_acc: 0.6539\n",
      "Epoch 4/30\n",
      "23333/23333 [==============================] - 3s 116us/step - loss: 0.7368 - acc: 0.6522 - val_loss: 0.7273 - val_acc: 0.6540\n",
      "Epoch 5/30\n",
      "23333/23333 [==============================] - 3s 115us/step - loss: 0.7193 - acc: 0.6525 - val_loss: 0.7107 - val_acc: 0.6553\n",
      "Epoch 6/30\n",
      "23333/23333 [==============================] - 3s 110us/step - loss: 0.7041 - acc: 0.6565 - val_loss: 0.7009 - val_acc: 0.6552\n",
      "Epoch 7/30\n",
      "23333/23333 [==============================] - 3s 115us/step - loss: 0.6925 - acc: 0.6589 - val_loss: 0.6914 - val_acc: 0.6576\n",
      "Epoch 8/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6804 - acc: 0.6659 - val_loss: 0.6823 - val_acc: 0.6569\n",
      "Epoch 9/30\n",
      "23333/23333 [==============================] - 3s 114us/step - loss: 0.6756 - acc: 0.6637 - val_loss: 0.6786 - val_acc: 0.6614\n",
      "Epoch 10/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6691 - acc: 0.6682 - val_loss: 0.6753 - val_acc: 0.6581\n",
      "Epoch 11/30\n",
      "23333/23333 [==============================] - 3s 118us/step - loss: 0.6628 - acc: 0.6705 - val_loss: 0.6715 - val_acc: 0.6612\n",
      "Epoch 12/30\n",
      "23333/23333 [==============================] - 3s 111us/step - loss: 0.6618 - acc: 0.6754 - val_loss: 0.6689 - val_acc: 0.6607\n",
      "Epoch 13/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6555 - acc: 0.6778 - val_loss: 0.6674 - val_acc: 0.6626\n",
      "Epoch 14/30\n",
      "23333/23333 [==============================] - 3s 116us/step - loss: 0.6547 - acc: 0.6799 - val_loss: 0.6675 - val_acc: 0.6627\n",
      "Epoch 15/30\n",
      "23333/23333 [==============================] - 3s 114us/step - loss: 0.6525 - acc: 0.6824 - val_loss: 0.6664 - val_acc: 0.6658\n",
      "Epoch 16/30\n",
      "23333/23333 [==============================] - 3s 112us/step - loss: 0.6533 - acc: 0.6807 - val_loss: 0.6666 - val_acc: 0.6654\n",
      "Epoch 17/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6511 - acc: 0.6826 - val_loss: 0.6666 - val_acc: 0.6654\n",
      "Epoch 18/30\n",
      "23333/23333 [==============================] - 3s 112us/step - loss: 0.6507 - acc: 0.6814 - val_loss: 0.6672 - val_acc: 0.6640\n",
      "acc: 66.08%\n",
      "Train on 23333 samples, validate on 10001 samples\n",
      "Epoch 1/30\n",
      "23333/23333 [==============================] - 15s 654us/step - loss: 0.9057 - acc: 0.5918 - val_loss: 0.8053 - val_acc: 0.6556\n",
      "Epoch 2/30\n",
      "23333/23333 [==============================] - 3s 116us/step - loss: 0.7856 - acc: 0.6510 - val_loss: 0.7654 - val_acc: 0.6625\n",
      "Epoch 3/30\n",
      "23333/23333 [==============================] - 3s 116us/step - loss: 0.7498 - acc: 0.6649 - val_loss: 0.7389 - val_acc: 0.6629\n",
      "Epoch 4/30\n",
      "23333/23333 [==============================] - 3s 120us/step - loss: 0.7260 - acc: 0.6681 - val_loss: 0.7181 - val_acc: 0.6613\n",
      "Epoch 5/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.7065 - acc: 0.6703 - val_loss: 0.7058 - val_acc: 0.6638\n",
      "Epoch 6/30\n",
      "23333/23333 [==============================] - 3s 115us/step - loss: 0.6916 - acc: 0.6760 - val_loss: 0.6929 - val_acc: 0.6650\n",
      "Epoch 7/30\n",
      "23333/23333 [==============================] - 3s 117us/step - loss: 0.6822 - acc: 0.6747 - val_loss: 0.6850 - val_acc: 0.6647\n",
      "Epoch 8/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6740 - acc: 0.6758 - val_loss: 0.6792 - val_acc: 0.6616\n",
      "Epoch 9/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6678 - acc: 0.6819 - val_loss: 0.6728 - val_acc: 0.6628\n",
      "Epoch 10/30\n",
      "23333/23333 [==============================] - 3s 114us/step - loss: 0.6613 - acc: 0.6825 - val_loss: 0.6699 - val_acc: 0.6662\n",
      "Epoch 11/30\n",
      "23333/23333 [==============================] - 3s 116us/step - loss: 0.6575 - acc: 0.6840 - val_loss: 0.6700 - val_acc: 0.6620\n",
      "Epoch 12/30\n",
      "23333/23333 [==============================] - 3s 113us/step - loss: 0.6549 - acc: 0.6846 - val_loss: 0.6661 - val_acc: 0.6663\n",
      "Epoch 13/30\n",
      "23333/23333 [==============================] - 3s 111us/step - loss: 0.6532 - acc: 0.6824 - val_loss: 0.6656 - val_acc: 0.6667\n",
      "Epoch 14/30\n",
      "23333/23333 [==============================] - 3s 121us/step - loss: 0.6512 - acc: 0.6871 - val_loss: 0.6656 - val_acc: 0.6652\n",
      "Epoch 15/30\n",
      "23333/23333 [==============================] - 3s 119us/step - loss: 0.6496 - acc: 0.6889 - val_loss: 0.6642 - val_acc: 0.6680\n",
      "Epoch 16/30\n",
      "23333/23333 [==============================] - 3s 117us/step - loss: 0.6483 - acc: 0.6913 - val_loss: 0.6659 - val_acc: 0.6628\n",
      "Epoch 17/30\n",
      "23333/23333 [==============================] - 3s 123us/step - loss: 0.6465 - acc: 0.6916 - val_loss: 0.6657 - val_acc: 0.6638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "23333/23333 [==============================] - 3s 114us/step - loss: 0.6458 - acc: 0.6924 - val_loss: 0.6661 - val_acc: 0.6668\n",
      "acc: 66.60%\n",
      "66.37% (+/- 0.22%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(8, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=30, batch_size=512, validation_split=0.3, verbose=1, callbacks=[early_stop])\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
