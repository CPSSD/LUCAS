{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Latest features on all statistical classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##### Duplicated! #####\n",
    "#######################\n",
    "\n",
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpZip\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake: 80466\n",
      "real: 80467\n",
      "all: 160933\n",
      "unused real: 447665\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "##### Duplicated! #####\n",
    "#######################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "fake_reviews = list(filter(lambda x: x.label, review_set.reviews))\n",
    "count_fake = len(fake_reviews)\n",
    "genuine_reviews = []\n",
    "unused_genuine_reviews = []\n",
    "counter_genuine = 0\n",
    "for review in shuffle(review_set.reviews):\n",
    "  if review.label == True:\n",
    "    continue\n",
    "  if counter_genuine <= count_fake:\n",
    "    genuine_reviews.append(review)\n",
    "    counter_genuine += 1\n",
    "  else:\n",
    "    unused_genuine_reviews.append(review)\n",
    "  \n",
    "concatted_reviews = shuffle(fake_reviews + genuine_reviews)\n",
    "print(\"fake:\", len(fake_reviews))\n",
    "print(\"real:\", len(genuine_reviews))\n",
    "print(\"all:\", len(concatted_reviews))\n",
    "print(\"unused real:\", len(unused_genuine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##### Duplicated! #####\n",
    "#######################\n",
    "def hi():\n",
    "  \"\"\"\n",
    "from exp2_feature_extraction import find_words\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "num_each_class = 8141\n",
    "reviews = shuffle(review_set.reviews)\n",
    "\n",
    "i = 0\n",
    "fake_reviews = []\n",
    "for x in reviews:\n",
    "  if i == num_each_class:\n",
    "    break\n",
    "  if x.label:\n",
    "    fake_reviews.append(x)\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "genuine_reviews = []\n",
    "for x in reviews:\n",
    "  if i == num_each_class:\n",
    "    break\n",
    "  if x.label == False:\n",
    "    fake_reviews.append(x)\n",
    "    i+=1\n",
    "    \n",
    "all_reviews = shuffle(fake_reviews + genuine_reviews)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in concatted_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latest_feature_extraction import get_features_maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features = get_features_maker(concatted_reviews, 775)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_features = get_features(concatted_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can only make our bag of words features so big before we crash with memr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy\n",
    "linearDA = LinearDiscriminantAnalysis()\n",
    "results = cross_validate(linearDA, predictor_features.toarray(), numpy.asarray(targets), cv=2,\n",
    "                         return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6899020151921419"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x for x in results['test_score'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "We normalise all our features to be between one and zero. We need to do this to suppress the mega features vs tiny features situation. Most classifiers use Euclidian distance, which has no knowledge of the units being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_features2 = get_features_maker(concatted_reviews[:n])(concatted_reviews[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sparse features are problematic here because we need to conver our data to array for standardisation. We have to limit the number of features because of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(predictor_features2.toarray())\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scaler.transform(predictor_features2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel=\"sigmoid\", gamma='scale', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"sigmoid\", gamma='scale', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should already be imported\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([3.42629886, 2.06137514]),\n",
       " 'score_time': array([1.97278214, 1.91485572]),\n",
       " 'test_score': array([0.55688623, 0.55711423])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(svc, scaled_features, targets[:n], cv=2, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
