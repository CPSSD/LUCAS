{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous experiment attempted to achieve high accuracy by using better features. Using the features that we found to work best we will increase the size of our dataset to investigate the impact. The previous experiment attempted to replicate the results of a paper, however this paper did not seem to use all of the data available to it. Here we will use all of the available data, while maintaining the balance of our two classes. The data used here will be about 10x bigger than in the last experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608598\n"
     ]
    }
   ],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpZip\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())\n",
    "print(len(review_set.reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake: 80466\n",
      "real: 80467\n",
      "all: 160933\n",
      "unused real: 447665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "fake_reviews = list(filter(lambda x: x.label, review_set.reviews))\n",
    "counter_fake = len(fake_reviews)\n",
    "genuine_reviews = []\n",
    "unused_genuine_reviews = []\n",
    "counter_genuine = 0\n",
    "for review in shuffle(review_set.reviews):\n",
    "  if review.label == True:\n",
    "    continue\n",
    "  if counter_genuine <= counter_fake:\n",
    "    genuine_reviews.append(review)\n",
    "    counter_genuine += 1\n",
    "  else:\n",
    "    unused_genuine_reviews.append(review)\n",
    "  \n",
    "concatted_reviews = fake_reviews + genuine_reviews\n",
    "print(\"fake:\", len(fake_reviews))\n",
    "print(\"real:\", len(genuine_reviews))\n",
    "print(\"all:\", len(concatted_reviews))\n",
    "print(\"unused real:\", len(unused_genuine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in concatted_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "from exp2_feature_extraction import reviewer_features\n",
    "\n",
    "corpus = [x.review_content for x in concatted_reviews]\n",
    "unigram_count_vect = CountVectorizer()\n",
    "unigram_count_vect.fit(corpus)\n",
    "\n",
    "def get_features(reviews):\n",
    "  reviews_corpus = [x.review_content for x in reviews]\n",
    "  features_ngram_bow = unigram_count_vect.transform(reviews_corpus)\n",
    "\n",
    "  reviews_reviewer_map = reviews_by_reviewer(reviews)\n",
    "  features_reviewer = [reviewer_features(x.user_id, reviews_reviewer_map) for x in reviews]\n",
    "\n",
    "  features = [features_reviewer for i in range(0, 4)]\n",
    "  features.append(features_ngram_bow)\n",
    "\n",
    "  return hstack([coo_matrix(x) for x in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.6736405194019781\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "predictor_features = get_features(concatted_reviews)\n",
    "naive_bayes = MultinomialNB()\n",
    "fold = 5\n",
    "results = cross_validate(naive_bayes, predictor_features, targets, cv=fold, return_train_score=False)\n",
    "print(\"Average accuracy:\", sum([x for x in results['test_score']])/fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is slightly better with the larger dataset, which shows the last experiment trained quite well on the smaller dataset, or at least that an increase of 10x does not improve the model dramatically.\n",
    "\n",
    "Now, because we have some spare data, let's see how many of the unused genuine reviews our model can correctly label as genuine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(predictor_features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_features = get_features(unused_genuine_reviews[:100000])\n",
    "results = naive_bayes.predict(unused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60188 of 100000\n"
     ]
    }
   ],
   "source": [
    "genuine_results = [x for x in results if x == False]\n",
    "print(len(genuine_results), \"of\", len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our classifier likes to classify things as fake, since our accuracy for an all genuine set is below the accuracy of our classifier. We don't have any unused fake reviews, but we can play around with our test set again, even though it's not very meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake set: 64485 of 80466 = 0.8013943777496085\n"
     ]
    }
   ],
   "source": [
    "used_fake_features = get_features(fake_reviews)\n",
    "used_fake_results = naive_bayes.predict(used_fake_features)\n",
    "\n",
    "used_fake_correct = len([x for x in used_fake_results if x == True])\n",
    "used_fake_total = len(used_fake_results)\n",
    "print(\"Fake set:\", used_fake_correct, \"of\", used_fake_total, \"=\", used_fake_correct/used_fake_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake set: 50456 of 80467 = 0.627039656008053\n"
     ]
    }
   ],
   "source": [
    "used_genuine_features = get_features(genuine_reviews)\n",
    "used_genuine_results = naive_bayes.predict(used_genuine_features)\n",
    "\n",
    "used_genuine_correct = len([x for x in used_genuine_results if x == False])\n",
    "used_genuine_total = len(used_genuine_results)\n",
    "print(\"Fake set:\", used_genuine_correct, \"of\", used_genuine_total, \"=\", used_genuine_correct/used_genuine_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These accuracies are using data that was in the test set, so we can't trust them. But we do see that our classifier is biased to classify as fake on an arbitrary review. Since we have already balanced our data set, we can perhaps increase the accuracy of our classifier by steering it, to remove this bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
