{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous experiment attempted to achieve high accuracy by using better features. These features were taken from a publication that claimed they worked. Since we could not replicate the results in the last experiment, here we will try using more data. We should be using a balanced dataset, now that we have x in the negative set we can reduce the positive set to the same number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608598\n"
     ]
    }
   ],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpZip\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())\n",
    "print(len(review_set.reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake: 80466\n",
      "real: 80467\n",
      "all: 160933\n",
      "unused real: 447665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "fake_reviews = list(filter(lambda x: x.label, review_set.reviews))\n",
    "counter_fake = len(fake_reviews)\n",
    "genuine_reviews = []\n",
    "unused_genuine_reviews = []\n",
    "counter_genuine = 0\n",
    "for review in shuffle(review_set.reviews):\n",
    "  if review.label == True:\n",
    "    continue\n",
    "  if counter_genuine <= counter_fake:\n",
    "    genuine_reviews.append(review)\n",
    "    counter_genuine += 1\n",
    "  else:\n",
    "    unused_genuine_reviews.append(review)\n",
    "  \n",
    "concatted_reviews = fake_reviews + genuine_reviews\n",
    "print(\"fake:\", len(fake_reviews))\n",
    "print(\"real:\", len(genuine_reviews))\n",
    "print(\"all:\", len(concatted_reviews))\n",
    "print(\"unused real:\", len(unused_genuine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words\n",
    "all_reviews = [(x, find_words(x.review_content)) for x in concatted_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import structural_features\n",
    "features_structural = [structural_features(x) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##### DUPLICATION CELL #####\n",
    "############################\n",
    "\n",
    "import nltk\n",
    "\n",
    "def pos_features(words, pos_tagger):\n",
    "  tag_map = {\n",
    "    \"CD\":  0, \"DT\":  0, \"EX\":  0, \"FW\":   0, \"IN\":  0, \"JJ\":  0, \"JJR\": 0, \"JJS\": 0, \"LS\":   0,\n",
    "    \"MD\":  0, \"NN\":  0, \"NNP\": 0, \"NNPS\": 0, \"NNS\": 0, \"PDT\": 0, \"POS\": 0, \"PRP\": 0, \"PRP$\": 0,\n",
    "    \"RB\":  0, \"RBR\": 0, \"RBS\": 0, \"RP\":   0, \"SYM\": 0, \"TO\":  0, \"UH\":  0, \"VB\":  0, \"VBD\":  0,\n",
    "    \"VBG\": 0, \"VBN\": 0, \"VBP\": 0, \"VBZ\":  0, \"WDT\": 0, \"WP\":  0, \"WP$\": 0, \"WRB\": 0, \"CC\":   0\n",
    "  }\n",
    "  tags = pos_tagger.pos_tag(words)\n",
    "  total = 0\n",
    "  for tag in tags:\n",
    "    key = tag[1]\n",
    "    if key in tag_map:\n",
    "      tag_map[key] += 1\n",
    "      total += 1\n",
    "  if total == 0:\n",
    "    return [0] * 36\n",
    "  order = [\"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\",\n",
    "           \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\",\n",
    "           \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"CC\"]\n",
    "  return [tag_map[x]/total for x in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pos = [pos_features(x[1], nltk) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "from exp2_feature_extraction import sentiment_features\n",
    "features_sentiment = [sentiment_features(x[1], sentiment_analyzer) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['place', 'like', 'food']\n",
      "1: ['pizza', 'best', 'slice']\n",
      "2: ['chicken', 'sauc', 'good']\n",
      "3: ['order', 'tabl', 'come']\n",
      "4: ['dish', 'order', 'dessert']\n",
      "5: ['brunch', 'coffe', 'egg']\n",
      "6: ['food', 'great', 'place']\n",
      "7: ['ramen', 'menu', 'nice']\n",
      "8: ['burger', 'beer', 'drink']\n",
      "9: ['good', 'place', 'food']\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "##### DUPLICATION CELL #####\n",
    "############################\n",
    "\n",
    "from exp2_feature_extraction import preprocess_words\n",
    "import gensim\n",
    "\n",
    "def get_topic_features_maker(reviews):\n",
    "  num_topics = 10\n",
    "  preprocessed_words = [preprocess_words(x[1]) for x in reviews]\n",
    "  \n",
    "  dictionary = gensim.corpora.Dictionary(preprocessed_words)\n",
    "  dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "  bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_words]\n",
    "  lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=2)\n",
    "    \n",
    "  for index, topic in lda_model.show_topics(formatted=False, num_words=3):\n",
    "    print('{}: {}'.format(index, [w[0] for w in topic]))\n",
    "  \n",
    "  def make_topic_features(review_words):\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess_words(review_words)))\n",
    "    return topic_features(topics, num_topics)\n",
    "  return make_topic_features\n",
    "\n",
    "topic_features_maker = get_topic_features_maker(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import topic_features\n",
    "features_topic = [topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##### DUPLICATION CELL #####\n",
    "############################\n",
    "\n",
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "reviews_reviewer_map = reviews_by_reviewer([x[0] for x in all_reviews])\n",
    "features_reviewer = [reviewer_features(x[0], reviews_reviewer_map) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "predictor_features = hstack([coo_matrix(features_structural), coo_matrix(features_sentiment), coo_matrix(features_pos),\n",
    "                             coo_matrix(features_topic), coo_matrix(features_reviewer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x[0].label for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.fit(predictor_features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.10538673, 0.1095612 , 0.11662054, 0.11348391, 0.10610676,\n",
       "        0.10550308, 0.10498762, 0.10460353, 0.10838938, 0.10805106]),\n",
       " 'score_time': array([0.00495863, 0.00488496, 0.00506663, 0.00478959, 0.00477529,\n",
       "        0.00490165, 0.00547576, 0.00493288, 0.00593281, 0.00481367]),\n",
       " 'test_score': array([0.59108985, 0.5968684 , 0.594383  , 0.59587424, 0.60500808,\n",
       "        0.58226668, 0.60939539, 0.60514541, 0.58861546, 0.59364902])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake reviews:\n",
      "799 out of 1000\n",
      "708 out of 1000\n",
      "674 out of 1000\n",
      "712 out of 1000\n",
      "Genuine reviews:\n",
      "560 out of 1000\n",
      "550 out of 1000\n",
      "568 out of 1000\n",
      "539 out of 1000\n",
      "Unused genuine reviews:\n"
     ]
    }
   ],
   "source": [
    "# TODO, what's the point of having the num_intervals?\n",
    "def print_num_fake_for_set(test_set, start, interval, num_intervals):\n",
    "  for i in range(1, num_intervals):\n",
    "    unused_reviews = [(x, find_words(x.review_content)) for x in test_set[start:start+interval]]\n",
    "\n",
    "    unused_structural = [structural_features(x) for x in unused_reviews]\n",
    "\n",
    "    unused_sentiment = [sentiment_features(x[1], sentiment_analyzer) for x in unused_reviews]\n",
    "\n",
    "    unused_pos = [pos_features(x[1], nltk) for x in unused_reviews]\n",
    "\n",
    "    unused_topic = [topic_features_maker(x[1]) for x in unused_reviews]\n",
    "\n",
    "    unused_reviewer_map = reviews_by_reviewer([x[0] for x in unused_reviews])\n",
    "    unused_reviewer = [reviewer_features(x[0], unused_reviewer_map) for x in unused_reviews]\n",
    "\n",
    "    unused_features = hstack([coo_matrix(unused_structural), coo_matrix(unused_sentiment), coo_matrix(unused_pos),\n",
    "                              coo_matrix(unused_topic), coo_matrix(unused_reviewer)])\n",
    "\n",
    "    results = cnb.predict(unused_features)\n",
    "    count = len([x for x in results if x])\n",
    "\n",
    "    print(count, \"out of\", len(results))\n",
    "    start+=interval\n",
    "    \n",
    "print(\"Fake reviews:\")\n",
    "print_num_fake_for_set(fake_reviews, 0, 1000, 5)\n",
    "print(\"Genuine reviews:\")\n",
    "print_num_fake_for_set(genuine_reviews, 0, 1000, 5)\n",
    "print(\"Unused genuine reviews:\")\n",
    "print_num_fake_for_set(unused_genuine_reviews, 0, 1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the classifier much is more accurate at guessing fake reviews than genuine reviews. This is probably because it is biased towards choosing fake for any arbitrary review. Since we have already balanced our data set, we can help fix this situation by changing the weights of our model, so that they are more fair towards genuine reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
