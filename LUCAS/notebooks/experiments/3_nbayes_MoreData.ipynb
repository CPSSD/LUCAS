{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous experiment attempted to achieve high accuracy by using better features. These features were taken from a publication that claimed they worked. Since we could not replicate the results in the last experiment, here we will try using more data. We should be using a balanced dataset, now that we have x in the negative set we can reduce the positive set to the same number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608598\n"
     ]
    }
   ],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpZip\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())\n",
    "print(len(review_set.reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake: 80466\n",
      "real: 80466\n",
      "all: 160932\n"
     ]
    }
   ],
   "source": [
    "fake_reviews = list(filter(lambda x: x.label, review_set.reviews))\n",
    "counter_fake = len(fake_reviews)\n",
    "genuine_reviews = []\n",
    "counter_genuine = 0\n",
    "for review in review_set.reviews:\n",
    "  if review.label == False:\n",
    "    genuine_reviews.append(review)\n",
    "    counter_genuine += 1\n",
    "  if counter_genuine == counter_fake:\n",
    "    break\n",
    "  \n",
    "concatted_reviews = fake_reviews + genuine_reviews\n",
    "print(\"fake:\", len(fake_reviews))\n",
    "print(\"real:\", len(genuine_reviews))\n",
    "print(\"all:\", len(concatted_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words\n",
    "from sklearn.utils import shuffle\n",
    "all_reviews = [(x, find_words(x.review_content)) for x in shuffle(concatted_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import structural_features\n",
    "features_structural = [structural_features(x) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##### DUPLICATION CELL #####\n",
    "############################\n",
    "\n",
    "import nltk\n",
    "\n",
    "def pos_features(words, pos_tagger):\n",
    "  tag_map = {\n",
    "    \"CD\":  0, \"DT\":  0, \"EX\":  0, \"FW\":   0, \"IN\":  0, \"JJ\":  0, \"JJR\": 0, \"JJS\": 0, \"LS\":   0,\n",
    "    \"MD\":  0, \"NN\":  0, \"NNP\": 0, \"NNPS\": 0, \"NNS\": 0, \"PDT\": 0, \"POS\": 0, \"PRP\": 0, \"PRP$\": 0,\n",
    "    \"RB\":  0, \"RBR\": 0, \"RBS\": 0, \"RP\":   0, \"SYM\": 0, \"TO\":  0, \"UH\":  0, \"VB\":  0, \"VBD\":  0,\n",
    "    \"VBG\": 0, \"VBN\": 0, \"VBP\": 0, \"VBZ\":  0, \"WDT\": 0, \"WP\":  0, \"WP$\": 0, \"WRB\": 0, \"CC\":   0\n",
    "  }\n",
    "  tags = pos_tagger.pos_tag(words)\n",
    "  total = 0\n",
    "  for tag in tags:\n",
    "    key = tag[1]\n",
    "    if key in tag_map:\n",
    "      tag_map[key] += 1\n",
    "      total += 1\n",
    "  if total == 0:\n",
    "    return [0] * 36\n",
    "  order = [\"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\",\n",
    "           \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\",\n",
    "           \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"CC\"]\n",
    "  return [tag_map[x]/total for x in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pos = [pos_features(x[1], nltk) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "from exp2_feature_extraction import sentiment_features\n",
    "features_sentiment = [sentiment_features(x[1], sentiment_analyzer) for x in all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##### DUPLICATION CELL #####\n",
    "############################\n",
    "\n",
    "import gensim\n",
    "\n",
    "def get_topic_features_maker(reviews):\n",
    "  num_topics = 10\n",
    "  preprocessed_words = [preprocess_words(x[1]) for x in reviews]\n",
    "  \n",
    "  dictionary = gensim.corpora.Dictionary(preprocessed_words)\n",
    "  dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "  bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_words]\n",
    "  lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=2)\n",
    "    \n",
    "  for index, topic in lda_model.show_topics(formatted=False, num_words=3):\n",
    "    print('{}: {}'.format(index, [w[0] for w in topic]))\n",
    "  \n",
    "  def make_topic_features(review_words):\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess_words(review_words)))\n",
    "    return topic_features(topics, num_topics)\n",
    "  return make_topic_features\n",
    "\n",
    "topic_features_maker = get_topic_features_maker(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topic = [topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##### DUPLICATION CELL #####\n",
    "############################\n",
    "\n",
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "reviews_reviewer_map = reviews_by_reviewer([x[0] for x in all_reviews])\n",
    "features_reviewer = [reviewer_features(x[0], reviews_reviewer_map) for x in all_reviews]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
