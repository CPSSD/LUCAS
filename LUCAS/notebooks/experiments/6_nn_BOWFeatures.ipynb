{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: Fully connected feedforward neural network with Bag of Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first deep learning experiment following the proof of concept in the parent directory, this notebook will experiment with the most basic form of neural network, which is simply a multi-layer feedforward Perceptron with all nodes connected to every other node. However, in this one we will use a much bigger dataset, because the previous notebook is deceptive(lol) in its accuracy figures because it's such a small, similar dataset.\n",
    "\n",
    "First, let's import and split our data. We'll use 20,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers as th\n",
    "\n",
    "reviews_set, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()\n",
    "reviews = reviews_set[:50000]\n",
    "X = [x.review_content for x in reviews]\n",
    "y = np.array([x.label for x in reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets limit the number of words we use from our reviews, to filter out some of the nonsense. 10000 is as good a number as any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a function that takes a bunch of reviews and returns them as word count vectors of size 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return tokenizer.texts_to_matrix(data, mode='count')\n",
    "\n",
    "X = np.array(tokenize(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a review to make sure it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 3. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's train and validate a model to see what our accuracies look like. \n",
    "This time, to help prevent overfitting, we're going to use k-fold cross validation, with k=10, to make sure our model isn't biased to a particular chunk of the dataset.\n",
    "We wont use any regularization methods this time around so we can see how they affect it when we add them in to our layers later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.6842 - acc: 0.5746 - val_loss: 0.6617 - val_acc: 0.6341\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6348 - acc: 0.6687 - val_loss: 0.6269 - val_acc: 0.6601\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.5959 - acc: 0.6959 - val_loss: 0.6159 - val_acc: 0.6687\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.5673 - acc: 0.7144 - val_loss: 0.6139 - val_acc: 0.6694\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.5424 - acc: 0.7327 - val_loss: 0.6175 - val_acc: 0.6693\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.5188 - acc: 0.7513 - val_loss: 0.6258 - val_acc: 0.6634\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.4955 - acc: 0.7660 - val_loss: 0.6362 - val_acc: 0.6621\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.4729 - acc: 0.7819 - val_loss: 0.6488 - val_acc: 0.6615\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4488 - acc: 0.7988 - val_loss: 0.6665 - val_acc: 0.6561\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.4247 - acc: 0.8144 - val_loss: 0.6823 - val_acc: 0.6547\n",
      "acc: 65.75%\n",
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 0.6726 - acc: 0.5759 - val_loss: 0.6475 - val_acc: 0.6518\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6213 - acc: 0.6777 - val_loss: 0.6215 - val_acc: 0.6701\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.5850 - acc: 0.7035 - val_loss: 0.6160 - val_acc: 0.6696\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.5591 - acc: 0.7215 - val_loss: 0.6160 - val_acc: 0.6713\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.5370 - acc: 0.7361 - val_loss: 0.6238 - val_acc: 0.6682\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.5170 - acc: 0.7489 - val_loss: 0.6296 - val_acc: 0.6662\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.4976 - acc: 0.7617 - val_loss: 0.6390 - val_acc: 0.6644\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.4792 - acc: 0.7723 - val_loss: 0.6506 - val_acc: 0.6612\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4619 - acc: 0.7838 - val_loss: 0.6661 - val_acc: 0.6547\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.4458 - acc: 0.7932 - val_loss: 0.6911 - val_acc: 0.6523\n",
      "acc: 66.09%\n",
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 0.6668 - acc: 0.5969 - val_loss: 0.6399 - val_acc: 0.6551\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6146 - acc: 0.6798 - val_loss: 0.6224 - val_acc: 0.6681\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.5837 - acc: 0.7017 - val_loss: 0.6133 - val_acc: 0.6752\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5606 - acc: 0.7175 - val_loss: 0.6150 - val_acc: 0.6713\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.5412 - acc: 0.7296 - val_loss: 0.6184 - val_acc: 0.6689\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.5238 - acc: 0.7413 - val_loss: 0.6256 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.5068 - acc: 0.7528 - val_loss: 0.6365 - val_acc: 0.6621\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.4896 - acc: 0.7635 - val_loss: 0.6441 - val_acc: 0.6619\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4740 - acc: 0.7725 - val_loss: 0.6603 - val_acc: 0.6573\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.4558 - acc: 0.7822 - val_loss: 0.6848 - val_acc: 0.6496\n",
      "acc: 64.39%\n",
      "Train on 31499 samples, validate on 13500 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.6789 - acc: 0.5673 - val_loss: 0.6532 - val_acc: 0.6467\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6289 - acc: 0.6714 - val_loss: 0.6224 - val_acc: 0.6692\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.5940 - acc: 0.6945 - val_loss: 0.6115 - val_acc: 0.6751\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5667 - acc: 0.7124 - val_loss: 0.6076 - val_acc: 0.6792\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.5432 - acc: 0.7270 - val_loss: 0.6118 - val_acc: 0.6784\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.5209 - acc: 0.7422 - val_loss: 0.6192 - val_acc: 0.6739\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.4996 - acc: 0.7568 - val_loss: 0.6314 - val_acc: 0.6670\n",
      "Epoch 8/10\n",
      " - 8s - loss: 0.4816 - acc: 0.7691 - val_loss: 0.6450 - val_acc: 0.6644\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4609 - acc: 0.7831 - val_loss: 0.6569 - val_acc: 0.6653\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.4410 - acc: 0.7956 - val_loss: 0.6741 - val_acc: 0.6574\n",
      "acc: 63.49%\n",
      "Train on 31499 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.6676 - acc: 0.5738 - val_loss: 0.6380 - val_acc: 0.6532\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.6098 - acc: 0.6810 - val_loss: 0.6177 - val_acc: 0.6708\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.5758 - acc: 0.7069 - val_loss: 0.6142 - val_acc: 0.6740\n",
      "Epoch 4/10\n",
      " - 10s - loss: 0.5468 - acc: 0.7269 - val_loss: 0.6163 - val_acc: 0.6720\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.5197 - acc: 0.7468 - val_loss: 0.6253 - val_acc: 0.6671\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.4946 - acc: 0.7628 - val_loss: 0.6381 - val_acc: 0.6678\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.4692 - acc: 0.7803 - val_loss: 0.6522 - val_acc: 0.6579\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.4456 - acc: 0.7954 - val_loss: 0.6744 - val_acc: 0.6586\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4216 - acc: 0.8101 - val_loss: 0.6907 - val_acc: 0.6557\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.3993 - acc: 0.8241 - val_loss: 0.7134 - val_acc: 0.6521\n",
      "acc: 65.34%\n",
      "Train on 31499 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.6710 - acc: 0.5774 - val_loss: 0.6439 - val_acc: 0.6520\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.6130 - acc: 0.6787 - val_loss: 0.6198 - val_acc: 0.6668\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.5793 - acc: 0.7029 - val_loss: 0.6147 - val_acc: 0.6737\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5522 - acc: 0.7215 - val_loss: 0.6154 - val_acc: 0.6752\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.5280 - acc: 0.7416 - val_loss: 0.6205 - val_acc: 0.6745\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.5059 - acc: 0.7569 - val_loss: 0.6278 - val_acc: 0.6669\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.4835 - acc: 0.7720 - val_loss: 0.6396 - val_acc: 0.6651\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.4617 - acc: 0.7870 - val_loss: 0.6540 - val_acc: 0.6604\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.4395 - acc: 0.7999 - val_loss: 0.6691 - val_acc: 0.6562\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.4172 - acc: 0.8136 - val_loss: 0.6873 - val_acc: 0.6517\n",
      "acc: 64.02%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.6621 - acc: 0.5955 - val_loss: 0.6333 - val_acc: 0.6611\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6053 - acc: 0.6874 - val_loss: 0.6177 - val_acc: 0.6703\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.5723 - acc: 0.7109 - val_loss: 0.6150 - val_acc: 0.6703\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.5471 - acc: 0.7295 - val_loss: 0.6185 - val_acc: 0.6671\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.5229 - acc: 0.7450 - val_loss: 0.6250 - val_acc: 0.6678\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.5008 - acc: 0.7570 - val_loss: 0.6389 - val_acc: 0.6642\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.4786 - acc: 0.7717 - val_loss: 0.6538 - val_acc: 0.6588\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.4577 - acc: 0.7850 - val_loss: 0.6703 - val_acc: 0.6549\n",
      "Epoch 9/10\n",
      " - 8s - loss: 0.4382 - acc: 0.7989 - val_loss: 0.6894 - val_acc: 0.6526\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.4202 - acc: 0.8092 - val_loss: 0.7072 - val_acc: 0.6473\n",
      "acc: 64.87%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 0.6641 - acc: 0.6012 - val_loss: 0.6395 - val_acc: 0.6559\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.6101 - acc: 0.6847 - val_loss: 0.6204 - val_acc: 0.6707\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.5781 - acc: 0.7069 - val_loss: 0.6162 - val_acc: 0.6700\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.5516 - acc: 0.7237 - val_loss: 0.6186 - val_acc: 0.6701\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.5291 - acc: 0.7398 - val_loss: 0.6259 - val_acc: 0.6682\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.5087 - acc: 0.7508 - val_loss: 0.6375 - val_acc: 0.6644\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.4887 - acc: 0.7642 - val_loss: 0.6527 - val_acc: 0.6615\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.4699 - acc: 0.7768 - val_loss: 0.6685 - val_acc: 0.6554\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4501 - acc: 0.7892 - val_loss: 0.6858 - val_acc: 0.6542\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.4314 - acc: 0.8026 - val_loss: 0.7126 - val_acc: 0.6482\n",
      "acc: 64.79%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.6682 - acc: 0.5814 - val_loss: 0.6420 - val_acc: 0.6510\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6136 - acc: 0.6817 - val_loss: 0.6223 - val_acc: 0.6654\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.5795 - acc: 0.7065 - val_loss: 0.6149 - val_acc: 0.6720\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.5520 - acc: 0.7271 - val_loss: 0.6162 - val_acc: 0.6728\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.5291 - acc: 0.7408 - val_loss: 0.6267 - val_acc: 0.6685\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.5068 - acc: 0.7561 - val_loss: 0.6342 - val_acc: 0.6643\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.4857 - acc: 0.7690 - val_loss: 0.6467 - val_acc: 0.6623\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.4664 - acc: 0.7814 - val_loss: 0.6683 - val_acc: 0.6581\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.4447 - acc: 0.7951 - val_loss: 0.6771 - val_acc: 0.6555\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.4240 - acc: 0.8072 - val_loss: 0.6975 - val_acc: 0.6537\n",
      "acc: 63.93%\n",
      "Train on 31500 samples, validate on 13501 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 0.6715 - acc: 0.5867 - val_loss: 0.6454 - val_acc: 0.6517\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.6177 - acc: 0.6792 - val_loss: 0.6214 - val_acc: 0.6682\n",
      "Epoch 3/10\n",
      " - 12s - loss: 0.5831 - acc: 0.7013 - val_loss: 0.6118 - val_acc: 0.6731\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.5571 - acc: 0.7202 - val_loss: 0.6114 - val_acc: 0.6715\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.5339 - acc: 0.7373 - val_loss: 0.6185 - val_acc: 0.6671\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.5121 - acc: 0.7508 - val_loss: 0.6252 - val_acc: 0.6688\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.4909 - acc: 0.7652 - val_loss: 0.6377 - val_acc: 0.6623\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.4699 - acc: 0.7794 - val_loss: 0.6521 - val_acc: 0.6600\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.4472 - acc: 0.7950 - val_loss: 0.6668 - val_acc: 0.6568\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.4261 - acc: 0.8094 - val_loss: 0.6877 - val_acc: 0.6528\n",
      "acc: 65.65%\n",
      "64.83% (+/- 0.83%)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu,),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs=10, batch_size=2048, validation_split=0.3, verbose=2)\n",
    "    scores = model.evaluate(X[test], y[test], verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. With only one hidden layer, 8 nodes, simple word count embeddings as features and no dropout or regularization, we get 65% accuracy. Not as good as our POC, but with almost 12x the data, definitley a good first step.\n",
    "\n",
    "Next steps: Add regularizations (L1 and L2), compare, add dropout, compare, add early stop callback, try TFIDF, add some more features, and call it a day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
