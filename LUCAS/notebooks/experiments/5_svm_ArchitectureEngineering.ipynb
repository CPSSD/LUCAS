{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deeper look at SVM architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last experiment that we ran, where we used a LinearSVM without changing any of the deafult parameters or feaure engineering, we achieved an accuracy of 63%. It also gave us convergence warnings for the linear kernel.\n",
    "We expect that SVMs should be more capable than this, so in this notebook we will iterate over the SVM design and try different approaches to the problem using this classifier. The recommended SVM kernel for text classification is the linear kernel, so we will attempt to obtain a result from this.\n",
    "Let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x.review_content for x in all_reviews]\n",
    "y = [1 if x.label else 0 for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with Logistic Regression\n",
    "Logistic Regression is simpler and faster than an SVM. It also correlates well with SVMs with a linear kernel. Both attempt to divide the samples. SVMs attempt to produce a wide margin in the division, and although logistic regression does not do this by design, it is known to produce results of this nature. Logistic regression is also a little easier to obtain metrics for, as we can also view the log loss metric. Because of all of this we will start with logistic regression to do our initial experimentation, and then switch to SVMs at the end when we are ready to change the penalty variable.\n",
    "\n",
    "First are some helper functions to get the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc_score_from_probabilities(probabilities, labels):\n",
    "  true_probabilities = [probabilities[i][1] for i in range(0, len(labels))]\n",
    "  return roc_auc_score(labels, true_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_uses_correct_probabilities (__main__.TestRocScoreFromProbabilities) ... /Users/niallwalsh/miniconda3/envs/lucas/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Please use assertEqual instead.\n",
      "  \n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a1bca75c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestRocScoreFromProbabilities(unittest.TestCase):\n",
    "  \n",
    "  def test_uses_correct_probabilities(self):\n",
    "    probabilities = [[0.9, 0.1], [0.1, 0.9]]\n",
    "    labels = [0, 1]\n",
    "    self.assertEquals(1, auroc_score_from_probabilities(probabilities, labels))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to automate the process of cross validation and finding the accuracy, the mean, variance and our other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "def run_cross_validate(model, X, y, get_features_fn, cv=5):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "    \"auroc\": [],\n",
    "    \"f1 scores\": [],\n",
    "    \"log loss\": []\n",
    "  }\n",
    "    \n",
    "  false_negatives = 0\n",
    "  false_positives = 0\n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    predicted = [0 if x[0] > x[1] else 1 for x in probabilities]\n",
    "    for i in range(0, len(predicted)):\n",
    "      if (predicted[i] == 0 and test_y[i] == 1):\n",
    "        false_negatives+=1\n",
    "      if (predicted[i] == 1 and test_y[i] == 0):\n",
    "        false_positives+=1\n",
    "    \n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(auroc_score_from_probabilities(probabilities, test_y))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "    metrics[\"log loss\"].append(log_loss(test_y, probabilities))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  metrics[\"mean log loss\"] = np.mean(metrics[\"log loss\"])\n",
    "  metrics[\"false negatives rate\"] = (false_negatives / cv) / num_samples\n",
    "  metrics[\"false positives rate\"] = (false_positives / cv) / num_samples\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cVec = CountVectorizer()\n",
    "def get_features(predictor_set, fit=False):\n",
    "  if fit:\n",
    "    return cVec.fit_transform(predictor_set)\n",
    "  return cVec.transform(predictor_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to evaluate\n",
    "\n",
    "### Accuracy\n",
    "This is the percentage of correct classifications. The higher the better, however it is not appropriate in all cases.\n",
    "\n",
    "This metric falls to the imbalanced classification problem. When there are many more of one class, the classifier can choose it much more, or all the time, to achieve a high accuracy.\n",
    "\n",
    "### False Positives, False Negatives\n",
    "False positives are samples that were classified as fake, but were in fact genuine. False negatives are samples that were classified as genuine, but were in fact fake. We want to reduce both of these values as much as possible. True positives and true negatives are the adverse values which we want to maximise, although they correlate to false positives and negatives so there's no need to include them here.\n",
    "\n",
    "One important question here is **False negatives vs False positives**? Is it worse to falsely suggest that something is fake, or is it worse to falsely suggest that something is genuine? Probably in this system a human might be paid to read those suspicious reviews. It would be good to catch all the fake reviews, plus some genuine ones, because this is just like filtering the huge number of reviews to make a human's job easier. In this case it is better to reduce false negatives. If humans are not reviewing the system, then this would be a different situation, it would probably be better to reduce false positives.\n",
    "\n",
    "### Recall\n",
    "Of all the fake reviews, what percentage were identified as fake? This is not subject to the imbalanced\n",
    "classification problem. We aim to maximise it as an indication of how well we are really identifying our fake reviews. \n",
    "\n",
    "We cannot focus solely on recall, because we could identify all reviews as fake and achieve 100% recall. Precision must be included in the consideration.\n",
    "\n",
    "### Precision\n",
    "Of all the reviews identified as fake, what percentage are actually fake? If we classify all reviews as fake, then our precision will be low. If we classify all reviews as genuine, then we wont have any precision either.\n",
    "\n",
    "In our case it might be more important to have a high recall, if we don't want to miss any fake reviews. Otherwise if we want to be as accurate as possible we can balance recall and precision.\n",
    "\n",
    "### F1 Score\n",
    "This is a harmonic mean of precision and recall. Because of this it punishes extreme values such as a recall of or a precision of 0.0\n",
    "\n",
    "This also acts as a single number metric representing precision and recall.\n",
    "\n",
    "### Area Under Curve (AUC) \n",
    "This gives us a measure of discrimination, how well we correctly classify both classes. This does not use a 'Yes' or 'No' which can make it more interesting than accuracy.\n",
    "\n",
    "At different classification thresholds, how well do we predict fake reviews as 'more fake' than genuine reviews. We plot the true positive rate against the false positive rate to get a graph. Changing the threshold allows us to create a graph because at low thresholds we will have more fake reviews, increasing the true positives rate. Decreasing the treshold means we will have less genuine reviews, decreasing the true negative rate, which therefore increases the false positive rate.\n",
    "\n",
    "An AUC of 0.8 means the chance of the model distinguishing positive and negative classes is 80%.\n",
    "\n",
    "### Mean Squared Error\n",
    "The average of the square difference between the original values and the predicted values. Adds focus to large errors, and is easier to compute than mean absolute error.\n",
    "\n",
    "The closer the mean squared error is to zero the better. It incorporates the variance and the bias of the estimator\n",
    "\n",
    "### Logarithmic loss\n",
    "This takes into account the uncertainty of a prediction, taking into account how far away from the actual label the prediction is. As the probability approaches correct the log loss reduces only very little. As the probability approaches incorrect the log loss increases rapidly. This means that confident incorrect values are highly penalized.\n",
    "\n",
    "We aim to minimize log loss.\n",
    "\n",
    "### Cohen's Kappa\n",
    "A reliability metric used when there is more than one classifier. Computes an agreement percentage of the used classifiers. It is out of scope at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Results\n",
    "\n",
    "Our results show quite normal results. The AUROC score and F1 score above are as expected, reflective of similar work in this field. We can see that the variance is not very large, so we can depend on our scores.\n",
    "\n",
    "Since there is nothing alarming, the next thing to consider is improving features or trying to find more predictive features. We might see better results if convert our bow to tfidf. This is especially important because the normalisation function of logistic regression uses regularisation that depends on the features being scaled. Tfidf will scale our features for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_bag_of_words(cv, tfidf, predictor_set, fit):\n",
    "  if fit:\n",
    "    return tfidf.fit_transform(cv.fit_transform(predictor_set))\n",
    "  else:\n",
    "    return tfidf.transform(cv.transform(predictor_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def get_features(predictor_features, fit=False):\n",
    "  return tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_features, fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like switching to Tfidf had a positive effect on our results, which is not unexpected. Let's have a look at our vocabulary, to see what words are actually in our bag of words. Let's try to clean our features more, hopefully a smaller number of features, more concentrated with quality words will improve accuracy. Currently our bag of words has the following shape: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160933, 88813)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I tried to improve features in a number of ways. I expected that stopwords are not just noise in this classification, unlike topic of sentiment classification. When I removed them it actually caused a drop in accuracy, so I kept them in. I also tried lemmatizing words, however this did not boost accuracy much, and for the time taken to do this I decided to just leave them out.\n",
    "\n",
    "Let's set a low limit for the vocab size. This cleans out everything from the typos to the random jibberish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160933, 10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "count_vectorizer.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try adding reviewer features, which have been proven to be predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scaled_reviewer_features(reviews):\n",
    "  reviewer_reviews = reviews_by_reviewer(reviews)\n",
    "  reviewer_predictors = [list(reviewer_features(x.user_id, reviewer_reviews)) for x in reviews]\n",
    "  return StandardScaler().fit_transform(reviewer_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviewer_features, reviews_by_reviewer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def get_features(predictor_features, fit=False):\n",
    "  predictor_features_text = [x.review_content for x in predictor_features]\n",
    "  bow = tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_features_text, fit)\n",
    "  reviewer_scaled = scaled_reviewer_features(predictor_features)\n",
    "  return hstack([coo_matrix(reviewer_scaled), bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try adding some more features, this time for review details like date posted, the id of the reviewer and the id of the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "from datetime import datetime as dt\n",
    "def extract_date_ordinals(reviews):\n",
    "  return [dt.strptime(x.date, '%Y-%m-%d').date().toordinal() for x in reviews]\n",
    "\n",
    "def get_features(predictor_features, fit=False):\n",
    "  predictor_features_text = [x.review_content for x in predictor_features]\n",
    "  bow = tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_features_text, fit)\n",
    "  reviewer_scaled = scaled_reviewer_features(predictor_features)\n",
    "\n",
    "  date_ordinals = extract_date_ordinals(predictor_features)\n",
    "  review_details = StandardScaler().fit_transform([[date_ordinals[i], predictor_features[i].user_id, predictor_features[i].product_id] for i in range(len(predictor_features))])\n",
    "    \n",
    "  return hstack([coo_matrix(reviewer_scaled), coo_matrix(review_details), bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this gave us a boost. I also tried adding a number of other features derived from these, but it seems that only the raw features would give me an increase in performance. These include sentiment, POS tags and structural features. Now let's try adding bigrams, which are known to help with improving accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niallwalsh/miniconda3/envs/lucas/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import sentiment_features, find_words\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "count_vectorizer.set_params(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bigrams gives us a tiny boost, and now we're closing in on a good statistical benchmark. Let's switch to SVC as we should have more tweakable options. We have to drop some of our metrics based on probabilities because the underlying implementation of LinearSVC does not expose them to us. We will disable dual because our number of samples exceeds our number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validate(model, X, y, get_features_fn, cv=5):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv)\n",
    "  metrics = { \"accuracies\": [], \"auroc\": [], \"f1 scores\": [] }\n",
    "    \n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    scores = model.decision_function(test_X)\n",
    "    \n",
    "    predicted = [1 if score >= 0 else 0 for score in scores]\n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(roc_auc_score(test_y, scores))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(LinearSVC(max_iter=2500, dual=False), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the accuracy is pretty similar, but a little lower. Now let's try automatic tuning of hyperparameters. Since this will be tuning our parameters towards a set, we will use a dev set for this, and have a test set to check how the tuned model works on totally unseen data. A major parameter to grid search on is C, the penalty parameter of the error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train_dev_set = all_reviews[:-5000]\n",
    "train_dev_y = y[:-5000]\n",
    "\n",
    "test_set = all_reviews[-5000:]\n",
    "test_set_y = y[-5000:]\n",
    "\n",
    "model = LinearSVC(max_iter=2500, dual=False)\n",
    "\n",
    "grid_search = GridSearchCV(cv=5, estimator=model, param_grid={\"C\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.3, 0.7, 1.0, 10.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(get_features(train_dev_set), train_dev_y).best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search has now found a 'best' value for C. Now let's try this value with our SVM to see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([10.10894036, 12.58361363, 11.71966481, 13.25194311, 13.75658846,\n",
       "        11.85658526, 11.91894174, 11.76314402, 12.6587317 , 12.27675748]),\n",
       " 'score_time': array([0.01081014, 0.01152873, 0.01058412, 0.01410556, 0.01031327,\n",
       "        0.01303434, 0.00918007, 0.01168156, 0.01051784, 0.01208258]),\n",
       " 'test_score': array([0.73857162, 0.75029813, 0.74135418, 0.74347423, 0.74464984,\n",
       "        0.74783012, 0.74007818, 0.74836017, 0.74286093, 0.74973496]),\n",
       " 'train_score': array([0.75967874, 0.75869228, 0.75879534, 0.75923704, 0.75892226,\n",
       "        0.75897379, 0.75981302, 0.7586278 , 0.75936396, 0.75885042])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(LinearSVC(max_iter=2500, dual=False, C=0.025), get_features(train_dev_set), train_dev_y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive, this is our best validation accuracy yet! Now let's run it on an unseen section of data, something that has been neglected for far too long at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(max_iter=2500, dual=False, C=0.025)\n",
    "model.fit(get_features(train_dev_set), train_dev_y)\n",
    "model.score(get_features(test_set), test_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great accuracy, better than Stanfords results with the SVM. Using grid search for this task proved to be very useful. Now let's take a look at a couple of other parameters. NuSVC, a proposed 'fix' to the 'c' parameter of the classic SVC, has been proven in one of our previous notebooks to be quite a lot better than the normal SVC. Let's give that a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8298f33cf42d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNuSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dev_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dev_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-15be0520afb1>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(predictor_features, fit)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mpredictor_features_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictor_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_features_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mreviewer_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_reviewer_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-15371a54a609>\u001b[0m in \u001b[0;36mtf_idf_bag_of_words\u001b[0;34m(cv, tfidf, predictor_set, fit)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lucas/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "model = NuSVC(kernel='sigmoid', gamma='scale')\n",
    "model.fit(get_features(train_dev_set), train_dev_y)\n",
    "model.score(get_features(test_set), test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lucas]",
   "language": "python",
   "name": "conda-env-lucas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
