{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deeper look at SVM architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last experiment that we ran, where we used a LinearSVM without changing any of the deafult parameters or feaure engineering, we achieved an accuracy of 63%. It also gave us convergence warnings for the linear kernel.\n",
    "In this notebook we will iterate over the SVM design and try different approaches to the problem using this classifier.\n",
    "Let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_set, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = reviews_set[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x.review_content for x in all_reviews]\n",
    "y = [1 if x.label else 0 for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with Logistic Regression\n",
    "\n",
    "\n",
    "~~## No AUROC or F1 Score with LinearSVC~~\n",
    "\n",
    "~~Since LinearSVC uses LibLinear it is not possible to retrieve the probabilities to calculate the AUROC or F1 score. The SVC implementations in sklearn have an option to return probabilities which can be used to calculate the AUC, however Liblinear does not provide this. It is possible to use CalibratedClassifierCV to obtain probabilities, however this also tries to tune the hyperparameters and severely limits the amount of data we can ue. If we want to use LinearSVC we will have to leave this out for now. It is also possible to use SVC with a linear kernel, but the LinearSVM is known to perform better.~~\n",
    "\n",
    "~~Instead of this, I will use Logistic Regression which is very similar to LinearSVC, and performs similarly. Logistic regression will allow me to view these metrics and once I have explored how to improve this I can switch back to LinearSVC. Both classifiers attempt to divide the samples, and logistic regression is known to do a good job at producing a wide margin in it's division, which is what SVMs try to do.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc_score_from_probabilities(probabilities, labels):\n",
    "  true_probabilities = [probabilities[i][1] for i in range(0, len(labels))]\n",
    "  return roc_auc_score(labels, true_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_uses_correct_probabilities (__main__.TestRocScoreFromProbabilities) ... /home/stefan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Please use assertEqual instead.\n",
      "  \n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f425daecb70>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestRocScoreFromProbabilities(unittest.TestCase):\n",
    "  \n",
    "  def test_uses_correct_probabilities(self):\n",
    "    probabilities = [[0.9, 0.1], [0.1, 0.9]]\n",
    "    labels = [0, 1]\n",
    "    self.assertEquals(1, auroc_score_from_probabilities(probabilities, labels))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to automate the process of cross validation and finding the accuracy, the mean and the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "\n",
    "def run_cross_validate(model, X, y, cv=5):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "    \"auroc\": [],\n",
    "    \"f1 scores\": [],\n",
    "    \"log loss\": []\n",
    "  }\n",
    "    \n",
    "  false_negatives = 0\n",
    "  false_positives = 0\n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = [X[x] for x in train_indices]\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = [X[x] for x in test_indices]\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    predicted = [0 if x[0] > x[1] else 1 for x in probabilities]\n",
    "    for i in range(0, len(predicted)):\n",
    "      if (predicted[i] == 0 and test_y[i] == 1):\n",
    "        false_negatives+=1\n",
    "      if (predicted[i] == 1 and test_y[i] == 0):\n",
    "        false_positives+=1\n",
    "    \n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(auroc_score_from_probabilities(probabilities, test_y))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "    metrics[\"log loss\"].append(log_loss(test_y, probabilities))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  metrics[\"mean log loss\"] = np.mean(metrics[\"log loss\"])\n",
    "  metrics[\"% false negatives\"] = (false_negatives / cv) / num_samples\n",
    "  metrics[\"% false positives\"] = (false_positives / cv) / num_samples\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = Pipeline([\n",
    "  ('cv', CountVectorizer()),\n",
    "  ('classifier', LogisticRegression(solver=\"liblinear\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6335916020994752,\n",
       "  0.6163459135216196,\n",
       "  0.62775,\n",
       "  0.6186546636659165,\n",
       "  0.6244061015253813],\n",
       " 'auroc': [0.6717081872765418,\n",
       "  0.6616381399886253,\n",
       "  0.6740033891220084,\n",
       "  0.6594462348431714,\n",
       "  0.6690801197038021],\n",
       " 'f1 scores': [0.6465766634522662,\n",
       "  0.6273367322165574,\n",
       "  0.6393799951562121,\n",
       "  0.6326186461093712,\n",
       "  0.6366715045960329],\n",
       " 'log loss': [0.842830348638973,\n",
       "  0.8295839561356689,\n",
       "  0.8308771414279466,\n",
       "  0.8298968201343846,\n",
       "  0.8273542307425769],\n",
       " 'mean accuracy': 0.6241496561624785,\n",
       " 'mean variance': 3.865438495834681e-05,\n",
       " 'mean auroc': 0.6671752141868298,\n",
       " 'mean f1 scores': 0.6365167083060881,\n",
       " 'mean log loss': 0.83210849941591,\n",
       " '% false negatives': 0.0336,\n",
       " '% false positives': 0.041569999999999996}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cross_validate(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to evaluate\n",
    "\n",
    "### Accuracy\n",
    "This is the percentage of correct classifications. The higher the better, however it is not appropriate in all cases.\n",
    "\n",
    "This metric falls to the imbalanced classification problem. When there are many more of one class, the classifier can choose it much more, or all the time, to achieve a high accuracy.\n",
    "\n",
    "### False Positives, False Negatives\n",
    "False positives are samples that were classified as fake, but were in fact genuine. False negatives are samples that were classified as genuine, but were in fact fake. We want to reduce both of these values as much as possible. True positives and true negatives are the adverse values which we want to maximise, although they correlate to false positives and negatives so there's no need to include them here.\n",
    "\n",
    "One important question here is **False negatives vs False positives**? Is it worse to falsely suggest that something is fake, or is it worse to falsely suggest that something is genuine? Probably in this system a human might be paid to read those suspicious reviews. It would be good to catch all the fake reviews, plus some genuine ones, because this is just like filtering the huge number of reviews to make a human's job easier. In this case it is better to reduce false negatives. If humans are not reviewing the system, then this would be a different situation, it would probably be better to reduce false positives.\n",
    "\n",
    "### Recall\n",
    "Of all the fake reviews, what percentage were identified as fake? This is not subject to the imbalanced\n",
    "classification problem. We aim to maximise it as an indication of how well we are really identifying our fake reviews. \n",
    "\n",
    "We cannot focus solely on recall, because we could identify all reviews as fake and achieve 100% recall. Precision must be included in the consideration.\n",
    "\n",
    "### Precision\n",
    "Of all the reviews identified as fake, what percentage are actually fake? If we classify all reviews as fake, then our precision will be low. If we classify all reviews as genuine, then we wont have any precision either.\n",
    "\n",
    "In our case it might be more important to have a high recall, if we don't want to miss any fake reviews. Otherwise if we want to be as accurate as possible we can balance recall and precision.\n",
    "\n",
    "### F1 Score\n",
    "This is a harmonic mean of precision and recall. Because of this it punishes extreme values such as a recall of or a precision of 0.0\n",
    "\n",
    "This also acts as a single number metric representing precision and recall.\n",
    "\n",
    "### Area Under Curve (AUC) \n",
    "This gives us a measure of discrimination, how well we correctly classify both classes. This does not use a 'Yes' or 'No' which can make it more interesting than accuracy.\n",
    "\n",
    "At different classification thresholds, how well do we predict fake reviews as 'more fake' than genuine reviews. We plot the true positive rate against the false positive rate to get a graph. Changing the threshold allows us to create a graph because at low thresholds we will have more fake reviews, increasing the true positives rate. Decreasing the treshold means we will have less genuine reviews, decreasing the true negative rate, which therefore increases the false positive rate.\n",
    "\n",
    "An AUC of 0.8 means the chance of the model distinguishing positive and negative classes is 80%.\n",
    "\n",
    "### Mean Squared Error\n",
    "The average of the square difference between the original values and the predicted values. Adds focus to large errors, and is easier to compute than mean absolute error.\n",
    "\n",
    "The closer the mean squared error is to zero the better. It incorporates the variance and the bias of the estimator\n",
    "\n",
    "### Logarithmic loss\n",
    "This takes into account the uncertainty of a prediction, taking into account how far away from the actual label the prediction is. As the probability approaches correct the log loss reduces only very little. As the probability approaches incorrect the log loss increases rapidly. This means that confident incorrect values are highly penalized.\n",
    "\n",
    "We aim to minimize log loss.\n",
    "\n",
    "### Cohen's Kappa\n",
    "A reliability metric used when there is more than one classifier. Computes an agreement percentage of the used classifiers. It is out of scope at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Results\n",
    "\n",
    "Our results show quite normal results. The AUROC score and F1 score above are as expected, reflective of similar work in this field. We can see that the variance is not very large, so we can depend on our scores.\n",
    "\n",
    "Since there is nothing alarming, the next thing to consider is improving features or trying to find more predictive features. We might see better results if convert our bow to tfidf. ~~We might see better results if we preprocess our text to lemmatize and remove stopwords. Since bag of words is our main feature here, this should hopefully be influential. In this case we are removing all of the stopwords, which may be a bad idea. We can't know for sure unless we experiment.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6690827293176705,\n",
       "  0.6510872281929517,\n",
       "  0.66775,\n",
       "  0.6546636659164791,\n",
       "  0.6619154788697175],\n",
       " 'auroc': [0.722337793250552,\n",
       "  0.7113747879740162,\n",
       "  0.7300764077506791,\n",
       "  0.7134590236596106,\n",
       "  0.7257707107888505],\n",
       " 'f1 scores': [0.6659939455095862,\n",
       "  0.6478304742684157,\n",
       "  0.666164280331575,\n",
       "  0.6546636659164792,\n",
       "  0.6551020408163266],\n",
       " 'log loss': [0.6156789102877588,\n",
       "  0.6212551438499597,\n",
       "  0.6076363719266309,\n",
       "  0.6188976615989417,\n",
       "  0.6118785210913428],\n",
       " 'mean accuracy': 0.6608998204593638,\n",
       " 'mean variance': 5.001862191968383e-05,\n",
       " 'mean auroc': 0.7206037446847416,\n",
       " 'mean f1 scores': 0.6579508813684766,\n",
       " 'mean log loss': 0.6150693217509268,\n",
       " '% false negatives': 0.03419,\n",
       " '% false positives': 0.03363}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "model = Pipeline([\n",
    "  ('cv', count_vectorizer),\n",
    "  ('tfidf', TfidfTransformer()),\n",
    "  ('classifier', LogisticRegression(solver=\"liblinear\"))\n",
    "])\n",
    "run_cross_validate(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like switching to Tfidf had a positive effect on our results, which is not unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words, preprocess_words\n",
    "def preprocess(review_content): # Not adding bigrams yet\n",
    "  return \" \".join(preprocess_words(find_words(review_content), bigrams=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All I wanted was a quick and healthy dinner.  When I walked in there were 2 staff members and no customers at all.  I ordered a \"designer\" shrimp salad but asked to sub the dressing.  I looked up to read the dressing menu, decided on chilled avocado out loud and turned to see he'd walked out of sight. After a minute he came back and slowly put the items in a bowl leaving me ample time to see some selections were a bit dried out, others also looked unappealing, I asked to add black olives but they don't have any. I told him 3 times which dressing I'd like on top.       Second salad I asked for a build your own and he asked me over and over what I'd said (though I only said 2 items at a time).  As he slowly put it together I tallied up the salads add ons and felt it was too pricey for what I was getting; a ton of lettuce and def less than a full serving of turkey.     Then I waited at the counter to pay from the girl who didn't seem to know how to work the register.  She asked for help and we waited four full minutes for a manager to come over.  I had been standing for more than 20 minutes waiting when I realized he had put avocado dressing on both salads and the person ordering it couldn't eat avocado.  I explained, hers was remade.  After 30 minutes I left only to get home and realize I was over charged for 2 $10 premium salads + $3.50 for turkey + 3 add ons and a can of coke running me $27.  At home I found they had given me the mistake salad and the fixed one meaning 2 turkey salads for the price of two shrimp salads!  I had a minor melt down due to low blood sugar levels and frustration and the sharp headache that developed from my wasted effort and wound up scrounging left overs I didn't want in the first place so that I didn't go back and kill someone over a salad.     I could have made my own FRESH salad the way I wanted it at home for a couple of dollars and in less time than they made it so I won't be going back, except to return the salad I never ordered and get my money back.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'want quick healthi dinner when walk staff member custom order design shrimp salad ask dress look read dress menu decid chill avocado loud turn walk sight after minut come slowli item bowl leav ampl time select dri look unapp ask black oliv tell time dress like second salad ask build ask say say item time slowli talli salad felt pricey get lettuc serv turkey then wait counter girl know work regist ask help wait minut manag come stand minut wait realiz avocado dress salad person order avocado explain remak after minut leav home realiz charg premium salad turkey coke run home give mistak salad fix mean turkey salad price shrimp salad minor melt blood sugar level frustrat sharp headach develop wast effort wind scroung leav over want place kill salad fresh salad want home coupl dollar time go return salad order money'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_content = all_reviews[0].review_content\n",
    "print(review_content)\n",
    "preprocess(review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lemmatized = [preprocess(x.review_content) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for lemmatized words with all stopwords and all words with <= 3 characters removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6285928517870533,\n",
       "  0.6183454136465883,\n",
       "  0.62775,\n",
       "  0.6286571642910728,\n",
       "  0.6311577894473619],\n",
       " 'auroc': [0.6680352775253957,\n",
       "  0.6601132271229335,\n",
       "  0.6667615034141229,\n",
       "  0.6678853578957584,\n",
       "  0.6720064294268845],\n",
       " 'f1 scores': [0.6438159156279961,\n",
       "  0.6312484907027288,\n",
       "  0.6360303104375459,\n",
       "  0.6463443677065968,\n",
       "  0.6451768101996632],\n",
       " 'log loss': [0.7759444849708552,\n",
       "  0.776865274259934,\n",
       "  0.7745017827134671,\n",
       "  0.7678159432315894,\n",
       "  0.7639525810825661],\n",
       " 'mean accuracy': 0.6269006438344153,\n",
       " 'mean variance': 1.9597118020436023e-05,\n",
       " 'mean auroc': 0.666960359077019,\n",
       " 'mean f1 scores': 0.6405231789349062,\n",
       " 'mean log loss': 0.7718160132516825,\n",
       " '% false negatives': 0.03293,\n",
       " '% false positives': 0.04169}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cross_validate(model, X_lemmatized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows only a slightly better result. Perhaps the different versions of words people use are actually important, and perhaps stopwords are important here too. At least more important than other tasks, for example identifying sentiment or topic. Let's try lemmatizing, but without removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review_content): # Not adding bigrams yet\n",
    "  return \" \".join(preprocess_words(find_words(review_content), bigrams=False, stopwords=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content = all_reviews[0].review_content\n",
    "print(review_content)\n",
    "preprocess(review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_helpers.get_accuracy(model, [preprocess(x.review_content) for x in all_reviews], y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
