{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deeper look at SVM architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last experiment that we ran, where we used a LinearSVM without changing any of the deafult parameters or feaure engineering, we achieved an accuracy of 63%. It also gave us convergence warnings for the linear kernel.\n",
    "In this notebook we will iterate over the SVM design and try different approaches to the problem using this classifier.\n",
    "Let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_set, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = reviews_set[:20000]\n",
    "\n",
    "X = [x.review_content for x in all_reviews]\n",
    "y = [1 if x.label else 0 for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with Logistic Regression\n",
    "\n",
    "\n",
    "~~## No AUROC or F1 Score with LinearSVC~~\n",
    "\n",
    "~~Since LinearSVC uses LibLinear it is not possible to retrieve the probabilities to calculate the AUROC or F1 score. The SVC implementations in sklearn have an option to return probabilities which can be used to calculate the AUC, however Liblinear does not provide this. It is possible to use CalibratedClassifierCV to obtain probabilities, however this also tries to tune the hyperparameters and severely limits the amount of data we can ue. If we want to use LinearSVC we will have to leave this out for now. It is also possible to use SVC with a linear kernel, but the LinearSVM is known to perform better.~~\n",
    "\n",
    "~~Instead of this, I will use Logistic Regression which is very similar to LinearSVC, and performs similarly. Logistic regression will allow me to view these metrics and once I have explored how to improve this I can switch back to LinearSVC. Both classifiers attempt to divide the samples, and logistic regression is known to do a good job at producing a wide margin in it's division, which is what SVMs try to do.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc_score_from_probabilities(probabilities, labels):\n",
    "  true_probabilities = [probabilities[i][1] for i in range(0, len(labels))]\n",
    "  return roc_auc_score(labels, true_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_uses_correct_probabilities (__main__.TestRocScoreFromProbabilities) ... /home/stefan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Please use assertEqual instead.\n",
      "  \n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f5b66394d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestRocScoreFromProbabilities(unittest.TestCase):\n",
    "  \n",
    "  def test_uses_correct_probabilities(self):\n",
    "    probabilities = [[0.9, 0.1], [0.1, 0.9]]\n",
    "    labels = [0, 1]\n",
    "    self.assertEquals(1, auroc_score_from_probabilities(probabilities, labels))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to automate the process of cross validation and finding the accuracy, the mean and the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "def run_cross_validate(model, X, y, get_features_fn, cv=5):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "    \"auroc\": [],\n",
    "    \"f1 scores\": [],\n",
    "    \"log loss\": []\n",
    "  }\n",
    "    \n",
    "  false_negatives = 0\n",
    "  false_positives = 0\n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    predicted = [0 if x[0] > x[1] else 1 for x in probabilities]\n",
    "    for i in range(0, len(predicted)):\n",
    "      if (predicted[i] == 0 and test_y[i] == 1):\n",
    "        false_negatives+=1\n",
    "      if (predicted[i] == 1 and test_y[i] == 0):\n",
    "        false_positives+=1\n",
    "    \n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(auroc_score_from_probabilities(probabilities, test_y))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "    metrics[\"log loss\"].append(log_loss(test_y, probabilities))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  metrics[\"mean log loss\"] = np.mean(metrics[\"log loss\"])\n",
    "  metrics[\"false negatives rate\"] = (false_negatives / cv) / num_samples\n",
    "  metrics[\"false positives rate\"] = (false_positives / cv) / num_samples\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6298425393651587,\n",
       "  0.6223444138965258,\n",
       "  0.62375,\n",
       "  0.6246561640410102,\n",
       "  0.6266566641660415],\n",
       " 'auroc': [0.674034894913445,\n",
       "  0.6668230945534108,\n",
       "  0.6716134280834111,\n",
       "  0.6671050696325319,\n",
       "  0.6693794205076535],\n",
       " 'f1 scores': [0.6502951593860685,\n",
       "  0.639809296781883,\n",
       "  0.6402103753287114,\n",
       "  0.6427041180671269,\n",
       "  0.6392848514133849],\n",
       " 'log loss': [0.8279193869383008,\n",
       "  0.8392649040012382,\n",
       "  0.8308313138743578,\n",
       "  0.8469880476800975,\n",
       "  0.828149195756153],\n",
       " 'mean accuracy': 0.6254499562937472,\n",
       " 'mean variance': 6.783056210080569e-06,\n",
       " 'mean auroc': 0.6697911815380906,\n",
       " 'mean f1 scores': 0.642460760195435,\n",
       " 'mean log loss': 0.8346305696500295,\n",
       " 'false negatives rate': 0.03331,\n",
       " 'false positives rate': 0.0416}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cVec = CountVectorizer()\n",
    "def get_features(predictor_set, fit=False):\n",
    "  if fit:\n",
    "    return cVec.fit_transform(predictor_set)\n",
    "  return cVec.transform(predictor_set)\n",
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to evaluate\n",
    "\n",
    "### Accuracy\n",
    "This is the percentage of correct classifications. The higher the better, however it is not appropriate in all cases.\n",
    "\n",
    "This metric falls to the imbalanced classification problem. When there are many more of one class, the classifier can choose it much more, or all the time, to achieve a high accuracy.\n",
    "\n",
    "### False Positives, False Negatives\n",
    "False positives are samples that were classified as fake, but were in fact genuine. False negatives are samples that were classified as genuine, but were in fact fake. We want to reduce both of these values as much as possible. True positives and true negatives are the adverse values which we want to maximise, although they correlate to false positives and negatives so there's no need to include them here.\n",
    "\n",
    "One important question here is **False negatives vs False positives**? Is it worse to falsely suggest that something is fake, or is it worse to falsely suggest that something is genuine? Probably in this system a human might be paid to read those suspicious reviews. It would be good to catch all the fake reviews, plus some genuine ones, because this is just like filtering the huge number of reviews to make a human's job easier. In this case it is better to reduce false negatives. If humans are not reviewing the system, then this would be a different situation, it would probably be better to reduce false positives.\n",
    "\n",
    "### Recall\n",
    "Of all the fake reviews, what percentage were identified as fake? This is not subject to the imbalanced\n",
    "classification problem. We aim to maximise it as an indication of how well we are really identifying our fake reviews. \n",
    "\n",
    "We cannot focus solely on recall, because we could identify all reviews as fake and achieve 100% recall. Precision must be included in the consideration.\n",
    "\n",
    "### Precision\n",
    "Of all the reviews identified as fake, what percentage are actually fake? If we classify all reviews as fake, then our precision will be low. If we classify all reviews as genuine, then we wont have any precision either.\n",
    "\n",
    "In our case it might be more important to have a high recall, if we don't want to miss any fake reviews. Otherwise if we want to be as accurate as possible we can balance recall and precision.\n",
    "\n",
    "### F1 Score\n",
    "This is a harmonic mean of precision and recall. Because of this it punishes extreme values such as a recall of or a precision of 0.0\n",
    "\n",
    "This also acts as a single number metric representing precision and recall.\n",
    "\n",
    "### Area Under Curve (AUC) \n",
    "This gives us a measure of discrimination, how well we correctly classify both classes. This does not use a 'Yes' or 'No' which can make it more interesting than accuracy.\n",
    "\n",
    "At different classification thresholds, how well do we predict fake reviews as 'more fake' than genuine reviews. We plot the true positive rate against the false positive rate to get a graph. Changing the threshold allows us to create a graph because at low thresholds we will have more fake reviews, increasing the true positives rate. Decreasing the treshold means we will have less genuine reviews, decreasing the true negative rate, which therefore increases the false positive rate.\n",
    "\n",
    "An AUC of 0.8 means the chance of the model distinguishing positive and negative classes is 80%.\n",
    "\n",
    "### Mean Squared Error\n",
    "The average of the square difference between the original values and the predicted values. Adds focus to large errors, and is easier to compute than mean absolute error.\n",
    "\n",
    "The closer the mean squared error is to zero the better. It incorporates the variance and the bias of the estimator\n",
    "\n",
    "### Logarithmic loss\n",
    "This takes into account the uncertainty of a prediction, taking into account how far away from the actual label the prediction is. As the probability approaches correct the log loss reduces only very little. As the probability approaches incorrect the log loss increases rapidly. This means that confident incorrect values are highly penalized.\n",
    "\n",
    "We aim to minimize log loss.\n",
    "\n",
    "### Cohen's Kappa\n",
    "A reliability metric used when there is more than one classifier. Computes an agreement percentage of the used classifiers. It is out of scope at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Results\n",
    "\n",
    "Our results show quite normal results. The AUROC score and F1 score above are as expected, reflective of similar work in this field. We can see that the variance is not very large, so we can depend on our scores.\n",
    "\n",
    "Since there is nothing alarming, the next thing to consider is improving features or trying to find more predictive features. We might see better results if convert our bow to tfidf. ~~We might see better results if we preprocess our text to lemmatize and remove stopwords. Since bag of words is our main feature here, this should hopefully be influential. In this case we are removing all of the stopwords, which may be a bad idea. We can't know for sure unless we experiment.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6628342914271432,\n",
       "  0.6585853536615845,\n",
       "  0.6715,\n",
       "  0.6624156039009752,\n",
       "  0.6609152288072018],\n",
       " 'auroc': [0.7180413579339924,\n",
       "  0.7220066549320763,\n",
       "  0.7318238456584436,\n",
       "  0.7173215112945878,\n",
       "  0.7198641317670225],\n",
       " 'f1 scores': [0.6678158089140607,\n",
       "  0.659351620947631,\n",
       "  0.6755555555555556,\n",
       "  0.6666666666666666,\n",
       "  0.6618453865336659],\n",
       " 'log loss': [0.615781638358499,\n",
       "  0.6121420811627103,\n",
       "  0.6065944124939086,\n",
       "  0.6172954348356238,\n",
       "  0.6146133744730216],\n",
       " 'mean accuracy': 0.663250095559381,\n",
       " 'mean variance': 1.922832248143387e-05,\n",
       " 'mean auroc': 0.7218115003172245,\n",
       " 'mean f1 scores': 0.666247007723516,\n",
       " 'mean log loss': 0.6132853882647528,\n",
       " 'false negatives rate': 0.033389999999999996,\n",
       " 'false positives rate': 0.033960000000000004}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def get_features(predictor_features, fit=False):\n",
    "  if fit:\n",
    "    return tfidf_transformer.fit_transform(count_vectorizer.fit_transform(predictor_features))\n",
    "  else:\n",
    "    return tfidf_transformer.transform(count_vectorizer.transform(predictor_features))\n",
    "\n",
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like switching to Tfidf had a positive effect on our results, which is not unexpected. Let's have a look at our vocabulary, to see what words are actually in our bag of words. Let's try to clean our features more, hopefully a smaller number of features, more concentrated with quality words will improve accuracy. Currently our bag of words has the following shape: ~~Let's be adventurous and try using a bag of words that only includes stopwords, on the hypothesis that it is the way people say things that is predictive of a fake review, rather than what they talk about.~~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 33772)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a low limit for the min document frequency. This cleans out everything from the typos to the random jibberish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 11548)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=4)\n",
    "count_vectorizer.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6628342914271432,\n",
       "  0.6585853536615845,\n",
       "  0.6715,\n",
       "  0.6624156039009752,\n",
       "  0.6609152288072018],\n",
       " 'auroc': [0.7180413579339924,\n",
       "  0.7220066549320763,\n",
       "  0.7318238456584436,\n",
       "  0.7173215112945878,\n",
       "  0.7198641317670225],\n",
       " 'f1 scores': [0.6678158089140607,\n",
       "  0.659351620947631,\n",
       "  0.6755555555555556,\n",
       "  0.6666666666666666,\n",
       "  0.6618453865336659],\n",
       " 'log loss': [0.615781638358499,\n",
       "  0.6121420811627103,\n",
       "  0.6065944124939086,\n",
       "  0.6172954348356238,\n",
       "  0.6146133744730216],\n",
       " 'mean accuracy': 0.663250095559381,\n",
       " 'mean variance': 1.922832248143387e-05,\n",
       " 'mean auroc': 0.7218115003172245,\n",
       " 'mean f1 scores': 0.666247007723516,\n",
       " 'mean log loss': 0.6132853882647528,\n",
       " 'false negatives rate': 0.033389999999999996,\n",
       " 'false positives rate': 0.033960000000000004}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try adding reviewer features, which have been proven to be predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6518370407398151,\n",
       "  0.6425893526618346,\n",
       "  0.64775,\n",
       "  0.6524131032758189,\n",
       "  0.6509127281820455],\n",
       " 'auroc': [0.7053698245109004,\n",
       "  0.7013279378206647,\n",
       "  0.7032494419799112,\n",
       "  0.7002054107163762,\n",
       "  0.7096352433961907],\n",
       " 'f1 scores': [0.6528781460254175,\n",
       "  0.6405228758169934,\n",
       "  0.644640605296343,\n",
       "  0.6599804305283757,\n",
       "  0.6511744127936032],\n",
       " 'log loss': [0.6252394261910932,\n",
       "  0.6304959625433099,\n",
       "  0.6293422524297301,\n",
       "  0.6293933869266951,\n",
       "  0.6227539646349712],\n",
       " 'mean accuracy': 0.6491004449719029,\n",
       " 'mean variance': 1.3193011312318335e-05,\n",
       " 'mean auroc': 0.7039575716848085,\n",
       " 'mean f1 scores': 0.6498392940921466,\n",
       " 'mean log loss': 0.6274449985451599,\n",
       " 'false negatives rate': 0.035480000000000005,\n",
       " 'false positives rate': 0.0347}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviewer_features, reviews_by_reviewer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=100, min_df=4)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def get_features(predictor_features, fit=False):\n",
    "  bow = None\n",
    "  predictor_features_text = [x.review_content for x in predictor_features]\n",
    "  if fit:\n",
    "    bow = tfidf_transformer.fit_transform(count_vectorizer.fit_transform(predictor_features_text))\n",
    "  else:\n",
    "    bow = tfidf_transformer.transform(count_vectorizer.transform(predictor_features_text))\n",
    "\n",
    "  reviewer_reviews = reviews_by_reviewer(predictor_features)\n",
    "  reviewer_predictors = [list(reviewer_features(x.user_id, reviewer_reviews)) for x in predictor_features]\n",
    "  reviewer_scaled = StandardScaler().fit_transform(reviewer_predictors)\n",
    "  return hstack([coo_matrix(reviewer_scaled), bow])\n",
    "\n",
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although reviewer features are predictive alone, here they are doing very little to increase the accuracy. In fact, our AUROC is worse! Let's try switching to LinearSVC now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from exp2_feature_extraction import find_words\n",
    "x_words = [find_words(x) for x in X]\n",
    "\n",
    "count_vectorizer.fit_transform([\" \".join([y for y in x if y not in string.punctuation]) for x in x_words]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "count_vectorizer.set_params(vocabulary=None, stop_words=None)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "model = Pipeline([\n",
    "  ('cv', count_vectorizer),\n",
    "  ('tfidf', TfidfTransformer()),\n",
    "  ('classifier', LogisticRegression(solver=\"liblinear\"))\n",
    "])\n",
    "run_cross_validate(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words, preprocess_words\n",
    "def preprocess(review_content): # Not adding bigrams yet\n",
    "  return \" \".join(preprocess_words(find_words(review_content), bigrams=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content = all_reviews[0].review_content\n",
    "print(review_content)\n",
    "preprocess(review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lemmatized = [preprocess(x.review_content) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for lemmatized words with all stopwords and all words with <= 3 characters removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(model, X_lemmatized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows only a slightly better result. Perhaps the different versions of words people use are actually important, and perhaps stopwords are important here too. At least more important than other tasks, for example identifying sentiment or topic. Let's try lemmatizing, but without removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review_content): # Not adding bigrams yet\n",
    "  return \" \".join(preprocess_words(find_words(review_content), bigrams=False, stopwords=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content = all_reviews[0].review_content\n",
    "print(review_content)\n",
    "preprocess(review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_helpers.get_accuracy(model, [preprocess(x.review_content) for x in all_reviews], y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
