{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deeper look at SVM architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last experiment that we ran, where we used a LinearSVM without changing any of the deafult parameters or feaure engineering, we achieved an accuracy of 63%. It also gave us convergence warnings for the linear kernel.\n",
    "We expect that SVMs should be more capable than this, so in this notebook we will iterate over the SVM design and try different approaches to the problem using this classifier. The recommended SVM kernel for text classification is the linear kernel, so we will attempt to obtain a result from this.\n",
    "Let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from exp4_data_feature_extraction import get_balanced_dataset\n",
    "from scripts import training_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_set, fake_reviews, genuine_reviews, unused_genuine_reviews = get_balanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = reviews_set\n",
    "\n",
    "X = [x.review_content for x in all_reviews]\n",
    "y = [1 if x.label else 0 for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with Logistic Regression\n",
    "Logistic Regression is simpler and faster than an SVM. It also correlates well with SVMs with a linear kernel. Both attempt to divide the samples. SVMs attempt to produce a wide margin in the division, and althugh logistic regression does not do this by design, it is known to produce results of this nature. Logistic regression is also a little easier to obtain metrics for, as we can also view the log loss metric. Because of all of this we will start with logistic regression to do our initial experimentation, and then switch to SVMs at the end when we are ready to change the penalty variable.\n",
    "\n",
    "First are some helper functions to get the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc_score_from_probabilities(probabilities, labels):\n",
    "  true_probabilities = [probabilities[i][1] for i in range(0, len(labels))]\n",
    "  return roc_auc_score(labels, true_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_uses_correct_probabilities (__main__.TestRocScoreFromProbabilities) ... /home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/ipykernel/__main__.py:8: DeprecationWarning: Please use assertEqual instead.\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f885ddb6be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestRocScoreFromProbabilities(unittest.TestCase):\n",
    "  \n",
    "  def test_uses_correct_probabilities(self):\n",
    "    probabilities = [[0.9, 0.1], [0.1, 0.9]]\n",
    "    labels = [0, 1]\n",
    "    self.assertEquals(1, auroc_score_from_probabilities(probabilities, labels))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to automate the process of cross validation and finding the accuracy, the mean and the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "def run_cross_validate(model, X, y, get_features_fn, cv=5):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "    \"auroc\": [],\n",
    "    \"f1 scores\": [],\n",
    "    \"log loss\": []\n",
    "  }\n",
    "    \n",
    "  false_negatives = 0\n",
    "  false_positives = 0\n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    predicted = [0 if x[0] > x[1] else 1 for x in probabilities]\n",
    "    for i in range(0, len(predicted)):\n",
    "      if (predicted[i] == 0 and test_y[i] == 1):\n",
    "        false_negatives+=1\n",
    "      if (predicted[i] == 1 and test_y[i] == 0):\n",
    "        false_positives+=1\n",
    "    \n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(auroc_score_from_probabilities(probabilities, test_y))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "    metrics[\"log loss\"].append(log_loss(test_y, probabilities))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  metrics[\"mean log loss\"] = np.mean(metrics[\"log loss\"])\n",
    "  metrics[\"false negatives rate\"] = (false_negatives / cv) / num_samples\n",
    "  metrics[\"false positives rate\"] = (false_positives / cv) / num_samples\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_cross_validate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-05a6a8b2b119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcVec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcVec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrun_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_cross_validate' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cVec = CountVectorizer()\n",
    "def get_features(predictor_set, fit=False):\n",
    "  if fit:\n",
    "    return cVec.fit_transform(predictor_set)\n",
    "  return cVec.transform(predictor_set)\n",
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to evaluate\n",
    "\n",
    "### Accuracy\n",
    "This is the percentage of correct classifications. The higher the better, however it is not appropriate in all cases.\n",
    "\n",
    "This metric falls to the imbalanced classification problem. When there are many more of one class, the classifier can choose it much more, or all the time, to achieve a high accuracy.\n",
    "\n",
    "### False Positives, False Negatives\n",
    "False positives are samples that were classified as fake, but were in fact genuine. False negatives are samples that were classified as genuine, but were in fact fake. We want to reduce both of these values as much as possible. True positives and true negatives are the adverse values which we want to maximise, although they correlate to false positives and negatives so there's no need to include them here.\n",
    "\n",
    "One important question here is **False negatives vs False positives**? Is it worse to falsely suggest that something is fake, or is it worse to falsely suggest that something is genuine? Probably in this system a human might be paid to read those suspicious reviews. It would be good to catch all the fake reviews, plus some genuine ones, because this is just like filtering the huge number of reviews to make a human's job easier. In this case it is better to reduce false negatives. If humans are not reviewing the system, then this would be a different situation, it would probably be better to reduce false positives.\n",
    "\n",
    "### Recall\n",
    "Of all the fake reviews, what percentage were identified as fake? This is not subject to the imbalanced\n",
    "classification problem. We aim to maximise it as an indication of how well we are really identifying our fake reviews. \n",
    "\n",
    "We cannot focus solely on recall, because we could identify all reviews as fake and achieve 100% recall. Precision must be included in the consideration.\n",
    "\n",
    "### Precision\n",
    "Of all the reviews identified as fake, what percentage are actually fake? If we classify all reviews as fake, then our precision will be low. If we classify all reviews as genuine, then we wont have any precision either.\n",
    "\n",
    "In our case it might be more important to have a high recall, if we don't want to miss any fake reviews. Otherwise if we want to be as accurate as possible we can balance recall and precision.\n",
    "\n",
    "### F1 Score\n",
    "This is a harmonic mean of precision and recall. Because of this it punishes extreme values such as a recall of or a precision of 0.0\n",
    "\n",
    "This also acts as a single number metric representing precision and recall.\n",
    "\n",
    "### Area Under Curve (AUC) \n",
    "This gives us a measure of discrimination, how well we correctly classify both classes. This does not use a 'Yes' or 'No' which can make it more interesting than accuracy.\n",
    "\n",
    "At different classification thresholds, how well do we predict fake reviews as 'more fake' than genuine reviews. We plot the true positive rate against the false positive rate to get a graph. Changing the threshold allows us to create a graph because at low thresholds we will have more fake reviews, increasing the true positives rate. Decreasing the treshold means we will have less genuine reviews, decreasing the true negative rate, which therefore increases the false positive rate.\n",
    "\n",
    "An AUC of 0.8 means the chance of the model distinguishing positive and negative classes is 80%.\n",
    "\n",
    "### Mean Squared Error\n",
    "The average of the square difference between the original values and the predicted values. Adds focus to large errors, and is easier to compute than mean absolute error.\n",
    "\n",
    "The closer the mean squared error is to zero the better. It incorporates the variance and the bias of the estimator\n",
    "\n",
    "### Logarithmic loss\n",
    "This takes into account the uncertainty of a prediction, taking into account how far away from the actual label the prediction is. As the probability approaches correct the log loss reduces only very little. As the probability approaches incorrect the log loss increases rapidly. This means that confident incorrect values are highly penalized.\n",
    "\n",
    "We aim to minimize log loss.\n",
    "\n",
    "### Cohen's Kappa\n",
    "A reliability metric used when there is more than one classifier. Computes an agreement percentage of the used classifiers. It is out of scope at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Results\n",
    "\n",
    "Our results show quite normal results. The AUROC score and F1 score above are as expected, reflective of similar work in this field. We can see that the variance is not very large, so we can depend on our scores.\n",
    "\n",
    "Since there is nothing alarming, the next thing to consider is improving features or trying to find more predictive features. We might see better results if convert our bow to tfidf. This is especially important because the normalisation function of logistic regression uses regularisation that depends on the features being scaled. Tfidf will scale our features for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_bag_of_words(cv, tfidf, predictor_set, fit):\n",
    "  if fit:\n",
    "    return tfidf.fit_transform(cv.fit_transform(predictor_set))\n",
    "  else:\n",
    "    return tfidf.transform(cv.transform(predictor_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9607172850f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_idf_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def get_features(predictor_features, fit=False):\n",
    "  return tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_set, fit)\n",
    "\n",
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like switching to Tfidf had a positive effect on our results, which is not unexpected. Let's have a look at our vocabulary, to see what words are actually in our bag of words. Let's try to clean our features more, hopefully a smaller number of features, more concentrated with quality words will improve accuracy. Currently our bag of words has the following shape: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 33653)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a low limit for the min document frequency. This cleans out everything from the typos to the random jibberish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 11533)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=4)\n",
    "count_vectorizer.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.6518370407398151,\n",
       "  0.6550862284428893,\n",
       "  0.66525,\n",
       "  0.6559139784946236,\n",
       "  0.6636659164791198],\n",
       " 'auroc': [0.71634402750229,\n",
       "  0.7159391043564783,\n",
       "  0.7199524948931135,\n",
       "  0.7156260114447396,\n",
       "  0.7168274889857265],\n",
       " 'f1 scores': [0.6573185731857318,\n",
       "  0.6563745019920318,\n",
       "  0.6681536555142503,\n",
       "  0.6575410652065704,\n",
       "  0.6624843161856964],\n",
       " 'log loss': [0.6160163377452522,\n",
       "  0.6176232360162895,\n",
       "  0.6149526840588501,\n",
       "  0.6183856677609213,\n",
       "  0.6180224249695648],\n",
       " 'mean accuracy': 0.6583506328312895,\n",
       " 'mean variance': 2.6974801977417865e-05,\n",
       " 'mean auroc': 0.7169378254364696,\n",
       " 'mean f1 scores': 0.6603744224168562,\n",
       " 'mean log loss': 0.6170000701101755,\n",
       " 'false negatives rate': 0.0337,\n",
       " 'false positives rate': 0.03463}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cross_validate(classifier, X, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try adding reviewer features, which have been proven to be predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scaled_reviewer_features(reviews):\n",
    "  reviewer_reviews = reviews_by_reviewer(reviews)\n",
    "  reviewer_predictors = [list(reviewer_features(x.user_id, reviewer_reviews)) for x in reviews]\n",
    "  return StandardScaler().fit_transform(reviewer_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-386c157b2665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewer_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"liblinear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviewer_features, reviews_by_reviewer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=1000, min_df=4)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def get_features(predictor_features, fit=False):\n",
    "  predictor_features_text = [x.review_content for x in predictor_features]\n",
    "  bow = tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_features_text, fit)\n",
    "  reviewer_scaled = scaled_reviewer_features(predictor_features)\n",
    "  return hstack([coo_matrix(reviewer_scaled), bow])\n",
    "\n",
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try adding some more features, this time for review details like date posted, the id of the reviewer and the id of the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9b6902c3bc5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewer_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrun_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"liblinear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=1000, min_df=4)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "from datetime import datetime as dt\n",
    "def extract_date_ordinals(reviews):\n",
    "  return [dt.strptime(x.date, '%Y-%m-%d').date().toordinal() for x in reviews]\n",
    "\n",
    "def get_features(predictor_features, fit=False):\n",
    "  predictor_features_text = [x.review_content for x in predictor_features]\n",
    "  bow = tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_features_text, fit)\n",
    "  reviewer_scaled = scaled_reviewer_features(predictor_features)\n",
    "\n",
    "  date_ordinals = extract_date_ordinals(predictor_features)\n",
    "  review_details = StandardScaler().fit_transform([[date_ordinals[i], predictor_features[i].user_id, predictor_features[i].product_id] for i in range(len(predictor_features))])\n",
    "    \n",
    "  return hstack([coo_matrix(reviewer_scaled), coo_matrix(review_details), bow])\n",
    "\n",
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this gave us a boost. Now let's try adding bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.7207344351932398,\n",
       "  0.7172150246994128,\n",
       "  0.7180451127819549,\n",
       "  0.719753930280246,\n",
       "  0.7177654881004163],\n",
       " 'auroc': [0.7941542649374345,\n",
       "  0.7921109198984456,\n",
       "  0.7930493710459263,\n",
       "  0.7926693562443541,\n",
       "  0.791249333855305],\n",
       " 'f1 scores': [0.7200124591185173,\n",
       "  0.7167486151739592,\n",
       "  0.7184562404988677,\n",
       "  0.7193877551020409,\n",
       "  0.718098311817279],\n",
       " 'mean accuracy': 0.7187027982110539,\n",
       " 'mean variance': 1.7513995686725878e-06,\n",
       " 'mean auroc': 0.792646649196293,\n",
       " 'mean f1 scores': 0.7185406763421328}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import sentiment_features, find_words\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=10000, min_df=4, ngram_range=(1, 2))\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "def get_features(predictor_features, fit=False):\n",
    "  predictor_features_text = [x.review_content for x in predictor_features]\n",
    "  bow = tf_idf_bag_of_words(count_vectorizer, tfidf_transformer, predictor_features_text, fit)\n",
    "  reviewer_scaled = scaled_reviewer_features(predictor_features)\n",
    "\n",
    "  date_ordinals = extract_date_ordinals(predictor_features)\n",
    "  \n",
    "  review_details = [[date_ordinals[i], predictor_features[i].user_id, predictor_features[i].product_id] for i in range(len(predictor_features))]\n",
    "  review_scaled = StandardScaler().fit_transform(review_details)\n",
    "\n",
    "  return hstack([coo_matrix(reviewer_scaled), coo_matrix(review_scaled), bow])\n",
    "\n",
    "run_cross_validate(LogisticRegression(solver=\"liblinear\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bigrams gives us a tiny boost, and now we're closing in on a good statistical benchmark. Let's switch to SVC as we should have more tweakable options. We have to drop some of our metrics based on probabilities because the underlying implementation of LinearSVC does not expose them to us. We will disable dual because our number of samples exceeds our number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validate(model, X, y, get_features_fn, cv=5):\n",
    "  skfSplitter = StratifiedKFold(n_splits=cv)\n",
    "  metrics = { \"accuracies\": [], \"auroc\": [], \"f1 scores\": [] }\n",
    "    \n",
    "  for train_indices, test_indices in skfSplitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    scores = model.decision_function(test_X)\n",
    "    \n",
    "    predicted = [1 if score >= 0 else 0 for score in scores]\n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(roc_auc_score(test_y, scores))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.7110413818814465,\n",
       "  0.7094168453102184,\n",
       "  0.7098117193810973,\n",
       "  0.7104020381532343,\n",
       "  0.7101845522898155],\n",
       " 'auroc': [0.783419792835397,\n",
       "  0.7821408596582322,\n",
       "  0.7823409564325181,\n",
       "  0.7830343720488236,\n",
       "  0.7806408518058535],\n",
       " 'f1 scores': [0.7117131078944922,\n",
       "  0.7090371752994246,\n",
       "  0.71113997649533,\n",
       "  0.7115223917551298,\n",
       "  0.7109389525875426],\n",
       " 'mean accuracy': 0.7101713074031626,\n",
       " 'mean variance': 3.017916598229258e-07,\n",
       " 'mean auroc': 0.7823153665561648,\n",
       " 'mean f1 scores': 0.7108703208063838}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cross_validate(LinearSVC(max_iter=2500, dual=False), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the accuracy is pretty similar, but a little lower. Now let's try automatic tuning of hyperparameters. Since this will be tuning our parameters towards a set, we will use a dev set for this, and have a test set to check how the tuned model works on totally unseen data. A major parameter to grid search on is C, the penalty parameter of the error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.025}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train_dev_set = all_reviews[:-5000]\n",
    "train_dev_y = y[:-5000]\n",
    "\n",
    "model = LinearSVC(max_iter=2500, dual=False)\n",
    "\n",
    "grid_search = GridSearchCV(cv=5, estimator=model, param_grid={\"C\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.3, 0.7, 1.0, 10.0]})\n",
    "grid_search.fit(get_features(train_dev_set), train_dev_y).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([7.38575029, 8.26084542, 7.40343952]),\n",
       " 'score_time': array([0.03568912, 0.03565645, 0.02719092]),\n",
       " 'test_score': array([0.74381962, 0.74452162, 0.74346345]),\n",
       " 'train_score': array([0.76037478, 0.7602832 , 0.76128362])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(LinearSVC(max_iter=2500, dual=False, C=0.025), get_features(train_dev_set), train_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracies': [0.721604324593016,\n",
       "  0.7199179793084164,\n",
       "  0.7190393338718698,\n",
       "  0.7199092773255452,\n",
       "  0.7161188094202449],\n",
       " 'auroc': [0.7923979350902117,\n",
       "  0.7927542076308028,\n",
       "  0.794105893125735,\n",
       "  0.792860062528262,\n",
       "  0.790102489819773],\n",
       " 'f1 scores': [0.7223203495398346,\n",
       "  0.7208373331681789,\n",
       "  0.7202474864655839,\n",
       "  0.7224360355922289,\n",
       "  0.7169893139228743],\n",
       " 'mean accuracy': 0.7193179449038184,\n",
       " 'mean variance': 3.2498678704489683e-06,\n",
       " 'mean auroc': 0.792444117638957,\n",
       " 'mean f1 scores': 0.7205661037377401}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cross_validate(LinearSVC(max_iter=2500, dual=False, C=0.1), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "run_cross_validate(NuSVC(kernel=\"sigmoid\", gamma=\"scale\"), all_reviews, y, get_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from exp2_feature_extraction import find_words\n",
    "x_words = [find_words(x) for x in X]\n",
    "\n",
    "count_vectorizer.fit_transform([\" \".join([y for y in x if y not in string.punctuation]) for x in x_words]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "count_vectorizer.set_params(vocabulary=None, stop_words=None)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "model = Pipeline([\n",
    "  ('cv', count_vectorizer),\n",
    "  ('tfidf', TfidfTransformer()),\n",
    "  ('classifier', LogisticRegression(solver=\"liblinear\"))\n",
    "])\n",
    "run_cross_validate(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words, preprocess_words\n",
    "def preprocess(review_content): # Not adding bigrams yet\n",
    "  return \" \".join(preprocess_words(find_words(review_content), bigrams=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content = all_reviews[0].review_content\n",
    "print(review_content)\n",
    "preprocess(review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lemmatized = [preprocess(x.review_content) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for lemmatized words with all stopwords and all words with <= 3 characters removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validate(model, X_lemmatized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows only a slightly better result. Perhaps the different versions of words people use are actually important, and perhaps stopwords are important here too. At least more important than other tasks, for example identifying sentiment or topic. Let's try lemmatizing, but without removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review_content): # Not adding bigrams yet\n",
    "  return \" \".join(preprocess_words(find_words(review_content), bigrams=False, stopwords=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content = all_reviews[0].review_content\n",
    "print(review_content)\n",
    "preprocess(review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_helpers.get_accuracy(model, [preprocess(x.review_content) for x in all_reviews], y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
