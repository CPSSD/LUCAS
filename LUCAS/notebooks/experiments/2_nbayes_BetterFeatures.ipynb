{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "This notebook has been heavily unit tested, and as a result a lot of code has been removed from the notebook itself. I have demonstrated as much as possible through importing units and running them on example values.\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpNYC\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words\n",
    "from sklearn.utils import shuffle\n",
    "all_reviews = [(x, find_words(x.review_content)) for x in shuffle(review_set.reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are:\n",
    "* Length of the review\n",
    "* Average word length\n",
    "* Number of sentences\n",
    "* Average sentence length\n",
    "* Percentage of numerals\n",
    "* percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 5.5, 2, 26.0, 0.25, 0.25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import structural_features\n",
    "\n",
    "review = review_pb2.Review()\n",
    "review.review_content = \"1 very horrible restaurant. Eat 10 Starbucks instead.\"\n",
    "structural_features((review, [\"1\", \"very\", \"horrible\", \"restaurant\", \"Eat\", \"10\", \"Starbucks\", \"instead\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_structural = [structural_features(x) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, '0.3333333333333333'),\n",
       " (11, '0.3333333333333333'),\n",
       " (35, '0.3333333333333333')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from exp2_feature_extraction import pos_features\n",
    "\n",
    "sample_pos_features = pos_features([\"Dog\", \"and\", \"beautiful\"], nltk)\n",
    "[(i, str(sample_pos_features[i])) for i in range(0, len(sample_pos_features)) if sample_pos_features[i] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 5 is JJ, which is for adjective. This represents \"beautiful\" in the input. 11 is NNP, which is proper noun, which represents \"Dog\". 35 is CC which is coordinating conjuction, which is \"and\". These are given as percentages, where each here is one third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pos = [pos_features(x[1], nltk) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "none: 0.0\n",
      "good: 0.4404\n",
      "bad: -0.5423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "print(\"Scores:\")\n",
    "print(\"none:\", sentiment_analyzer.polarity_scores(\"none\")[\"compound\"])\n",
    "print(\"good:\", sentiment_analyzer.polarity_scores(\"good\")[\"compound\"])\n",
    "print(\"bad:\", sentiment_analyzer.polarity_scores(\"bad\")[\"compound\"])\n",
    "\n",
    "from exp2_feature_extraction import sentiment_features\n",
    "sentiment_features([\"good\", \"good\", \"none\", \"bad\"], sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sentiment = [sentiment_features(x[1], sentiment_analyzer) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the topic model features from LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some data cleaning, which will improve our results when generating LDA topics. This cleaning is stemming and lemmatization, and removing words with three or less characters.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming will reduce a word to a 'stem' form, which can be used to normalise words that mean the same thing. For example 'cleanly' and 'cleanest' would be stemmed to 'clean'. Lemmatization uses a vocabulary and morphological analysis to more intelligently normalise words, for example 'car' and 'automobile' could go to 'vehicle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gonna', 'stay', 'coffe', 'littl', 'store', 'happili', 'surpris']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import preprocess_words\n",
    "preprocess_words([\"I\", \"was\", \"gonna\", \"just\", \"stay\", \"in\", \"for\", \"coffee\", \"but\", \"this\", \"little\", \"store\", \"happily\", \"surprised\", \"me\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['food', 'order', 'wait']\n",
      "1: ['food', 'place', 'great']\n",
      "2: ['flavor', 'dish', 'like']\n",
      "3: ['dessert', 'pasta', 'restaur']\n",
      "4: ['burger', 'fri', 'like']\n",
      "5: ['brunch', 'good', 'egg']\n",
      "6: ['place', 'great', 'drink']\n",
      "7: ['pizza', 'place', 'good']\n",
      "8: ['chicken', 'sauc', 'good']\n",
      "9: ['ramen', 'pork', 'noodl']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def get_topic_features_maker(reviews):\n",
    "  num_topics = 10\n",
    "  preprocessed_words = [preprocess_words(x[1]) for x in reviews]\n",
    "  \n",
    "  dictionary = gensim.corpora.Dictionary(preprocessed_words)\n",
    "  dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "  bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_words]\n",
    "  lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=2)\n",
    "    \n",
    "  for index, topic in lda_model.show_topics(formatted=False, num_words=3):\n",
    "    print('{}: {}'.format(index, [w[0] for w in topic]))\n",
    "  \n",
    "  def make_topic_features(review_words):\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess_words(review_words)))\n",
    "    return topic_features(topics, num_topics)\n",
    "  return make_topic_features\n",
    "\n",
    "topic_features_maker = get_topic_features_maker(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02, 0.020000432, 0.02, 0.54804873, 0.02, 0.02, 0.02, 0.2919508, 0.02, 0.02]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import topic_features\n",
    "\n",
    "words = [\"Pizza\", \"Pasta\", \"Italian\", \"Spaghetti\"]\n",
    "topic_features_maker(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topic = [topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes the maximum number of reviews in a day, average review length, standard deviation of reviewer's ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "reviews_reviewer_map = reviews_by_reviewer([x[0] for x in all_reviews])\n",
    "features_reviewer = [reviewer_features(x[0], reviews_reviewer_map) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import structural_features\n",
    "\n",
    "def features_row(review, reviews_reviewer_map, sentiment_analyzer, pos_tagger, make_topic_features):\n",
    "  words = find_words(review.review_content)\n",
    "  row = list(structural_features(review))\n",
    "  row += list(sentiment_features(words, sentiment_analyzer))\n",
    "  row += pos_features(words, pos_tagger)\n",
    "  row += make_topic_features(words)\n",
    "  row += list(reviewer_features(review, reviews_reviewer_map))\n",
    "  return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I include a test to check my features are generated correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Review' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e4fc9800e4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtest_make_topic_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_reviews_reviewer_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentiment_analyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_make_topic_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Structural features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0c8695d2da0e>\u001b[0m in \u001b[0;36mfeatures_row\u001b[0;34m(review, reviews_reviewer_map, sentiment_analyzer, pos_tagger, make_topic_features)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeatures_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews_reviewer_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_topic_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructural_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpos_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Amazoff/LUCAS/notebooks/experiments/exp2_feature_extraction.py\u001b[0m in \u001b[0;36mstructural_features\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstructural_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mreview_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0mlength_of_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Review' object does not support indexing"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "review = review_pb2.Review()\n",
    "review.review_content = \"1 really horrible restaurant. Drink 10 Starbucks instead.\"\n",
    "review.user_id = 1\n",
    "review.date = \"2011-07-28\"\n",
    "review.rating = 5\n",
    "\n",
    "review2 = review_pb2.Review()\n",
    "review2.review_content = \"example\"\n",
    "review2.user_id = 1\n",
    "review2.date = \"2011-07-28\"\n",
    "review2.rating = 4\n",
    "\n",
    "test_reviews_reviewer_map = {\n",
    "    1: [review, review2]\n",
    "}\n",
    "\n",
    "def analyze_sentiment(word):\n",
    "  score = 0.0\n",
    "  if word == \"1\":\n",
    "    score = 0.1\n",
    "  if word in [\"horrible\", \"instead\"]:\n",
    "    score = -0.5\n",
    "  return { \"compound\": score }\n",
    "\n",
    "test_sentiment_analyzer = Mock()\n",
    "test_sentiment_analyzer.polarity_scores = analyze_sentiment\n",
    "\n",
    "def tag(words):\n",
    "  tag_map = {\n",
    "    \"really\": \"CD\", \"horrible\": \"DT\", \"restaurant\": \"CD\", \"Drink\": \"FW\", \"Starbucks\": \"JJ\"\n",
    "  }\n",
    "  return [(x, tag_map[x]) for x in [y for y in words if y in tag_map] if x]\n",
    "\n",
    "test_pos_tagger = Mock()\n",
    "test_pos_tagger.pos_tag = tag\n",
    "\n",
    "test_make_topic_features = lambda x: [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "row = features_row(review, test_reviews_reviewer_map, test_sentiment_analyzer, test_pos_tagger, test_make_topic_features)\n",
    "\n",
    "# Structural features\n",
    "assert row[0] == 57\n",
    "assert row[1] == 6\n",
    "assert row[2] == 2\n",
    "assert row[3] == 28.0\n",
    "assert row[4] == 0.25\n",
    "assert row[5] == 0.25\n",
    "# Sentiment features\n",
    "assert row[6] == 0.125 # 1/8\n",
    "assert row[7] == 0.25  # 2/8\n",
    "# POS features\n",
    "assert row[8:44] == [0.4, 0.2, 0.0, 0.2, 0.0, 0.2] + [0.0] * 30\n",
    "# Topic features (This is not testing much as it doesn't use the real thing)\n",
    "assert row[44:49] == [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# Reviewer features\n",
    "assert row[49] == 2\n",
    "assert row[50] == 32\n",
    "assert float(\"%0.2f\"%row[51]**2) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "predictor_features = hstack([coo_matrix(features_structural), coo_matrix(features_sentiment), coo_matrix(features_pos),\n",
    "                             coo_matrix(features_topic), coo_matrix(features_reviewer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x[0].label for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.27180743, 0.27057576, 0.26438594, 0.27046919, 0.24466586,\n",
       "        0.24403763, 0.24148178, 0.24270821, 0.24425125, 0.24098802]),\n",
       " 'score_time': array([0.01063466, 0.01147199, 0.01149797, 0.01017594, 0.01002407,\n",
       "        0.01007032, 0.0102942 , 0.01008558, 0.00996137, 0.01010466]),\n",
       " 'test_score': array([0.5306077 , 0.53459032, 0.53158247, 0.53411686, 0.53523088,\n",
       "        0.5343267 , 0.530149  , 0.53111074, 0.53704323, 0.52774064])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
