{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "data_file_path = 'data/yelpNYC'\n",
    "from protos import review_set_pb2, review_pb2\n",
    "import nltk\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(data_file_path, 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are length of the review, average word length, number of sentences, average sentence length, percentage of numerals, percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_avg_token_length, find_numerals_ratio\n",
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "\n",
    "find_words = lambda text: nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def structural_features(review):\n",
    "  review_content = review.review_content\n",
    "  length_of_review = len(review_content)\n",
    "  words = find_words(review_content)\n",
    "  avg_word_length = find_avg_token_length(words)\n",
    "  sentences = nltk.tokenize.sent_tokenize(review_content)\n",
    "  sentence_length_of_review = len(sentences)\n",
    "  avg_sentence_length = find_avg_token_length(sentences)\n",
    "  numerals_ratio = find_numerals_ratio(words)\n",
    "  capitalised_word_ratio = find_capitalised_word_ratio(words)\n",
    "  return (length_of_review, avg_word_length, sentence_length_of_review,\n",
    "          avg_sentence_length, numerals_ratio, capitalised_word_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = lambda text: nltk.pos_tag(words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import functools\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_features(review):\n",
    "  polarities = []\n",
    "  num_positive = 0\n",
    "  num_negative = 0\n",
    "  for word in find_words(review.review_content):\n",
    "    polarity = sentiment_analyzer.polarity_scores(word)['compound']\n",
    "    polarities.append(1 if polarity > 0 else -1 if polarity < 0 else 0)\n",
    "  reduce_func = lambda c, p: (c[0] + 1, c[1]) if p > 0 else (c[0], c[1] + 1) if p < 0 else (c[0], c[1])\n",
    "  num_positive, num_negative = functools.reduce(reduce_func, polarities, (0, 0))\n",
    "  total = num_positive + num_negative\n",
    "  if total == 0:\n",
    "    return (0, 0)\n",
    "  return (num_positive / total, num_negative / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the topic model features from LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = review_set.reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "reviews_words = []\n",
    "for review in all_reviews:\n",
    "  reviews_words.append(find_words(review.review_content))\n",
    "dictionary = gensim.corpora.Dictionary(reviews_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in reviews_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=10, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_features(review):\n",
    "  words = find_words(review.review_content)\n",
    "  bow = dictionary.doc2bow(words)\n",
    "  topics = lda_model.get_document_topics(bow)\n",
    "  t = [0] * 10\n",
    "  for topic in topics:\n",
    "    t[topic[0]] = topic[1]\n",
    "  return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes the maximum number of reviews in a day, average review length, standard deviation of reviewer's ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "from sklearn.utils import shuffle\n",
    "reviews = shuffle(all_reviews)\n",
    "reviews_reviewer_map = reviews_by_reviewer(reviews)\n",
    "\n",
    "predictor_features = []\n",
    "for review in reviews:\n",
    "  next_entry = []\n",
    "  next_entry += list(structural_features(review))\n",
    "  next_entry += list(sentiment_features(review))\n",
    "  next_entry += topic_features(review)\n",
    "  next_entry += list(reviewer_features(review, reviews_reviewer_map))\n",
    "  predictor_features.append(next_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix, vstack\n",
    "def format_row(features_row):\n",
    "  return coo_matrix(features_row)\n",
    "\n",
    "new_predictor_features = vstack([format_row(x) for x in predictor_features[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.74030423, 0.72387052, 0.70630813, 0.70531082, 0.70363092,\n",
       "        0.70707583, 0.71101546, 0.70721221, 0.70555878, 0.71357942]),\n",
       " 'score_time': array([0.07638741, 0.07003665, 0.06944442, 0.06952405, 0.07006812,\n",
       "        0.06943226, 0.0694654 , 0.06953883, 0.06995726, 0.06973457]),\n",
       " 'test_score': array([0.56856793, 0.57330251, 0.5556453 , 0.56294213, 0.56528157,\n",
       "        0.57276145, 0.55217936, 0.56667781, 0.53860294, 0.54417335])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=10, return_train_score=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
