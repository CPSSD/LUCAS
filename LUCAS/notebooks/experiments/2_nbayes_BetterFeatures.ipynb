{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "This notebook has been heavily unit tested, and as a result a lot of code has been removed from the notebook itself. I have demonstrated as much as possible through importing units and running them on example values.\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from protos import review_set_pb2, review_pb2\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(\"data/yelpNYC\", 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_words\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "num_each_class = 8141\n",
    "reviews = shuffle(review_set.reviews)\n",
    "\n",
    "i = 0\n",
    "fake_reviews = []\n",
    "for x in reviews:\n",
    "  if i == num_each_class:\n",
    "    break\n",
    "  if x.label:\n",
    "    fake_reviews.append(x)\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "genuine_reviews = []\n",
    "for x in reviews:\n",
    "  if i == num_each_class:\n",
    "    break\n",
    "  if x.label == False:\n",
    "    fake_reviews.append(x)\n",
    "    i+=1\n",
    "    \n",
    "all_reviews = [(x, find_words(x.review_content)) for x in shuffle(fake_reviews + genuine_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16282\n"
     ]
    }
   ],
   "source": [
    "print(len(all_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are:\n",
    "* Length of the review\n",
    "* Average word length\n",
    "* Number of sentences\n",
    "* Average sentence length\n",
    "* Percentage of numerals\n",
    "* percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 5.5, 2, 26.0, 0.25, 0.25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import structural_features\n",
    "\n",
    "review = review_pb2.Review()\n",
    "review.review_content = \"1 very horrible restaurant. Eat 10 Starbucks instead.\"\n",
    "structural_features((review, [\"1\", \"very\", \"horrible\", \"restaurant\", \"Eat\", \"10\", \"Starbucks\", \"instead\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_structural = [structural_features(x) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, '0.3333333333333333'),\n",
       " (11, '0.3333333333333333'),\n",
       " (35, '0.3333333333333333')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from exp2_feature_extraction import pos_features\n",
    "\n",
    "sample_pos_features = pos_features([\"Dog\", \"and\", \"beautiful\"], nltk)\n",
    "[(i, str(sample_pos_features[i])) for i in range(0, len(sample_pos_features)) if sample_pos_features[i] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 5 is JJ, which is for adjective. This represents \"beautiful\" in the input. 11 is NNP, which is proper noun, which represents \"Dog\". 35 is CC which is coordinating conjuction, which is \"and\". These are given as percentages, where each here is one third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pos = [pos_features(x[1], nltk) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features. These will be:\n",
    "* Percentage of words that have positive sentiment\n",
    "* Percentage of words that have negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "none: 0.0\n",
      "good: 0.4404\n",
      "bad: -0.5423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "print(\"Scores:\")\n",
    "print(\"none:\", sentiment_analyzer.polarity_scores(\"none\")[\"compound\"])\n",
    "print(\"good:\", sentiment_analyzer.polarity_scores(\"good\")[\"compound\"])\n",
    "print(\"bad:\", sentiment_analyzer.polarity_scores(\"bad\")[\"compound\"])\n",
    "\n",
    "from exp2_feature_extraction import sentiment_features\n",
    "sentiment_features([\"good\", \"good\", \"none\", \"bad\"], sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sentiment = [sentiment_features(x[1], sentiment_analyzer) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the topic model features from LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some data cleaning, which will improve our results when generating LDA topics. This cleaning is stemming and lemmatization, and removing words with three or less characters.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming will reduce a word to a 'stem' form, which can be used to normalise words that mean the same thing. For example 'cleanly' and 'cleanest' would be stemmed to 'clean'. Lemmatization uses a vocabulary and morphological analysis to more intelligently normalise words, for example 'car' and 'automobile' could go to 'vehicle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gonna', 'stay', 'coffe', 'littl', 'store', 'happili', 'surpris']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import preprocess_words\n",
    "preprocess_words([\"I\", \"was\", \"gonna\", \"just\", \"stay\", \"in\", \"for\", \"coffee\", \"but\", \"this\", \"little\", \"store\", \"happily\", \"surprised\", \"me\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/lucas/lib/python3.7/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def get_topic_features_maker(reviews, num_topics=100, bigrams=False):\n",
    "  preprocessed_words = [preprocess_words(x[1], bigrams=bigrams) for x in reviews]\n",
    "  \n",
    "  dictionary = gensim.corpora.Dictionary(preprocessed_words)\n",
    "  dictionary.filter_extremes(no_below=15, no_above=0.33, keep_n=100000)\n",
    "  bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_words]\n",
    "  lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=2)\n",
    "    \n",
    "  def make_topic_features(review_words):\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess_words(review_words, bigrams=bigrams)))\n",
    "    return topic_features(topics, num_topics)\n",
    "\n",
    "  def get_terms(topic_id):\n",
    "    return [dictionary.id2token[x[0]] + \" \" + str(x[1]) for x in lda_model.get_topic_terms(topic_id)]\n",
    "  return (make_topic_features, get_terms)\n",
    "\n",
    "topic_features_maker, get_topic_terms = get_topic_features_maker(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ramen 0.14306363',\n",
       "  'noodl 0.10816208',\n",
       "  'pork 0.05333299',\n",
       "  'broth 0.04551034',\n",
       "  'wait 0.043994926',\n",
       "  'bun 0.038505413',\n",
       "  'spici 0.030381061',\n",
       "  'bowl 0.029212855',\n",
       "  'soup 0.023365302',\n",
       "  'extra 0.019488957'],\n",
       " ['pizza 0.3116736',\n",
       "  'slice 0.050003514',\n",
       "  'crust 0.04422086',\n",
       "  'best 0.028866129',\n",
       "  'chees 0.028013052',\n",
       "  'fresh 0.018963119',\n",
       "  'sauc 0.018711366',\n",
       "  'top 0.018625498',\n",
       "  'brooklyn 0.01526527',\n",
       "  'mozzarella 0.014776899'],\n",
       " ['pasta 0.16058289',\n",
       "  'restaur 0.05215843',\n",
       "  'dish 0.050591186',\n",
       "  'italian 0.028142707',\n",
       "  'veget 0.026763672',\n",
       "  'both 0.02527639',\n",
       "  'order 0.022927662',\n",
       "  'veal 0.022089723',\n",
       "  'ragu 0.016029848',\n",
       "  'nick 0.01597658']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import topic_features\n",
    "\n",
    "words = [\"Pizza\", \"Pasta\", \"Ramen\", \"Noodles\"]\n",
    "tf = topic_features_maker(words)\n",
    "[get_topic_terms(i) for i in range(0, len(tf)) if tf[i] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unigram_topic = [topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_topic_features_maker, get_topic_terms2 = get_topic_features_maker(all_reviews, bigrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"Fried Chicken\", \"Pizza Slice\"]\n",
    "tf2 = bigram_topic_features_maker(words)\n",
    "print(tf2)\n",
    "[get_topic_terms2(i) for i in range(0, len(tf2)) if tf2[i] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bigram_topic = [bigram_topic_features_maker(x[1]) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes:\n",
    "* The maximum number of reviews in a day\n",
    "* average review length\n",
    "* standard deviation of reviewer's ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6.0, 1.4142135623730951, 0.5, 0.5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp2_feature_extraction import reviewer_features\n",
    "\n",
    "review1 = review_pb2.Review()\n",
    "review1.review_content=\"1234\"\n",
    "review1.date=\"2018-11-12\"\n",
    "review1.rating=2\n",
    "\n",
    "review2 = review_pb2.Review()\n",
    "review2.review_content=\"12345678\"\n",
    "review2.date=\"2018-11-12\"\n",
    "review2.rating=4\n",
    "\n",
    "user_id = 1234\n",
    "reviewer_map = {\n",
    "    user_id: [review1, review2]\n",
    "}\n",
    "\n",
    "reviewer_features(user_id, reviewer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, our first feature is the number of reviews on this day (2), the average review length (6.0) and the standard devation of our ratings (1.41...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "reviews_reviewer_map = reviews_by_reviewer([x[0] for x in all_reviews])\n",
    "features_reviewer = [reviewer_features(x[0].user_id, reviews_reviewer_map) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "We normalise all our features to be between one and zero. We need to do this to suppress the mega features vs tiny features situation. Most classifiers use Euclidian distance, which has no knowledge of the units being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_row(review, reviews_reviewer_map, sentiment_analyzer, pos_tagger, make_topic_features):\n",
    "  words = find_words(review.review_content)\n",
    "  row = list(structural_features(review))\n",
    "  row += list(sentiment_features(words, sentiment_analyzer))\n",
    "  row += pos_features(words, pos_tagger)\n",
    "  row += make_topic_features(words)\n",
    "  row += list(reviewer_features(review, reviews_reviewer_map))\n",
    "  return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I include a test to check my features are generated correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Review' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e4fc9800e4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtest_make_topic_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_reviews_reviewer_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentiment_analyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_make_topic_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Structural features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6b47f4784bd2>\u001b[0m in \u001b[0;36mfeatures_row\u001b[0;34m(review, reviews_reviewer_map, sentiment_analyzer, pos_tagger, make_topic_features)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeatures_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews_reviewer_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_topic_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructural_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpos_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Amazoff/LUCAS/notebooks/experiments/exp2_feature_extraction.py\u001b[0m in \u001b[0;36mstructural_features\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstructural_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mreview_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0mlength_of_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Review' object does not support indexing"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "review = review_pb2.Review()\n",
    "review.review_content = \"1 really horrible restaurant. Drink 10 Starbucks instead.\"\n",
    "review.user_id = 1\n",
    "review.date = \"2011-07-28\"\n",
    "review.rating = 5\n",
    "\n",
    "review2 = review_pb2.Review()\n",
    "review2.review_content = \"example\"\n",
    "review2.user_id = 1\n",
    "review2.date = \"2011-07-28\"\n",
    "review2.rating = 4\n",
    "\n",
    "test_reviews_reviewer_map = {\n",
    "    1: [review, review2]\n",
    "}\n",
    "\n",
    "def analyze_sentiment(word):\n",
    "  score = 0.0\n",
    "  if word == \"1\":\n",
    "    score = 0.1\n",
    "  if word in [\"horrible\", \"instead\"]:\n",
    "    score = -0.5\n",
    "  return { \"compound\": score }\n",
    "\n",
    "test_sentiment_analyzer = Mock()\n",
    "test_sentiment_analyzer.polarity_scores = analyze_sentiment\n",
    "\n",
    "def tag(words):\n",
    "  tag_map = {\n",
    "    \"really\": \"CD\", \"horrible\": \"DT\", \"restaurant\": \"CD\", \"Drink\": \"FW\", \"Starbucks\": \"JJ\"\n",
    "  }\n",
    "  return [(x, tag_map[x]) for x in [y for y in words if y in tag_map] if x]\n",
    "\n",
    "test_pos_tagger = Mock()\n",
    "test_pos_tagger.pos_tag = tag\n",
    "\n",
    "test_make_topic_features = lambda x: [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "row = features_row(review, test_reviews_reviewer_map, test_sentiment_analyzer, test_pos_tagger, test_make_topic_features)\n",
    "\n",
    "# Structural features\n",
    "assert row[0] == 57\n",
    "assert row[1] == 6\n",
    "assert row[2] == 2\n",
    "assert row[3] == 28.0\n",
    "assert row[4] == 0.25\n",
    "assert row[5] == 0.25\n",
    "# Sentiment features\n",
    "assert row[6] == 0.125 # 1/8\n",
    "assert row[7] == 0.25  # 2/8\n",
    "# POS features\n",
    "assert row[8:44] == [0.4, 0.2, 0.0, 0.2, 0.0, 0.2] + [0.0] * 30\n",
    "# Topic features (This is not testing much as it doesn't use the real thing)\n",
    "assert row[44:49] == [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# Reviewer features\n",
    "assert row[49] == 2\n",
    "assert row[50] == 32\n",
    "assert float(\"%0.2f\"%row[51]**2) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "predictor_features = hstack([coo_matrix(features_structural), coo_matrix(features_sentiment), coo_matrix(features_pos),\n",
    "                             coo_matrix(features_topic), coo_matrix(features_reviewer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x[0].label for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.02335429, 0.01304007, 0.01229262, 0.01186943, 0.0153687 ]),\n",
       " 'score_time': array([0.00145555, 0.00159121, 0.00147748, 0.00139523, 0.00144601]),\n",
       " 'test_score': array([0.59576427, 0.60380835, 0.59060197, 0.61578624, 0.5958231 ])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=5, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannot replicate results. adding more topics seems to have virtually no difference. using bigrams alone for lda topics seems to improve by 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
