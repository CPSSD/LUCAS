{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following experiment 1, we now want to try to find better features. This will take inspiration from existing research done at Stanford. We will derive the same features and attempt to replicate the same benchmark as them. [Paper here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjf1Pbo6ZveAhUKBsAKHbeSAicQFjAAegQIBhAC&url=http%3A%2F%2Fcs229.stanford.edu%2Fproj2017%2Ffinal-reports%2F5229663.pdf&usg=AOvVaw1SAoqP8hAkRiRJH9lwmeEn)\n",
    "\n",
    "First the same setup as Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../..'))\n",
    "data_file_path = 'data/yelpNYC'\n",
    "from protos import review_set_pb2, review_pb2\n",
    "import nltk\n",
    "review_set = review_set_pb2.ReviewSet()\n",
    "with open(data_file_path, 'rb') as f:\n",
    "  review_set.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features will be the structural features. These are length of the review, average word length, number of sentences, average sentence length, percentage of numerals, percentage of capitalized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_avg_token_length, find_numerals_ratio\n",
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "\n",
    "find_words = lambda text: nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def structural_features(review):\n",
    "  review_content = review.review_content\n",
    "  length_of_review = len(review_content)\n",
    "  words = find_words(review_content)\n",
    "  avg_word_length = find_avg_token_length(words)\n",
    "  sentences = nltk.tokenize.sent_tokenize(review_content)\n",
    "  sentence_length_of_review = len(sentences)\n",
    "  avg_sentence_length = find_avg_token_length(sentences)\n",
    "  numerals_ratio = find_numerals_ratio(words)\n",
    "  capitalised_word_ratio = find_capitalised_word_ratio(words)\n",
    "  return (length_of_review, avg_word_length, sentence_length_of_review,\n",
    "          avg_sentence_length, numerals_ratio, capitalised_word_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the Part of Speech features. There are 36 part of speech categories. Descriptions can be found [here](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(words):\n",
    "  tag_map = {\n",
    "    \"CD\":  0, \"DT\":  0, \"EX\":  0, \"FW\":   0, \"IN\":  0, \"JJ\":  0, \"JJR\": 0, \"JJS\": 0, \"LS\":   0,\n",
    "    \"MD\":  0, \"NN\":  0, \"NNP\": 0, \"NNPS\": 0, \"NNS\": 0, \"PDT\": 0, \"POS\": 0, \"PRP\": 0, \"PRP$\": 0,\n",
    "    \"RB\":  0, \"RBR\": 0, \"RBS\": 0, \"RP\":   0, \"SYM\": 0, \"TO\":  0, \"UH\":  0, \"VB\":  0, \"VBD\":  0,\n",
    "    \"VBG\": 0, \"VBN\": 0, \"VBP\": 0, \"VBZ\":  0, \"WDT\": 0, \"WP\":  0, \"WP$\": 0, \"WRB\": 0, \"CC\":   0\n",
    "  }\n",
    "  tags = nltk.pos_tag(words)\n",
    "  for tag in tags:\n",
    "    key = tag[1]\n",
    "    if key in tag_map:\n",
    "      tag_map[key] += 1\n",
    "  order = [\"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\",\n",
    "           \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\",\n",
    "           \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"CC\"]\n",
    "  return [tag_map[x] for x in order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import functools\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_features(review_words):\n",
    "  polarities = []\n",
    "  num_positive = 0\n",
    "  num_negative = 0\n",
    "  for word in review_words:\n",
    "    polarity = sentiment_analyzer.polarity_scores(word)['compound']\n",
    "    polarities.append(1 if polarity > 0 else -1 if polarity < 0 else 0)\n",
    "  reduce_func = lambda c, p: (c[0] + 1, c[1]) if p > 0 else (c[0], c[1] + 1) if p < 0 else (c[0], c[1])\n",
    "  num_positive, num_negative = functools.reduce(reduce_func, polarities, (0, 0))\n",
    "  total = num_positive + num_negative\n",
    "  if total == 0:\n",
    "    return (0, 0)\n",
    "  return (num_positive / total, num_negative / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the topic model features from LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = review_set.reviews[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some data cleaning, which will improve our results when generating LDA topics. This cleaning is stemming and lemmatization, and removing words with three or less characters.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming will reduce a word to a 'stem' form, which can be used to normalise words that mean the same thing. For example 'cleanly' and 'cleanest' would be stemmed to 'clean'. Lemmatization uses a vocabulary and morphological analysis to more intelligently normalise words, for example 'car' and 'automobile' could go to 'vehicle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "  lemmatized = []\n",
    "  for word in words:\n",
    "    lemmatized.append(stemmer.stem(lemmatizer.lemmatize(word, pos='v')))\n",
    "  return lemmatized\n",
    "\n",
    "def preprocess_words(words):\n",
    "  return lemmatize_words([x for x in words if len(x) > 3])\n",
    "\n",
    "preprocessed_words = [preprocess_words(find_words(x.review_content)) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=10, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_features(review):\n",
    "  words = find_words(review.review_content)\n",
    "  bow = dictionary.doc2bow(preprocess_words(words))\n",
    "  topics = lda_model.get_document_topics(bow)\n",
    "  t = [0] * 10\n",
    "  for topic in topics:\n",
    "    t[topic[0]] = topic[1]\n",
    "  return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the reviewer features. This includes the maximum number of reviews in a day, average review length, standard deviation of reviewer's ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import find_capitalised_word_ratio\n",
    "from exp2_feature_extraction import max_date_occurrences\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def reviewer_features(review, reviews_by_reviewer):\n",
    "  reviews = reviews_by_reviewer[review.user_id]\n",
    "  max_reviews_in_day = max_date_occurrences(reviews)\n",
    "  average_review_length = functools.reduce(lambda total, review: total + len(review.review_content), reviews, 0) / len(reviews)\n",
    "  ratings_stdev = 0 if len(reviews) == 1 else statistics.stdev([x.rating for x in reviews])\n",
    "  return (max_reviews_in_day, average_review_length, ratings_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put our features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp2_feature_extraction import reviews_by_reviewer\n",
    "from sklearn.utils import shuffle\n",
    "reviews = shuffle(all_reviews)\n",
    "reviews_reviewer_map = reviews_by_reviewer(reviews)\n",
    "\n",
    "predictor_features = []\n",
    "for review in reviews:\n",
    "  review_words = find_words(review.review_content)\n",
    "  next_entry = []\n",
    "  next_entry += list(structural_features(review))\n",
    "  next_entry += list(sentiment_features(review_words))\n",
    "  next_entry += pos_features(review_words)\n",
    "  next_entry += topic_features(review)\n",
    "  next_entry += list(reviewer_features(review, reviews_reviewer_map))\n",
    "  predictor_features.append(next_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, vstack\n",
    "def format_row(features_row):\n",
    "  return coo_matrix(features_row)\n",
    "\n",
    "new_predictor_features = vstack([format_row(x) for x in predictor_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x.label for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.45631766, 0.45523643, 0.44877887, 0.44843102, 0.44655967,\n",
       "        0.44545507, 0.447119  , 0.44638729, 0.44400477, 0.44465542]),\n",
       " 'score_time': array([0.07666707, 0.0496645 , 0.04661965, 0.04664135, 0.04653859,\n",
       "        0.04677415, 0.04654527, 0.04655075, 0.04710889, 0.04622293]),\n",
       " 'test_score': array([0.39326067, 0.601     , 0.562     , 0.5819    , 0.5081    ,\n",
       "        0.5411    , 0.4716    , 0.6047    , 0.6133    , 0.54705471])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(cnb, predictor_features, targets, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
