{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept of different binary text classifiers on our data\n",
    "\n",
    "In the world of ML, there is a vast amount of choice when it comes to which classification method to use.\n",
    "In this notebook, I will be demonstrating how 4 of the most popular classifiers perform:\n",
    "\n",
    "- <b>Multinomial Naive Bayes</b>, the 'punching bag' benchmark classifier of the ML world, using class membership probabilities found by feature vector weights, to predict the membership of a new data point,\n",
    "\n",
    "- <b>Logistic Regression</b>, a method that uses the sigmoid function to transform a representation of how far a new data point lies from a decision boundary found via gradient descent to a class probability,\n",
    "\n",
    "- <b>K-nearest-neighbours</b>, where the classification of X is a vote of the K nearest items to X,\n",
    "\n",
    "- <b>SVM</b>, a method that tries to find a hyperplane to seperate classes by treating them as coordinates in an m dimensional space, m being the number of features.\n",
    "\n",
    "The dataset that we are using in this notebook is very small, containing only 1600 data points, 800 of each class.\n",
    "\n",
    "However, it is a good dataset to use to produce a POC in this notebook, because the feature extraction is fast, it is balanced, and the labels belong to the gold standard. This means we can cross examine multiple classifiers with multiple features in a fast manner to get a feel for how they perform.\n",
    "\n",
    "In terms of producing a reliable model to serve on our API, it is not a good choice, as it does not generalize well.\n",
    "\n",
    "Without further ado, let us begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing some modules for data processing and manipulation, Numpy and Pandas.\n",
    "\n",
    "I also import a helper function to process the raw data into a frame for us, for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>deceptive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We stayed at the Schicago Hilton for 4 days an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hotel is located 1/2 mile from the train stati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I made my reservation at the Hilton Chicago be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>When most people think Hilton, they think luxu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>My husband and I recently stayed stayed at the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review deceptive\n",
       "0         0  We stayed at the Schicago Hilton for 4 days an...         1\n",
       "1         0  Hotel is located 1/2 mile from the train stati...         1\n",
       "2         0  I made my reservation at the Hilton Chicago be...         1\n",
       "3         0  When most people think Hilton, they think luxu...         1\n",
       "4         0  My husband and I recently stayed stayed at the...         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "      fname=\"opspam.pkl\", \n",
    "      origin=\"https://storage.googleapis.com/lucas0/opspam.pkl\", \n",
    "      extract=False)\n",
    "df = pd.read_pickle(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. We have 3 columns: \n",
    "- Sentiment (0 is negative, 1 is positive)\n",
    "- Review, our review text,\n",
    "- Deceptive (0 is genuine, 1 is deceptive)\n",
    "\n",
    "Sentiment and Deceptive are pre-labelled for us.\n",
    "We will be focusing on the deceptive column, the label we wish to predict.\n",
    "\n",
    "Let's seperate our data from the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "y = np.asarray(df['deceptive'], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classifiers only work on numeric features, not the strings that our reviews are currently represented by. To represent our reviews as numeric features, we use a Bag of Words model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer takes our review and returns a $m$-dimensional array, where $m$ is our vocabularly size and $m_i$ is 1 if the word $i$ appears in the review, 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the shape of our data after this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9571)\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, 1600 represents the number of reviews in our data, and 9571 the number of words (size of vocab.)\n",
    "Let's see what happens if we remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9284)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "print(cv.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will help our classifier slightly. Stop words generally don't contribute anything to class membership, and add noise.\n",
    "\n",
    "Here's what our first review looks like in count vector format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3266)\t1\n",
      "  (0, 337)\t1\n",
      "  (0, 7482)\t1\n",
      "  (0, 9210)\t1\n",
      "  (0, 1754)\t1\n",
      "  (0, 9023)\t1\n",
      "  (0, 4085)\t1\n",
      "  (0, 3744)\t1\n",
      "  (0, 4770)\t1\n",
      "  (0, 2829)\t1\n",
      "  (0, 6507)\t1\n",
      "  (0, 2535)\t1\n",
      "  (0, 4343)\t1\n",
      "  (0, 5252)\t1\n",
      "  (0, 2172)\t1\n",
      "  (0, 699)\t1\n",
      "  (0, 4902)\t1\n",
      "  (0, 2859)\t1\n",
      "  (0, 8777)\t1\n",
      "  (0, 1514)\t1\n",
      "  (0, 6982)\t1\n",
      "  (0, 5775)\t1\n",
      "  (0, 2603)\t1\n",
      "  (0, 3518)\t1\n",
      "  (0, 4727)\t1\n",
      "  :\t:\n",
      "  (0, 6979)\t2\n",
      "  (0, 731)\t1\n",
      "  (0, 4143)\t4\n",
      "  (0, 7848)\t2\n",
      "  (0, 8566)\t1\n",
      "  (0, 6886)\t1\n",
      "  (0, 9231)\t1\n",
      "  (0, 387)\t1\n",
      "  (0, 8374)\t1\n",
      "  (0, 8174)\t1\n",
      "  (0, 889)\t2\n",
      "  (0, 3154)\t1\n",
      "  (0, 4839)\t1\n",
      "  (0, 1719)\t1\n",
      "  (0, 571)\t1\n",
      "  (0, 3728)\t1\n",
      "  (0, 2849)\t1\n",
      "  (0, 5563)\t1\n",
      "  (0, 7104)\t1\n",
      "  (0, 1943)\t1\n",
      "  (0, 5528)\t1\n",
      "  (0, 2311)\t1\n",
      "  (0, 4050)\t3\n",
      "  (0, 7137)\t1\n",
      "  (0, 7849)\t1\n"
     ]
    }
   ],
   "source": [
    "print((cv.fit_transform(X)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ie. the 4050'th word in the vocab appeared in the first review 3 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply another transformation. \n",
    "\n",
    "$Tf$ is term frequency, and corresponds to how many times a word appears in a review. The higher, more chances are that this particular review is relevant to this word.  \n",
    "\n",
    "$df$ is the number of reviews the word occured in. The higher, the less we should weight the review because of the word. (If a word occurs in a large number of reviews , it wont play important role in finding out reviews relevant to  the word).\n",
    "\n",
    "Therefore, we use the calculation $tf * \\frac {1} {df} $.\n",
    "\n",
    "This is known as $Tf-idf$, and provides a better representation of our data than just word counts. It weights words that are important to a review's classification higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our first count vector and see what it looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9246)\t0.07738232325341368\n",
      "  (0, 9231)\t0.07738232325341368\n",
      "  (0, 9210)\t0.07738232325341368\n",
      "  (0, 9201)\t0.07738232325341368\n",
      "  (0, 9023)\t0.07738232325341368\n",
      "  (0, 9014)\t0.07738232325341368\n",
      "  (0, 8848)\t0.07738232325341368\n",
      "  (0, 8813)\t0.07738232325341368\n",
      "  (0, 8777)\t0.07738232325341368\n",
      "  (0, 8566)\t0.07738232325341368\n",
      "  (0, 8419)\t0.07738232325341368\n",
      "  (0, 8401)\t0.15476464650682736\n",
      "  (0, 8374)\t0.07738232325341368\n",
      "  (0, 8316)\t0.15476464650682736\n",
      "  (0, 8174)\t0.07738232325341368\n",
      "  (0, 8059)\t0.07738232325341368\n",
      "  (0, 7849)\t0.07738232325341368\n",
      "  (0, 7848)\t0.15476464650682736\n",
      "  (0, 7761)\t0.07738232325341368\n",
      "  (0, 7580)\t0.07738232325341368\n",
      "  (0, 7482)\t0.07738232325341368\n",
      "  (0, 7249)\t0.07738232325341368\n",
      "  (0, 7201)\t0.07738232325341368\n",
      "  (0, 7137)\t0.07738232325341368\n",
      "  (0, 7104)\t0.07738232325341368\n",
      "  :\t:\n",
      "  (0, 2311)\t0.07738232325341368\n",
      "  (0, 2172)\t0.07738232325341368\n",
      "  (0, 1943)\t0.07738232325341368\n",
      "  (0, 1879)\t0.07738232325341368\n",
      "  (0, 1754)\t0.07738232325341368\n",
      "  (0, 1721)\t0.15476464650682736\n",
      "  (0, 1719)\t0.07738232325341368\n",
      "  (0, 1717)\t0.07738232325341368\n",
      "  (0, 1712)\t0.07738232325341368\n",
      "  (0, 1711)\t0.15476464650682736\n",
      "  (0, 1564)\t0.07738232325341368\n",
      "  (0, 1514)\t0.07738232325341368\n",
      "  (0, 1474)\t0.15476464650682736\n",
      "  (0, 1448)\t0.07738232325341368\n",
      "  (0, 1410)\t0.07738232325341368\n",
      "  (0, 990)\t0.15476464650682736\n",
      "  (0, 939)\t0.07738232325341368\n",
      "  (0, 889)\t0.15476464650682736\n",
      "  (0, 731)\t0.07738232325341368\n",
      "  (0, 699)\t0.07738232325341368\n",
      "  (0, 658)\t0.07738232325341368\n",
      "  (0, 571)\t0.07738232325341368\n",
      "  (0, 387)\t0.07738232325341368\n",
      "  (0, 386)\t0.07738232325341368\n",
      "  (0, 337)\t0.07738232325341368\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.fit_transform(cv.fit_transform(X)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Classifier Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun stuff! Let's import all of our classifiers listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.svm as svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines allow us to combine feature extractors with classifiers to make life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'acc': 'accuracy',\n",
    "    'auroc': 'roc_auc',\n",
    "    'f1': 'f1'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't much different things we can try with Naive Bayes, however we can see the difference of unigrams to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc_score_from_probabilities(probabilities, labels):\n",
    "  true_probabilities = [probabilities[i][1] for i in range(0, len(labels))]\n",
    "  return roc_auc_score(labels, true_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "def run_cross_validate1(model, X, y, get_features_fn, cv=5):\n",
    "  splitter = StratifiedShuffleSplit(n_splits=cv, test_size=0.2)\n",
    "  metrics = {\n",
    "    \"accuracies\": [],\n",
    "    \"auroc\": [],\n",
    "    \"f1 scores\": [],\n",
    "    \"log loss\": []\n",
    "  }\n",
    "    \n",
    "  false_negatives = 0\n",
    "  false_positives = 0\n",
    "  for train_indices, test_indices in splitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "\n",
    "    model.fit(training_X, training_y)\n",
    "    probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    predicted = [0 if x[0] > x[1] else 1 for x in probabilities]\n",
    "    for i in range(0, len(predicted)):\n",
    "      if (predicted[i] == 0 and test_y[i] == 1):\n",
    "        false_negatives+=1\n",
    "      if (predicted[i] == 1 and test_y[i] == 0):\n",
    "        false_positives+=1\n",
    "    \n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(auroc_score_from_probabilities(probabilities, test_y))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "    metrics[\"log loss\"].append(log_loss(test_y, probabilities))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  metrics[\"mean log loss\"] = np.mean(metrics[\"log loss\"])\n",
    "  metrics[\"false negatives rate\"] = (false_negatives / cv) / num_samples\n",
    "  metrics[\"false positives rate\"] = (false_positives / cv) / num_samples\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_unigram = CountVectorizer(stop_words='english', ngram_range = (0, 1))\n",
    "cv_bigram = CountVectorizer(stop_words='english', ngram_range = (1, 2))\n",
    "mnb = MultinomialNB(alpha=2)\n",
    "\n",
    "def get_features(X, fit=False):\n",
    "    return X\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(('Bigram MNB tfidf', Pipeline([ ('cv', cv_bigram), ('tfidf', tfidf), ('mnb', mnb) ])))\n",
    "models.append(('Unigram MNB tfidf', Pipeline([ ('cv', cv_unigram), ('tfidf', tfidf), ('mnb', mnb) ])))\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model[0]\n",
    "  accuracies = run_cross_validate1(model[1], [x for x in X], y, get_features, cv=10)\n",
    "  entries.append((model_name, accuracies['mean accuracy'], accuracies['mean auroc'], accuracies['mean f1 scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'accuracy', 'auroc', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "Bigram MNB tfidf     0.865937\n",
       "Unigram MNB tfidf    0.851250\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "Bigram MNB tfidf     0.954734\n",
       "Unigram MNB tfidf    0.948762\n",
       "Name: auroc, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').auroc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "Bigram MNB tfidf     0.876378\n",
       "Unigram MNB tfidf    0.863841\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our Bigram-trained MNB is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple of different types of SVM with a bunch of different parameters. NuSVC is the classic SVC except you can tweak the number of support vectors. Let's compare the different types, using bigram tf-idf vectors as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "def run_cross_validate2(model, X, y, get_features_fn, cv=5):\n",
    "  splitter = StratifiedShuffleSplit(n_splits=cv, test_size=0.2)\n",
    "  metrics = { \"accuracies\": [], \"auroc\": [], \"f1 scores\": [] }\n",
    "    \n",
    "  for train_indices, test_indices in splitter.split(X, y):\n",
    "    training_X = get_features_fn([X[x] for x in train_indices], fit=True)\n",
    "    training_y = [y[x] for x in train_indices]\n",
    "    test_X = get_features_fn([X[x] for x in test_indices])\n",
    "    test_y = [y[x] for x in test_indices]\n",
    "  \n",
    "    model.fit(training_X, training_y)\n",
    "    scores = model.decision_function(test_X)\n",
    "    \n",
    "    predicted = [1 if score >= 0 else 0 for score in scores]\n",
    "    metrics[\"accuracies\"].append(accuracy_score(test_y, predicted))\n",
    "    metrics[\"auroc\"].append(roc_auc_score(test_y, scores))\n",
    "    metrics[\"f1 scores\"].append(f1_score(test_y, predicted))\n",
    "\n",
    "  num_samples = len(X)\n",
    "  metrics[\"mean accuracy\"] = np.mean(metrics[\"accuracies\"])\n",
    "  metrics[\"mean variance\"] = np.var(metrics[\"accuracies\"])\n",
    "  metrics[\"mean auroc\"] = np.mean(metrics[\"auroc\"])\n",
    "  metrics[\"mean f1 scores\"] = np.mean(metrics[\"f1 scores\"])\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store models and param sets\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (1, 2))\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(('SVC', Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.SVC(kernel=\"sigmoid\", gamma='scale', random_state=42)) ])))\n",
    "models.append(('LinearSVC', Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.LinearSVC(random_state=42)) ])))\n",
    "models.append(('NuSVC', Pipeline([ ('cv', cv), ('tfidf', tfidf), ('svm', svm.NuSVC(kernel='sigmoid', gamma='scale', random_state=42)) ])))\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model[0]\n",
    "  accuracies = run_cross_validate2(model[1], [x for x in X], y, get_features, cv=10)\n",
    "  entries.append((model_name, accuracies['mean accuracy'], accuracies['mean auroc'], accuracies['mean f1 scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'accuracy', 'auroc', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "LinearSVC    0.887188\n",
       "NuSVC        0.883750\n",
       "SVC          0.860313\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "LinearSVC    0.957461\n",
       "NuSVC        0.956477\n",
       "SVC          0.947082\n",
       "Name: auroc, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').auroc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "LinearSVC    0.890778\n",
       "NuSVC        0.888324\n",
       "SVC          0.850551\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the LinearSVC is our best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Classifier Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create all of our pipelines, one for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbayes = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('nbayes', MultinomialNB()) ])\n",
    "logreg = Pipeline([ ('cv', cv), ('tfidf', tfidf), ('logreg', LogisticRegression(random_state=42, solver='liblinear')) ])\n",
    "\n",
    "models = {'Naive Bayes': nbayes, 'Log. Regression':logreg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting an accurate test/train split, we split our data up using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get our accuracy scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models.keys():\n",
    "  model_name = model\n",
    "  accuracies = run_cross_validate1(models[model_name], [x for x in X], y, get_features, cv=10)\n",
    "  entries.append((model_name, accuracies['mean accuracy'], accuracies['mean auroc'], accuracies['mean f1 scores']))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'accuracy', 'auroc', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 4 classifier accuracies against each other using the matplotlib library Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "Log. Regression    0.871250\n",
       "Naive Bayes        0.865312\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "Log. Regression    0.945820\n",
       "Naive Bayes        0.951773\n",
       "Name: auroc, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').auroc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "Log. Regression    0.875232\n",
       "Naive Bayes        0.874346\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the SVM worked best, whereas the k-NN algorithm does relatively poorly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

